{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "home=os.path.expanduser(\"~\")\n",
    "os.environ[\"SPARK_HOME\"]=os.path.join(home, 'spark-2.0.0-bin-hadoop2.7')\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder.master(\"local\").appName(\"myApp\").config(conf=myConf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF(Term Frequency-Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tf\tterm frequency 단어의 빈도 수\tft,d / (number of words in d) = 1/4 = 0.25\n",
    "(3번째 문서에 stopwords를 제외하면 4개의 단어, wisdom은 1회 나타난다.)\n",
    "* df\tdocument frequency 단어가 나타난 문서 수\t3 (wisdom이 포함된 문서는 3)\n",
    "* N\tnumber of documents 전체 문서의 수\t11 (전체의 문서는 11개)\n",
    "* idf\tinverse document frequency 단어가 나타난 문서의 비율을 거꾸로\tln(N+1 / df+1) + 1 = log(12/4) + 1 = 1.09861 + 1\n",
    "0으로 나뉘는 것을 방지하기 위해 smoothing, 즉 1을 더한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.09861228867\n"
     ]
    }
   ],
   "source": [
    "doc=[\n",
    "    \"When I find myself in times of trouble\",\n",
    "    \"Mother Mary comes to me\",\n",
    "    \"Speaking words of wisdom, let it be\",\n",
    "    \"And in my hour of darkness\",\n",
    "    \"She is standing right in front of me\",\n",
    "    \"Speaking words of wisdom, let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Whisper words of wisdom, let it be\"\n",
    "]\n",
    "import math\n",
    "tf=1./4\n",
    "df=3.\n",
    "N=11.\n",
    "idf=math.log((N+1)/(df+1))+1\n",
    "print idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1]\n",
      " [1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1]]\n",
      "{u'and': 0, u'be': 1, u'right': 17, u'whisper': 25, u'is': 8, u'it': 9, u'wisdom': 26, u'me': 12, u'let': 10, u'words': 27, u'in': 7, u'front': 5, u'trouble': 23, u'find': 4, u'standing': 20, u'comes': 2, u'myself': 15, u'darkness': 3, u'hour': 6, u'of': 16, u'when': 24, u'times': 21, u'to': 22, u'she': 18, u'mother': 13, u'my': 14, u'mary': 11, u'speaking': 19}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "print vectorizer.fit_transform(doc).todense()\n",
    "print vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9)\t2.791759469228055\n",
      "  (0, 10)\t2.791759469228055\n",
      "  (1, 5)\t2.791759469228055\n",
      "  (1, 4)\t2.791759469228055\n",
      "  (1, 0)\t2.791759469228055\n",
      "  (2, 7)\t2.386294361119891\n",
      "  (2, 13)\t2.09861228866811\n",
      "  (2, 12)\t2.09861228866811\n",
      "  (2, 3)\t1.4054651081081644\n",
      "  (3, 2)\t2.791759469228055\n",
      "  (3, 1)\t2.791759469228055\n",
      "  (4, 8)\t2.791759469228055\n",
      "  (4, 6)\t2.791759469228055\n",
      "  (5, 7)\t2.386294361119891\n",
      "  (5, 13)\t2.09861228866811\n",
      "  (5, 12)\t2.09861228866811\n",
      "  (5, 3)\t1.4054651081081644\n",
      "  (6, 3)\t1.4054651081081644\n",
      "  (7, 3)\t1.4054651081081644\n",
      "  (8, 3)\t1.4054651081081644\n",
      "  (9, 3)\t1.4054651081081644\n",
      "  (10, 13)\t2.09861228866811\n",
      "  (10, 12)\t2.09861228866811\n",
      "  (10, 3)\t1.4054651081081644\n",
      "  (10, 11)\t2.791759469228055\n",
      "{u'standing': 8, u'right': 6, u'darkness': 1, u'hour': 2, u'whisper': 11, u'times': 9, u'let': 3, u'speaking': 7, u'words': 13, u'mother': 5, u'trouble': 10, u'wisdom': 12, u'mary': 4, u'comes': 0}\n",
      "[2.79175947 2.79175947 2.79175947 1.40546511 2.79175947 2.79175947\n",
      " 2.79175947 2.38629436 2.79175947 2.79175947 2.79175947 2.79175947\n",
      " 2.09861229 2.09861229]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, stop_words='english',norm = None)\n",
    "print vectorizer.fit_transform(doc)\n",
    "print vectorizer.vocabulary_\n",
    "print vectorizer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+\n",
      "|sent                                  |\n",
      "+--------------------------------------+\n",
      "|When I find myself in times of trouble|\n",
      "|Mother Mary comes to me               |\n",
      "|Speaking words of wisdom, let it be   |\n",
      "|And in my hour of darkness            |\n",
      "|She is standing right in front of me  |\n",
      "|Speaking words of wisdom, let it be   |\n",
      "|우리 Let it be                          |\n",
      "|나 Let it be                           |\n",
      "|너 Let it be                           |\n",
      "|Let it be                             |\n",
      "|Whisper words of wisdom, let it be    |\n",
      "+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc=[\n",
    "    [\"When I find myself in times of trouble\"],\n",
    "    [\"Mother Mary comes to me\"],\n",
    "    [\"Speaking words of wisdom, let it be\"],\n",
    "    [\"And in my hour of darkness\"],\n",
    "    [\"She is standing right in front of me\"],\n",
    "    [\"Speaking words of wisdom, let it be\"],\n",
    "    [u\"우리 Let it be\"],\n",
    "    [u\"나 Let it be\"],\n",
    "    [u\"너 Let it be\"],\n",
    "    [\"Let it be\"],\n",
    "    [\"Whisper words of wisdom, let it be\"]\n",
    "]\n",
    "\n",
    "myDf=spark.createDataFrame(doc,['sent'])\n",
    "myDf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|                sent|sentLabel|\n",
      "+--------------------+---------+\n",
      "|When I find mysel...|      9.0|\n",
      "|Mother Mary comes...|      8.0|\n",
      "|Speaking words of...|      0.0|\n",
      "|And in my hour of...|      5.0|\n",
      "|She is standing r...|      4.0|\n",
      "|Speaking words of...|      0.0|\n",
      "|        우리 Let it be|      6.0|\n",
      "|         나 Let it be|      1.0|\n",
      "|         너 Let it be|      2.0|\n",
      "|           Let it be|      7.0|\n",
      "|Whisper words of ...|      3.0|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "labelIndexer = StringIndexer(inputCol=\"sent\", outputCol=\"sentLabel\")\n",
    "model=labelIndexer.fit(myDf)\n",
    "siDf=model.transform(myDf)\n",
    "siDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(sent=u'When I find myself in times of trouble', words=[u'when', u'i', u'find', u'myself', u'in', u'times', u'of', u'trouble'])\n",
      "Row(sent=u'Mother Mary comes to me', words=[u'mother', u'mary', u'comes', u'to', u'me'])\n",
      "Row(sent=u'Speaking words of wisdom, let it be', words=[u'speaking', u'words', u'of', u'wisdom,', u'let', u'it', u'be'])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"sent\", outputCol=\"words\")\n",
    "tokDf = tokenizer.transform(myDf)\n",
    "for r in tokDf.select(\"sent\", \"words\").take(3):\n",
    "    print r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                sent|            wordsReg|\n",
      "+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|\n",
      "|Mother Mary comes...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|\n",
      "|And in my hour of...|[and, in, my, hou...|\n",
      "|She is standing r...|[she, is, standin...|\n",
      "|Speaking words of...|[speaking, words,...|\n",
      "|        우리 Let it be|   [우리, let, it, be]|\n",
      "|         나 Let it be|    [나, let, it, be]|\n",
      "|         너 Let it be|    [너, let, it, be]|\n",
      "|           Let it be|       [let, it, be]|\n",
      "|Whisper words of ...|[whisper, words, ...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "re = RegexTokenizer(inputCol=\"sent\", outputCol=\"wordsReg\", pattern=\"\\s+\")\n",
    "reDf=re.transform(myDf)\n",
    "reDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "* http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i me my myself we our ours ourselves you your yours yourself yourselves he him his himself she her hers herself it its itself they them their theirs themselves what which who whom this that these those am is are was were be been being have has had having do does did doing a an the and but if or because as until while of at by for with about against between into through during before after above below to from up down in out on off over under again further then once here there when where why how all any both each few more most other some such no nor not only own same so than too very s t can will just don should now d ll m o re ve y ain aren couldn didn doesn hadn hasn haven isn ma mightn mustn needn shan shouldn wasn weren won wouldn 나 너 우리 나 너 우리\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "stopwords=list()\n",
    "_stopwords=stop.getStopWords()\n",
    "for e in _stopwords:\n",
    "    stopwords.append(e)\n",
    "\n",
    "_mystopwords=[u\"나\",u\"너\", u\"우리\"]\n",
    "for e in _mystopwords:\n",
    "    stopwords.append(e)\n",
    "stop.setStopWords(stopwords)\n",
    "\n",
    "for e in stop.getStopWords():\n",
    "    print e,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                sent|            wordsReg|             nostops|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|[find, times, tro...|\n",
      "|Mother Mary comes...|[mother, mary, co...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|\n",
      "|And in my hour of...|[and, in, my, hou...|    [hour, darkness]|\n",
      "|She is standing r...|[she, is, standin...|[standing, right,...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|\n",
      "|        우리 Let it be|   [우리, let, it, be]|               [let]|\n",
      "|         나 Let it be|    [나, let, it, be]|               [let]|\n",
      "|         너 Let it be|    [너, let, it, be]|               [let]|\n",
      "|           Let it be|       [let, it, be]|               [let]|\n",
      "|Whisper words of ...|[whisper, words, ...|[whisper, words, ...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopDf=stop.transform(reDf)\n",
    "stopDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                sent|             nostops|                  cv|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[find, times, tro...|(16,[8,9,14],[1.0...|\n",
      "|Mother Mary comes...|[mother, mary, co...|(16,[11,12,13],[1...|\n",
      "|Speaking words of...|[speaking, words,...|(16,[0,1,2,3],[1....|\n",
      "|And in my hour of...|    [hour, darkness]|(16,[5,10],[1.0,1...|\n",
      "|She is standing r...|[standing, right,...|(16,[4,6,15],[1.0...|\n",
      "|Speaking words of...|[speaking, words,...|(16,[0,1,2,3],[1....|\n",
      "|        우리 Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|         나 Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|         너 Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|           Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|Whisper words of ...|[whisper, words, ...|(16,[0,1,2,7],[1....|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n",
      "let wisdom, words speaking right hour standing whisper times trouble darkness mother mary comes find front\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"nostops\", outputCol=\"cv\", vocabSize=30,minDF=1.0)\n",
    "cvModel = cv.fit(stopDf)\n",
    "cvDf = cvModel.transform(stopDf)\n",
    "\n",
    "cvDf.select('sent','nostops','cv').show()\n",
    "for v in cvModel.vocabulary:\n",
    "    print v,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(nostops=[u'find', u'times', u'trouble'], hash=SparseVector(50, {10: 1.0, 24: 1.0, 43: 1.0}))\n",
      "Row(nostops=[u'mother', u'mary', u'comes'], hash=SparseVector(50, {1: 1.0, 21: 1.0, 24: 1.0}))\n",
      "Row(nostops=[u'speaking', u'words', u'wisdom,', u'let'], hash=SparseVector(50, {9: 1.0, 12: 1.0, 14: 1.0, 41: 1.0}))\n",
      "Row(nostops=[u'hour', u'darkness'], hash=SparseVector(50, {23: 1.0, 27: 1.0}))\n",
      "Row(nostops=[u'standing', u'right', u'front'], hash=SparseVector(50, {24: 1.0, 43: 1.0, 46: 1.0}))\n",
      "Row(nostops=[u'speaking', u'words', u'wisdom,', u'let'], hash=SparseVector(50, {9: 1.0, 12: 1.0, 14: 1.0, 41: 1.0}))\n",
      "Row(nostops=[u'let'], hash=SparseVector(50, {14: 1.0}))\n",
      "Row(nostops=[u'let'], hash=SparseVector(50, {14: 1.0}))\n",
      "Row(nostops=[u'let'], hash=SparseVector(50, {14: 1.0}))\n",
      "Row(nostops=[u'let'], hash=SparseVector(50, {14: 1.0}))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "hashTF = HashingTF(inputCol=\"nostops\", outputCol=\"hash\", numFeatures=50)\n",
    "hashDf = hashTF.transform(stopDf)\n",
    "idf = IDF(inputCol=\"hash\", outputCol=\"idf\")\n",
    "idfModel = idf.fit(hashDf)\n",
    "idfDf = idfModel.transform(hashDf)\n",
    "for e in idfDf.select(\"nostops\",\"hash\").take(10):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                sent|               words|              ngrams|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|[when i, i find, ...|\n",
      "|Mother Mary comes...|[mother, mary, co...|[mother mary, mar...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking words, ...|\n",
      "|And in my hour of...|[and, in, my, hou...|[and in, in my, m...|\n",
      "|She is standing r...|[she, is, standin...|[she is, is stand...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking words, ...|\n",
      "|        우리 Let it be|   [우리, let, it, be]|[우리 let, let it, ...|\n",
      "|         나 Let it be|    [나, let, it, be]|[나 let, let it, i...|\n",
      "|         너 Let it be|    [너, let, it, be]|[너 let, let it, i...|\n",
      "|           Let it be|       [let, it, be]|     [let it, it be]|\n",
      "|Whisper words of ...|[whisper, words, ...|[whisper words, w...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n",
      "Row(words=[u'when', u'i', u'find', u'myself', u'in', u'times', u'of', u'trouble'], ngrams=[u'when i', u'i find', u'find myself', u'myself in', u'in times', u'times of', u'of trouble'])\n",
      "Row(words=[u'mother', u'mary', u'comes', u'to', u'me'], ngrams=[u'mother mary', u'mary comes', u'comes to', u'to me'])\n",
      "Row(words=[u'speaking', u'words', u'of', u'wisdom,', u'let', u'it', u'be'], ngrams=[u'speaking words', u'words of', u'of wisdom,', u'wisdom, let', u'let it', u'it be'])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "ngramDf = ngram.transform(tokDf)\n",
    "ngramDf.show()\n",
    "for e in ngramDf.select(\"words\",\"ngrams\").take(3):\n",
    "    print e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 연속데이터의 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/ds_spark_heightweight.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_heightweight.txt\n",
    "1   65.78   112.99\n",
    "2   71.52   136.49\n",
    "3   69.40   153.03\n",
    "4   68.22   142.34\n",
    "5   67.79   144.30\n",
    "6   68.70   123.30\n",
    "7   69.80   141.49\n",
    "8   70.01   136.46\n",
    "9   67.90   112.37\n",
    "10  66.78   120.67\n",
    "11  66.49   127.45\n",
    "12  67.62   114.14\n",
    "13  68.30   125.61\n",
    "14  67.12   122.46\n",
    "15  68.28   116.09\n",
    "16  71.09   140.00\n",
    "17  66.46   129.50\n",
    "18  68.65   142.97\n",
    "19  71.23   137.90\n",
    "20  67.13   124.04\n",
    "21  67.83   141.28\n",
    "22  68.88   143.54\n",
    "23  63.48   97.90\n",
    "24  68.42   129.50\n",
    "25  67.63   141.85\n",
    "26  67.21   129.72\n",
    "27  70.84   142.42\n",
    "28  67.49   131.55\n",
    "29  66.53   108.33\n",
    "30  65.44   113.89\n",
    "31  69.52   103.30\n",
    "32  65.81   120.75\n",
    "33  67.82   125.79\n",
    "34  70.60   136.22\n",
    "35  71.80   140.10\n",
    "36  69.21   128.75\n",
    "37  66.80   141.80\n",
    "38  67.66   121.23\n",
    "39  67.81   131.35\n",
    "40  64.05   106.71\n",
    "41  68.57   124.36\n",
    "42  65.18   124.86\n",
    "43  69.66   139.67\n",
    "44  67.97   137.37\n",
    "45  65.98   106.45\n",
    "46  68.67   128.76\n",
    "47  66.88   145.68\n",
    "48  67.70   116.82\n",
    "49  69.82   143.62\n",
    "50  69.09   134.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "rdd=spark.sparkContext\\\n",
    "    .textFile(os.path.join('data','ds_spark_heightweight.txt'))\n",
    "\n",
    "myRdd=rdd.map(lambda line:[float(x) for x in line.split()])\n",
    "myDf=spark.createDataFrame(myRdd,[\"id\",\"weight\",\"height\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+\n",
      "|  id|weight|height|weight2|\n",
      "+----+------+------+-------+\n",
      "| 1.0| 65.78|112.99|    0.0|\n",
      "| 2.0| 71.52|136.49|    1.0|\n",
      "| 3.0|  69.4|153.03|    1.0|\n",
      "| 4.0| 68.22|142.34|    1.0|\n",
      "| 5.0| 67.79| 144.3|    0.0|\n",
      "| 6.0|  68.7| 123.3|    1.0|\n",
      "| 7.0|  69.8|141.49|    1.0|\n",
      "| 8.0| 70.01|136.46|    1.0|\n",
      "| 9.0|  67.9|112.37|    0.0|\n",
      "|10.0| 66.78|120.67|    0.0|\n",
      "+----+------+------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "binarizer = Binarizer(threshold=68.0, inputCol=\"weight\", outputCol=\"weight2\")\n",
    "binDf = binarizer.transform(myDf)\n",
    "binDf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+-------+\n",
      "|  id|weight|height|weight2|height3|\n",
      "+----+------+------+-------+-------+\n",
      "| 1.0| 65.78|112.99|    0.0|    0.0|\n",
      "| 2.0| 71.52|136.49|    1.0|    1.0|\n",
      "| 3.0|  69.4|153.03|    1.0|    2.0|\n",
      "| 4.0| 68.22|142.34|    1.0|    2.0|\n",
      "| 5.0| 67.79| 144.3|    0.0|    2.0|\n",
      "| 6.0|  68.7| 123.3|    1.0|    0.0|\n",
      "| 7.0|  69.8|141.49|    1.0|    2.0|\n",
      "| 8.0| 70.01|136.46|    1.0|    1.0|\n",
      "| 9.0|  67.9|112.37|    0.0|    0.0|\n",
      "|10.0| 66.78|120.67|    0.0|    0.0|\n",
      "+----+------+------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\n",
    "discretizer = QuantileDiscretizer(numBuckets=3, inputCol=\"height\", outputCol=\"height3\")\n",
    "qdDf = discretizer.fit(binDf).transform(binDf)\n",
    "qdDf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Index: integer (nullable = true)\n",
      " |--  Height(Inches)\": double (nullable = true)\n",
      " |--  \"Weight(Pounds)\": double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib\n",
    "_url = 'https://people.sc.fsu.edu/~jburkardt/data/csv/hw_200.csv'\n",
    "_fname = os.path.join(os.getcwd(),'data','hw_200.csv')\n",
    "\n",
    "if(not os.path.exists(_fname)):\n",
    "    urllib.urlretrieve(_url,_fname)\n",
    "    \n",
    "df = spark.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header='true', inferschema='true').load('data/hw_200.csv')\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['\"Index\"', ' Height(Inches)\"', ' \"Weight(Pounds)\"'],\n",
       " ['1', ' 65.78', ' 112.99'],\n",
       " ['2', ' 71.52', ' 136.49'],\n",
       " ['3', ' 69.40', ' 153.03'],\n",
       " ['4', ' 68.22', ' 142.34'],\n",
       " ['5', ' 67.79', ' 144.30'],\n",
       " ['6', ' 68.70', ' 123.30'],\n",
       " ['7', ' 69.80', ' 141.49'],\n",
       " ['8', ' 70.01', ' 136.46'],\n",
       " ['9', ' 67.90', ' 112.37'],\n",
       " ['10', ' 66.78', ' 120.67'],\n",
       " ['11', ' 66.49', ' 127.45'],\n",
       " ['12', ' 67.62', ' 114.14'],\n",
       " ['13', ' 68.30', ' 125.61'],\n",
       " ['14', ' 67.12', ' 122.46'],\n",
       " ['15', ' 68.28', ' 116.09'],\n",
       " ['16', ' 71.09', ' 140.00'],\n",
       " ['17', ' 66.46', ' 129.50'],\n",
       " ['18', ' 68.65', ' 142.97'],\n",
       " ['19', ' 71.23', ' 137.90'],\n",
       " ['20', ' 67.13', ' 124.04'],\n",
       " ['21', ' 67.83', ' 141.28'],\n",
       " ['22', ' 68.88', ' 143.54'],\n",
       " ['23', ' 63.48', ' 97.90'],\n",
       " ['24', ' 68.42', ' 129.50'],\n",
       " ['25', ' 67.63', ' 141.85'],\n",
       " ['26', ' 67.21', ' 129.72'],\n",
       " ['27', ' 70.84', ' 142.42'],\n",
       " ['28', ' 67.49', ' 131.55'],\n",
       " ['29', ' 66.53', ' 108.33'],\n",
       " ['30', ' 65.44', ' 113.89'],\n",
       " ['31', ' 69.52', ' 103.30'],\n",
       " ['32', ' 65.81', ' 120.75'],\n",
       " ['33', ' 67.82', ' 125.79'],\n",
       " ['34', ' 70.60', ' 136.22'],\n",
       " ['35', ' 71.80', ' 140.10'],\n",
       " ['36', ' 69.21', ' 128.75'],\n",
       " ['37', ' 66.80', ' 141.80'],\n",
       " ['38', ' 67.66', ' 121.23'],\n",
       " ['39', ' 67.81', ' 131.35'],\n",
       " ['40', ' 64.05', ' 106.71'],\n",
       " ['41', ' 68.57', ' 124.36'],\n",
       " ['42', ' 65.18', ' 124.86'],\n",
       " ['43', ' 69.66', ' 139.67'],\n",
       " ['44', ' 67.97', ' 137.37'],\n",
       " ['45', ' 65.98', ' 106.45'],\n",
       " ['46', ' 68.67', ' 128.76'],\n",
       " ['47', ' 66.88', ' 145.68'],\n",
       " ['48', ' 67.70', ' 116.82'],\n",
       " ['49', ' 69.82', ' 143.62'],\n",
       " ['50', ' 69.09', ' 134.93'],\n",
       " ['51', ' 69.91', ' 147.02'],\n",
       " ['52', ' 67.33', ' 126.33'],\n",
       " ['53', ' 70.27', ' 125.48'],\n",
       " ['54', ' 69.10', ' 115.71'],\n",
       " ['55', ' 65.38', ' 123.49'],\n",
       " ['56', ' 70.18', ' 147.89'],\n",
       " ['57', ' 70.41', ' 155.90'],\n",
       " ['58', ' 66.54', ' 128.07'],\n",
       " ['59', ' 66.36', ' 119.37'],\n",
       " ['60', ' 67.54', ' 133.81'],\n",
       " ['61', ' 66.50', ' 128.73'],\n",
       " ['62', ' 69.00', ' 137.55'],\n",
       " ['63', ' 68.30', ' 129.76'],\n",
       " ['64', ' 67.01', ' 128.82'],\n",
       " ['65', ' 70.81', ' 135.32'],\n",
       " ['66', ' 68.22', ' 109.61'],\n",
       " ['67', ' 69.06', ' 142.47'],\n",
       " ['68', ' 67.73', ' 132.75'],\n",
       " ['69', ' 67.22', ' 103.53'],\n",
       " ['70', ' 67.37', ' 124.73'],\n",
       " ['71', ' 65.27', ' 129.31'],\n",
       " ['72', ' 70.84', ' 134.02'],\n",
       " ['73', ' 69.92', ' 140.40'],\n",
       " ['74', ' 64.29', ' 102.84'],\n",
       " ['75', ' 68.25', ' 128.52'],\n",
       " ['76', ' 66.36', ' 120.30'],\n",
       " ['77', ' 68.36', ' 138.60'],\n",
       " ['78', ' 65.48', ' 132.96'],\n",
       " ['79', ' 69.72', ' 115.62'],\n",
       " ['80', ' 67.73', ' 122.52'],\n",
       " ['81', ' 68.64', ' 134.63'],\n",
       " ['82', ' 66.78', ' 121.90'],\n",
       " ['83', ' 70.05', ' 155.38'],\n",
       " ['84', ' 66.28', ' 128.94'],\n",
       " ['85', ' 69.20', ' 129.10'],\n",
       " ['86', ' 69.13', ' 139.47'],\n",
       " ['87', ' 67.36', ' 140.89'],\n",
       " ['88', ' 70.09', ' 131.59'],\n",
       " ['89', ' 70.18', ' 121.12'],\n",
       " ['90', ' 68.23', ' 131.51'],\n",
       " ['91', ' 68.13', ' 136.55'],\n",
       " ['92', ' 70.24', ' 141.49'],\n",
       " ['93', ' 71.49', ' 140.61'],\n",
       " ['94', ' 69.20', ' 112.14'],\n",
       " ['95', ' 70.06', ' 133.46'],\n",
       " ['96', ' 70.56', ' 131.80'],\n",
       " ['97', ' 66.29', ' 120.03'],\n",
       " ['98', ' 63.43', ' 123.10'],\n",
       " ['99', ' 66.77', ' 128.14'],\n",
       " ['100', ' 68.89', ' 115.48'],\n",
       " ['101', ' 64.87', ' 102.09'],\n",
       " ['102', ' 67.09', ' 130.35'],\n",
       " ['103', ' 68.35', ' 134.18'],\n",
       " ['104', ' 65.61', ' 98.64'],\n",
       " ['105', ' 67.76', ' 114.56'],\n",
       " ['106', ' 68.02', ' 123.49'],\n",
       " ['107', ' 67.66', ' 123.05'],\n",
       " ['108', ' 66.31', ' 126.48'],\n",
       " ['109', ' 69.44', ' 128.42'],\n",
       " ['110', ' 63.84', ' 127.19'],\n",
       " ['111', ' 67.72', ' 122.06'],\n",
       " ['112', ' 70.05', ' 127.61'],\n",
       " ['113', ' 70.19', ' 131.64'],\n",
       " ['114', ' 65.95', ' 111.90'],\n",
       " ['115', ' 70.01', ' 122.04'],\n",
       " ['116', ' 68.61', ' 128.55'],\n",
       " ['117', ' 68.81', ' 132.68'],\n",
       " ['118', ' 69.76', ' 136.06'],\n",
       " ['119', ' 65.46', ' 115.94'],\n",
       " ['120', ' 68.83', ' 136.90'],\n",
       " ['121', ' 65.80', ' 119.88'],\n",
       " ['122', ' 67.21', ' 109.01'],\n",
       " ['123', ' 69.42', ' 128.27'],\n",
       " ['124', ' 68.94', ' 135.29'],\n",
       " ['125', ' 67.94', ' 106.86'],\n",
       " ['126', ' 65.63', ' 123.29'],\n",
       " ['127', ' 66.50', ' 109.51'],\n",
       " ['128', ' 67.93', ' 119.31'],\n",
       " ['129', ' 68.89', ' 140.24'],\n",
       " ['130', ' 70.24', ' 133.98'],\n",
       " ['131', ' 68.27', ' 132.58'],\n",
       " ['132', ' 71.23', ' 130.70'],\n",
       " ['133', ' 69.10', ' 115.56'],\n",
       " ['134', ' 64.40', ' 123.79'],\n",
       " ['135', ' 71.10', ' 128.14'],\n",
       " ['136', ' 68.22', ' 135.96'],\n",
       " ['137', ' 65.92', ' 116.63'],\n",
       " ['138', ' 67.44', ' 126.82'],\n",
       " ['139', ' 73.90', ' 151.39'],\n",
       " ['140', ' 69.98', ' 130.40'],\n",
       " ['141', ' 69.52', ' 136.21'],\n",
       " ['142', ' 65.18', ' 113.40'],\n",
       " ['143', ' 68.01', ' 125.33'],\n",
       " ['144', ' 68.34', ' 127.58'],\n",
       " ['145', ' 65.18', ' 107.16'],\n",
       " ['146', ' 68.26', ' 116.46'],\n",
       " ['147', ' 68.57', ' 133.84'],\n",
       " ['148', ' 64.50', ' 112.89'],\n",
       " ['149', ' 68.71', ' 130.76'],\n",
       " ['150', ' 68.89', ' 137.76'],\n",
       " ['151', ' 69.54', ' 125.40'],\n",
       " ['152', ' 67.40', ' 138.47'],\n",
       " ['153', ' 66.48', ' 120.82'],\n",
       " ['154', ' 66.01', ' 140.15'],\n",
       " ['155', ' 72.44', ' 136.74'],\n",
       " ['156', ' 64.13', ' 106.11'],\n",
       " ['157', ' 70.98', ' 158.96'],\n",
       " ['158', ' 67.50', ' 108.79'],\n",
       " ['159', ' 72.02', ' 138.78'],\n",
       " ['160', ' 65.31', ' 115.91'],\n",
       " ['161', ' 67.08', ' 146.29'],\n",
       " ['162', ' 64.39', ' 109.88'],\n",
       " ['163', ' 69.37', ' 139.05'],\n",
       " ['164', ' 68.38', ' 119.90'],\n",
       " ['165', ' 65.31', ' 128.31'],\n",
       " ['166', ' 67.14', ' 127.24'],\n",
       " ['167', ' 68.39', ' 115.23'],\n",
       " ['168', ' 66.29', ' 124.80'],\n",
       " ['169', ' 67.19', ' 126.95'],\n",
       " ['170', ' 65.99', ' 111.27'],\n",
       " ['171', ' 69.43', ' 122.61'],\n",
       " ['172', ' 67.97', ' 124.21'],\n",
       " ['173', ' 67.76', ' 124.65'],\n",
       " ['174', ' 65.28', ' 119.52'],\n",
       " ['175', ' 73.83', ' 139.30'],\n",
       " ['176', ' 66.81', ' 104.83'],\n",
       " ['177', ' 66.89', ' 123.04'],\n",
       " ['178', ' 65.74', ' 118.89'],\n",
       " ['179', ' 65.98', ' 121.49'],\n",
       " ['180', ' 66.58', ' 119.25'],\n",
       " ['181', ' 67.11', ' 135.02'],\n",
       " ['182', ' 65.87', ' 116.23'],\n",
       " ['183', ' 66.78', ' 109.17'],\n",
       " ['184', ' 68.74', ' 124.22'],\n",
       " ['185', ' 66.23', ' 141.16'],\n",
       " ['186', ' 65.96', ' 129.15'],\n",
       " ['187', ' 68.58', ' 127.87'],\n",
       " ['188', ' 66.59', ' 120.92'],\n",
       " ['189', ' 66.97', ' 127.65'],\n",
       " ['190', ' 68.08', ' 101.47'],\n",
       " ['191', ' 70.19', ' 144.99'],\n",
       " ['192', ' 65.52', ' 110.95'],\n",
       " ['193', ' 67.46', ' 132.86'],\n",
       " ['194', ' 67.41', ' 146.34'],\n",
       " ['195', ' 69.66', ' 145.59'],\n",
       " ['196', ' 65.80', ' 120.84'],\n",
       " ['197', ' 66.11', ' 115.78'],\n",
       " ['198', ' 68.24', ' 128.30'],\n",
       " ['199', ' 68.02', ' 127.47'],\n",
       " ['200', ' 71.39', ' 127.88 ']]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "r=requests.get('https://people.sc.fsu.edu/~jburkardt/data/csv/hw_200.csv')\n",
    "wc=r.content.splitlines()\n",
    "\n",
    "ma = map(lambda x : x.split(','), wc)\n",
    "\n",
    "ma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- weight2: double (nullable = true)\n",
      " |-- height3: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+---+------+------+-------+-------+---------+\n",
      "| id|weight|height|weight2|height3| features|\n",
      "+---+------+------+-------+-------+---------+\n",
      "|1.0| 65.78|112.99|    0.0|    0.0|(2,[],[])|\n",
      "|2.0| 71.52|136.49|    1.0|    1.0|[1.0,1.0]|\n",
      "|3.0|  69.4|153.03|    1.0|    2.0|[1.0,2.0]|\n",
      "|4.0| 68.22|142.34|    1.0|    2.0|[1.0,2.0]|\n",
      "|5.0| 67.79| 144.3|    0.0|    2.0|[0.0,2.0]|\n",
      "+---+------+------+-------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "va = VectorAssembler(inputCols=[\"weight2\",\"height3\"],outputCol=\"features\")\n",
    "vaDf = va.transform(qdDf)\n",
    "vaDf.printSchema()\n",
    "vaDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0L, \"a b c d e spark\", 1.0),\n",
    "    (1L, \"b d\", 0.0),\n",
    "    (2L, \"spark f g h\", 1.0),\n",
    "    (3L, \"hadoop mapreduce\", 0.0),\n",
    "    (4L, \"my dog has flea problems. help please.\",0.0)\n",
    "    ], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(df)\n",
    "myDf = model.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예제 연설문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/20191021_policeAddress.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/20191021_policeAddress.txt\n",
    "존경하는 국민 여러분, 경찰관 여러분, 일흔네 돌 ‘경찰의 날’입니다.\n",
    " \n",
    "국민의 안전을 위해 밤낮없이 애쓰시는 전국의 15만 경찰관 여러분께 먼저 감사를 드립니다. 전몰·순직 경찰관들의 고귀한 희생에 경의를 표합니다. 유가족 여러분께 위로의 마음을 전합니다.\n",
    " \n",
    "오늘 홍조근정훈장을 받으신 중앙경찰학교장 이은정 치안감님, 근정포장을 받으신 광주남부경찰서 김동현 경감님을 비롯한 수상자 여러분께 각별한 축하와 감사를 드립니다. 또한 경찰 영웅으로 추서되신 차일혁, 최중락님께 국민의 사랑을 전해드립니다.\n",
    " \n",
    "사랑하는 경찰관 여러분,\n",
    " \n",
    "여러분의 헌신적 노력으로 우리의 치안은 참 좋아졌습니다. 지난해 범죄 발생은 2015년에 비해 15.1% 줄었습니다. 같은 기간 교통사고 사망자는 18.2% 감소했습니다.\n",
    " \n",
    "치안의 개선은 국민의 체감으로 나타나고 있습니다. 올해 상반기 국민의 체감안전도는 74.5점으로 역대 최고를 기록했습니다. 범죄안전도는 처음으로 80점을 넘었습니다.\n",
    " \n",
    "한국을 찾는 외국 관광객들도 우리의 치안을 가장 좋게 평가합니다. 한국의 무엇이 좋았느냐는 물음에 외국 관광객들은 7년 연속으로 치안이 가장 좋았다고 응답했습니다. 개발도상국들은 우리의 경찰을 모범으로 삼으려 합니다.\n",
    " \n",
    "올해는 ‘경찰의 날’에 맞춰 국제치안산업박람회와 서울국제경찰청장회의가 함께 열립니다. 우리의 치안 발전과 치안산업 발전이 세계에 더 널리 알려지게 될 것입니다.\n",
    " \n",
    "자랑스러운 경찰관 여러분,\n",
    " \n",
    "경찰헌장은 “나라와 겨레를 위하여 충성”을 다한다는 다짐으로 시작합니다. 헌장처럼 우리 경찰은 ‘나라와 겨레를 위한 충성’의 길을 걸으려 노력해 왔습니다.\n",
    " \n",
    "대한민국 경찰은 1945년 광복 직후에 공식 탄생했습니다. 그러나 그 뿌리는 대한민국 임시정부에 닿아 있습니다.\n",
    " \n",
    "임시정부 초대 경무국장 백범 김구 선생과 나석주, 나창헌, 유상근 의사 등 임시정부 경찰은 앞장서서 일제와 싸웠습니다. 일본 관헌에게 폭탄을 던지고, 밀정을 응징하며, 임정 요인들을 보호했습니다.\n",
    " \n",
    "광복 이후 6‧25전쟁에서도 경찰은 국군과 함께 피를 흘렸습니다. 그 전쟁에서 1만여 명의 경찰관이 목숨을 잃었습니다. 그 후로도 경찰은 국민의 안전을 지키고 국가의 안보를 도왔습니다. 역대 경찰의 헌신에 대해 국민과 함께 거듭 감사의 말씀을 드립니다. 감사합니다.\n",
    " \n",
    "그러나 잘못도 없지는 않았습니다. 한때 경찰은 공권력을 무리하게 집행하며 국민의 인권을 훼손했습니다. 부실하거나 불공정한 수사로 국민의 지탄을 받은 적도 있습니다. 무기력한 법 집행으로 국민께 걱정을 드리기도 했습니다.\n",
    " \n",
    "지금 경찰은 과거를 돌아보며 국민과 국가에 충성하는 경찰로 거듭나려고 노력하고 있습니다. 경찰은 문재인 정부 들어 가장 먼저 개혁위원회를 만들고 자체개혁에 나섰습니다. 경찰의 개혁을 국민은 큰 기대로 주목하고 있습니다.\n",
    " \n",
    "검경 수사권 조정과 자치경찰제 도입이 국회에서 논의되고 있습니다. 국회가 조속히 입법을 매듭지어 주시기 바랍니다. 그리하여 경찰이 중립성, 공정성, 전문성을 갖추고 본연의 임무를 충실히 수행하는 선진경찰로 더욱 발전해 주기를 소망합니다.\n",
    " \n",
    "정부는 경찰의 근무여건을 개선하기 위해 노력하고 있습니다. 이미 경찰관 8,572명을 늘렸고, 앞으로도 충원을 계속할 것입니다. 특히 일선 경찰의 근무환경을 개선하겠습니다.\n",
    " \n",
    "정부는 누구도 법 위에 군림하지 못하는 법치주의를 확립하고자 합니다. 그러자면 검찰과 경찰이 법을 누구에게나 엄정하고 공정하게 집행해야 됩니다. 수사 또한 엄정하고 공정해야 합니다. 동시에 검찰과 경찰 스스로도 법을 엄격히 준수해야 합니다. 특히 공권력이 인권의 제약을 수반하는 경우에는 절제하며 행사하는 것이 마땅합니다. 검찰개혁과 경찰개혁은 더 미룰 수 없는 시대적 과제가 됐습니다.\n",
    " \n",
    "경찰헌장은 따뜻한 경찰, 의로운 경찰, 공정한 경찰을 다짐합니다. 흔들림 없이 그 길로 가시기 바랍니다. 국민이 여러분을 응원할 것입니다.\n",
    "\n",
    "행사를 준비하신 민갑룡 경찰청장님과 관계자 여러분, 고맙습니다. 함께하신 진영 행정안전부 장관님, 이용범 인천시의회 의장님, 박정훈 경찰위원장님, 강영규 경우회장님과 역대 경찰청장님, 그리고 우리 시민 경찰님들을 비롯한 내빈 여러분, 고맙습니다.\n",
    " \n",
    "감사합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "police=spark.read\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"delimiter\",\" \")\\\n",
    "    .option(\"inferSchema\",\"true\")\\\n",
    "    .schema(\n",
    "        StructType([\n",
    "            StructField(\"sent\",StringType()),\n",
    "            ])\n",
    "    )\\\n",
    "    .text(os.path.join(\"data\", \"20191021_policeAddress.txt\")) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                sent|\n",
      "+--------------------+\n",
      "|존경하는 국민 여러분, 경찰관 ...|\n",
      "|                    |\n",
      "|국민의 안전을 위해 밤낮없이 애...|\n",
      "|                    |\n",
      "|오늘 홍조근정훈장을 받으신 중앙...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "police.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                sent|               words|\n",
      "+--------------------+--------------------+\n",
      "|존경하는 국민 여러분, 경찰관 ...|[존경하는, 국민, 여러분,, ...|\n",
      "|                    |                  []|\n",
      "|국민의 안전을 위해 밤낮없이 애...|[국민의, 안전을, 위해, 밤낮...|\n",
      "|                    |                  []|\n",
      "|오늘 홍조근정훈장을 받으신 중앙...|[오늘, 홍조근정훈장을, 받으신...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"sent\", outputCol=\"words\")\n",
    "tokDf = tokenizer.transform(police)\n",
    "tokDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "존경하는 국민 여러분, 경찰관 여러분, 일흔네 돌 ‘경찰의 날’입니다.\n",
      " \n",
      "국민의 안전을 위해 밤낮없이 애쓰시는 전국의 15만 경찰관 여러분께 먼저 감사를 드립니다. 전몰·순직 경찰관들의 고귀한 희생에 경의를 표합니다. 유가족 여러분께 위로의 마음을 전합니다.\n"
     ]
    }
   ],
   "source": [
    "for r in tokDf.select(\"sent\").take(3):\n",
    "    print r[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopWordsRemover_4cfd80e4116081c1a529"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "stop = StopWordsRemover(inputCol=\"words\", outputCol=\"nostops\")\n",
    "stop.setStopWords([u\"돌\",u\"너\", u\"우리\"])\n",
    "_mystopwords=[u\"나\",u\"너\", u\"우리\"]\n",
    "\n",
    "stopwords = list()\n",
    "for e in _mystopwords:\n",
    "    stopwords.append(e)\n",
    "stop.setStopWords(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                sent|               words|             nostops|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|존경하는 국민 여러분, 경찰관 ...|[존경하는, 국민, 여러분,, ...|[존경하는, 국민, 여러분,, ...|\n",
      "|                    |                  []|                  []|\n",
      "|국민의 안전을 위해 밤낮없이 애...|[국민의, 안전을, 위해, 밤낮...|[국민의, 안전을, 위해, 밤낮...|\n",
      "|                    |                  []|                  []|\n",
      "|오늘 홍조근정훈장을 받으신 중앙...|[오늘, 홍조근정훈장을, 받으신...|[오늘, 홍조근정훈장을, 받으신...|\n",
      "|                    |                  []|                  []|\n",
      "|       사랑하는 경찰관 여러분,|   [사랑하는, 경찰관, 여러분,]|   [사랑하는, 경찰관, 여러분,]|\n",
      "|                    |                  []|                  []|\n",
      "|여러분의 헌신적 노력으로 우리의...|[여러분의, 헌신적, 노력으로,...|[여러분의, 헌신적, 노력으로,...|\n",
      "|                    |                  []|                  []|\n",
      "|치안의 개선은 국민의 체감으로 ...|[치안의, 개선은, 국민의, 체...|[치안의, 개선은, 국민의, 체...|\n",
      "|                    |                  []|                  []|\n",
      "|한국을 찾는 외국 관광객들도 우...|[한국을, 찾는, 외국, 관광객...|[한국을, 찾는, 외국, 관광객...|\n",
      "|                    |                  []|                  []|\n",
      "|올해는 ‘경찰의 날’에 맞춰 국...|[올해는, ‘경찰의, 날’에, ...|[올해는, ‘경찰의, 날’에, ...|\n",
      "|                    |                  []|                  []|\n",
      "|      자랑스러운 경찰관 여러분,|  [자랑스러운, 경찰관, 여러분,]|  [자랑스러운, 경찰관, 여러분,]|\n",
      "|                    |                  []|                  []|\n",
      "|경찰헌장은 “나라와 겨레를 위하...|[경찰헌장은, “나라와, 겨레를...|[경찰헌장은, “나라와, 겨레를...|\n",
      "|                    |                  []|                  []|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopDf=stop.transform(tokDf)\n",
    "stopDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
