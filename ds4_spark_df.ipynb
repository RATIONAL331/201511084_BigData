{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark DataFrame\n",
    "\n",
    "* Last updated 20191015TUE1000 20170606_20170221_20161125\n",
    "\n",
    "## S.1 학습내용\n",
    "\n",
    "### S.1.1 목표\n",
    "\n",
    "* Spark DataFrame을 사용할 수 있다.\n",
    "* Spark SQL을 사용하여 데이터를 추출할 수 있다.\n",
    "* Spark 데이터를 MongoDB에 쓰고, 읽을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.1.2 목차\n",
    "\n",
    "* S.4 DataFrame 생성\n",
    "* S.4.1 schema에서 생성하기\n",
    "* S.4.2 RDD에서 생성하기\n",
    "* S.4.3 Pandas\n",
    "* S.4.4 csv 파일 읽기\n",
    "* S.4.5 tsv 파일 읽기\n",
    "* S.4.6 JSON 파일 읽기\n",
    "* S.4.7 Parquet 파일 읽기, 쓰기\n",
    "* S.5 DataFrame API \n",
    "* S.6 Spark SQL \n",
    "* S.7 MongoDB Spark connector\n",
    "* S.7.1 설정\n",
    "* S.7.2 uri \n",
    "* S.7.3 MongoDB Python API \n",
    "* S.7.4 연습으로 쓰기, 읽기\n",
    "* S.8 spark-submit\n",
    "* S.8.1 간단한 작업\n",
    "* S.8.2 MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.1.3 문제\n",
    "\n",
    "* 문제 S-1: 네트워크에 불법적으로 침입하는 사용자의 분석\n",
    "* 문제 S-2: Twitter JSON 데이터 읽기\n",
    "* 문제 S-3: 뉴욕에서 출생한 신생아 분석\n",
    "* 문제 S-4: 우버 택시의 운행기록 분석\n",
    "* 문제 S-5: JDBC를 사용해서 데이터 읽기\n",
    "* 문제 S-6: MongoDB 저장된 열린데이터 읽어오는 spark-submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## S.2 IPython Notebook에서 SparkSession 생성하기\n",
    "\n",
    "Spark가 설치된 디렉토리를 SPARK_HOME으로 설정하고, 실행에 필요한 'py4j-0.10.1-src.zip', 'pyspark.zip' 라이브러리를 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"SPARK_HOME\"]=os.path.join(os.environ['HOME'],'Downloads','spark-2.0.0-bin-hadoop2.7')\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Spark를 사용하려면 ```SparkSession``` 객체를 생성해야 한다.\n",
    "\n",
    "줄 | 설명\n",
    "-----|-----\n",
    "1 | pyspark 라이브러리 가져오기\n",
    "2 | SparkSession에 필요한 설정을 넣어서 만들어 준다. 설정은 SparkSession이 만들지기 전에 해 두어야 한다. 여기서는 설정을 별도로 하지 않고 비워 놓았다.\n",
    "3 | SparkSession은 ```getOrCreate()```로 이미 만들어져 있으면 현재 것을, 없으면 생성하는 방식을 취하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "print spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.3 DataFrame\n",
    "\n",
    "### 특징\n",
    "\n",
    "DataFrame은 **행, 열로 구조화**된 데이터구조이다.\n",
    "관계형데이터베이스 RDB의 테이블이나 엑셀 쉬트sheet와 비슷하다.\n",
    "또는 Pandas 또는 R을 사용해 보았다면 거기서 제공되는 DataFrame과 유사하다.\n",
    "Apache Spark 1.0에서는 **SchemaRDD**라는 명칭으로 시험적으로 제공되었다. 이름에서 보듯이 RDD에 스키마를 얹어서 만든 개념이다.\n",
    "그러나 Spark의 DataFrame은 **대용량 데이터를 하기 위해 만들어진 프레임워크로 분산**해서 사용할 수 있게 고안되었다.\n",
    "\n",
    "\n",
    "앞서 사용했던 **RDD가 schema를 정하지 않는** 것과 달리, **DataFrame은 모델 schema**를 설정해서 사용을 한다. '열'에 대해 명칭 및 **데이터 타잎**을 가지고 있고, 이를 지켜서 데이터를 저장하게 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### DataFrame의 Schema\n",
    "\n",
    "**Column**은 DataFrame의 **열**이고, **데이터 타잎**을 가진다.\n",
    "**Row**는 DataFrame의 행으로, 데이터 요소항목을 묶어서 구성한다. **Python list나 dict**를 사용하여 Row를 구성할 수 있다.\n",
    "\n",
    "**데이터 타잎**은 다음과 같다.\n",
    "\n",
    "```python\n",
    "NullType\n",
    "StringType\n",
    "BinaryType\n",
    "BooleanType\n",
    "DateType\n",
    "TimestampType\n",
    "DoubleType\n",
    "DecimalType\n",
    "ShortType\n",
    "ArrayType\n",
    "MapType\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### DataFrame 주요 API\n",
    "\n",
    "RDD와 마찬가지로 DataFrame을 구성하여 **머신러닝**의 입력데이터로 사용할 수 있다.\n",
    "현재 버전 2.0부터 RDD에 대한 지원은 줄여나가고, **버전 3.0 이후에는 DataFrame API를 공식적으로 지원**한다고 발표한 바 있다. RDD보다 우선적으로 사용하는 것이 좋겠다.\n",
    "\n",
    "기능 | 설명 | 예제\n",
    "-------|-------|-------\n",
    "```json``` | json 읽기 | ```spark.read.json(\"employee.json\")```\n",
    "```show``` | 데이터 읽기 | ```df.show()```\n",
    "```schema``` | 데이터 schema 보기 | ```df.printSchema()```\n",
    "```select``` | 열을 선택 | ```df.select(\"name\")```\n",
    "```filter``` | 조건 선택 | ```df.filter(dfs(\"age\") > 23).show()```\n",
    "```groupBy``` | 그룹 | ```df.groupBy(\"age\").count().show()```\n",
    "```dropna``` | na를 삭제 | ```df.dropna()``` ```df.na.drop()```\n",
    "```fillna``` | na를 값으로 채우기 | ```fillna()```\n",
    "```count``` | 행 세기 | ```df.count()```\n",
    "```drop``` | 삭제 | ```df.drop(\"name\")```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.4 DataFrame 생성\n",
    "\n",
    "DataFrame은 관계형데이터베이스 RDB 테이블과 같이, schema를 정해서 생성한다.\n",
    "**schema를 정해주지 않으면, Spark가 자동으로 유추**하게 된다.\n",
    "RDD와 같이, 외부 파일 또는 배열과 같은 자료구조에서 읽어서 생성하고, 생성된 DataFrame은 분산하여 처리할 수 있게 된다.\n",
    "\n",
    "생성 방법 | 설명 | 함수\n",
    "----------|----------|----------\n",
    "**외부**에서 읽기 | Hive, csv, JSON, RDB, XML, Parquet, Apache Cassandra, RDD 등 | **```spark.createDataFrame()```** 또는 **```spark.read()```**\n",
    "**내부**에서 읽기 | Python list, dict 또는 ```pyspark.sql.Row``` 객체를 사용해서 한 줄씩 만들 수 있다. | **```spark.createDataFrame()```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.4.1 schema로 생성하기\n",
    "\n",
    "schema는 모델을 말한다. 데이터의 컬럼의 명칭과 데이터타잎을 정해야 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 자동으로 인식하는 schema\n",
    "\n",
    "우선 단순하게 Python 자료구조를 사용해서 생성해 본다.\n",
    "아래 열이 3개인 데이터를 **```createDataFrame()```** 함수를 사용하여 넣어 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myList=[('1','kim, js',170),\n",
    "        ('1','lee, sm', 175),\n",
    "        ('2','lim, yg',180),\n",
    "        ('2','lee',170)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame(myList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "이 경우 Spark가 **자동으로 schema를 설정**한다.\n",
    "```printSchema()``` 함수로 schema를 출력해 보기로 하자.\n",
    "* **컬럼**명은 **일련번호**를 가지고 생성된다. schema를 정하지 않았으므로, 열은 '_1', '_2'와 같이 명명된다.\n",
    "* **데이터 타잎**도 유추해서 생성한다. 올바르게 되지 않을 경우도 있다는 점에 유의한다. 아래에서 보듯이 처음 수는 ```String``` 다음 이름은 ```String``` 키는 ```long```으로 인식하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n",
      "[Row(_1=u'1', _2=u'kim, js', _3=170)]\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()\n",
    "print myDf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 컬럼명 설정\n",
    "\n",
    "앞서 컬럼 Column을 정의하지 않고 DataFrame을 생성하였는데, 이번에는 **컬럼을 정해서** 생성하자.\n",
    "**```createDataFrame()```** 함수에 **인자로 컬럼명을 리스트 ```['year','name','height']```로** 정해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(year=u'1', name=u'kim, js', height=170)]\n"
     ]
    }
   ],
   "source": [
    "print spark.createDataFrame(myList, ['year','name','height']).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "간단한 연산식으로 데이터 100개를 생성해 보자.\n",
    "Python list인 names, items 배열에서 **modulus**를 활용하여 하나씩 선택하여 데이터를 생성한다.\n",
    "**names**는 4개이므로 4로 나눈 나머지와 **items**는 6개이므로 6으로 나눈 나머지를 하나씩 선택하고 있다.\n",
    "그리고 컬럼명을 ```[\"name\",\"item\"]```으로 정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|name|      item|\n",
      "+----+----------+\n",
      "| kim|  espresso|\n",
      "| lee|     latte|\n",
      "| lee| americano|\n",
      "| lim|  affocato|\n",
      "| kim|long black|\n",
      "| lee|  macciato|\n",
      "| lee|  espresso|\n",
      "| lim|     latte|\n",
      "| kim| americano|\n",
      "| lee|  affocato|\n",
      "+----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = [\"kim\",\"lee\",\"lee\",\"lim\"]\n",
    "items = [\"espresso\",\"latte\",\"americano\",\"affocato\",\"long black\",\"macciato\"]\n",
    "df = spark.createDataFrame([(names[i%4], items[i%6]) for i in range(100)],\\\n",
    "                           [\"name\",\"item\"])\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "컬럼을 골라내어 **```select()```**할 수 있다.\n",
    "* **```item```** 열의 **```substr```**으로 1,3문자를 선택\n",
    "* **```alias```**로 **컬럼명**을 정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|short name|\n",
      "+----------+\n",
      "|       esp|\n",
      "|       lat|\n",
      "|       ame|\n",
      "+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.item.substr(1, 3).alias(\"short name\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Row 객체를 사용해서 생성\n",
    "\n",
    "**Row**를 사용해 보자.\n",
    "Row는 **이름(Column)이 붙여진 행**으로 **관계형데이터베이스 레코드 Record**와 같다. \n",
    "속성 명은 'year', 'name', 'height'로 명명한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "Person = Row('year','name', 'height')\n",
    "row1=Person('1','kim, js',170)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "속성명을 읽을 때에는 **```row.key```** 또는 Python dict형식으로 **```row[key]```**와 같이 속성을 읽을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row1:  1 kim, js\n"
     ]
    }
   ],
   "source": [
    "print \"row1: \", row1.year, row1.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "위에서 설정한 Row를 사용하여 DataFrame을 만들어 보자.\n",
    "**Python list에 Row를 넣어** 구성한다.\n",
    "첫번째는 앞서 만든 row1 객체를 넣을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRows = [row1,\n",
    "          Person('1','lee, sm', 175),\n",
    "          Person('2','lim, yg',180),\n",
    "          Person('2','lee',170)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame(myRows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "printSchema()를 해보면, **데이터 타잎**은 string, long으로 Spark에서 **자동** 인식되었다는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      "\n",
      "None\n",
      "+----+-------+------+\n",
      "|year|   name|height|\n",
      "+----+-------+------+\n",
      "|   1|kim, js|   170|\n",
      "|   1|lee, sm|   175|\n",
      "|   2|lim, yg|   180|\n",
      "|   2|    lee|   170|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print myDf.printSchema()\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### schema를 정의하고 생성\n",
    "\n",
    "모델 schema를 정하고, **데이터 타잎**을 정의해 DataFrame을 생성해 본다.\n",
    "**```StructType```**으로 구조체를 선언하고, 컬럼에 대해 **```StructField```**를 설정한다.\n",
    "* **컬럼**의 명칭\n",
    "* 앞서 소개했던 **데이터 타잎**\n",
    "* 마지막은 **NULL**이 허용되는지 여부\n",
    "\n",
    "```python\n",
    "StructType([\n",
    "    StructField(컬럼명, StringType(), True),\n",
    "    ...\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "mySchema=StructType([\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "앞서 **myRows**를 데이터로, **mySchema**에서는 컬럼 명과 데이터타잎을 정의하여 ```createDataFrame()```함수의 인자로 넘겨주고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(year=u'1', name=u'kim, js', height=170)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf=spark.createDataFrame(myRows, mySchema)\n",
    "myDf.printSchema()\n",
    "myDf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.4.2 RDD에서 생성하기\n",
    "\n",
    "RDD는 schema가 정해지지 않은 비구조적 데이터이다.\n",
    "이와 같이 **schema를 정의하지 않으면, Spark는 schema를 유추**하게 된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### schema 자동 인식\n",
    "\n",
    "RDD로부터 DataFrame을 생성할 수 있다. 이 경우 schema를 설정하지 않으면 자동으로 인식된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "myList=[('1','kim, js',170),('1','lee, sm', 175),('2','lim, yg',180),('2','lee',170)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "myRdd = spark.sparkContext.parallelize(myList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```toDF()```로 변환하거나 직접 ```createDataFrame()``` 함수를 사용하여 DataFrame을 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf=myRdd.toDF()\n",
    "rddDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf=spark.createDataFrame(myRdd)\n",
    "rddDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "DataFrame은 관계형데이터베이스의 테이블과 매우 유사하다. **SQL 명령**을 사용하듯이 ```where()```, ```select()```, ```groupby()``` 함수를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| _1|     _2|\n",
      "+---+-------+\n",
      "|  1|kim, js|\n",
      "|  2|    lee|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf.where(rddDf._3 < 175)\\\n",
    "    .select([rddDf._1, rddDf._2])\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```groupBy()```하면 행의 값으로 구분지어서 평균, 합계, 갯수, 최대, 최소 등을 구할 수 있다.\n",
    "첫 컬럼을 ```groupby()```해서 최대값 ```max()```를 구해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| _1|max(_3)|\n",
      "+---+-------+\n",
      "|  1|    175|\n",
      "|  2|    180|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf.groupby(rddDf._1).max().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Row를 사용\n",
    "\n",
    "학년year는 앞에서는 **string**으로 인식되었다. 이번 예제에서는 **형변환**을 해 본다.\n",
    "RDD의 ```map()``` 함수를 사용하여 각 속성을 읽고 ```int()``` 함수로 형변환을 한다.\n",
    "각 속성에 명칭, year, name, height를 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_myRdd=myRdd.map(lambda x:Row(year=int(x[0]),name=x[1],height=int(x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- height: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(height=170, name=u'kim, js', year=1)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myDf=spark.createDataFrame(_myRdd)\n",
    "_myDf.printSchema()\n",
    "_myDf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```Row()```를 사용하여 RDD를 생성할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType, TimestampType\n",
    "r1=Row(name=\"js1\",age=10)\n",
    "r2=Row(name=\"js2\",age=20)\n",
    "_myRdd=spark.sparkContext.parallelize([r1,r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=10, name='js1'), Row(age=20, name='js2')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myRdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "####  schema를 정의하고 생성\n",
    "\n",
    "앞서 보았듯이, schema를 정의하고 RDD에서 DataFrame을 생성할 수 있다.\n",
    "**```StructType```**을 선언하고,\n",
    "컬럼에 대해 **```StructField```**를 **컬럼명**, **데이터 타잎**, **NULL**이 허용되는지 여부를 설정한다.\n",
    "단, 컬럼명이 정렬되면서 age, name 순서가 변경된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 10| js1|\n",
      "| 20| js2|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema=StructType([\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    #StructField(\"created\", TimestampType(), True)\n",
    "])\n",
    "_myDf=spark.createDataFrame(_myRdd,schema)\n",
    "_myDf.printSchema()\n",
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* schema를 정해서 RDD로부터 DataFrame을 생성할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n",
      "+---+----+------+\n",
      "| id|name|height|\n",
      "+---+----+------+\n",
      "|  1| kim|  50.0|\n",
      "|  2| lee|  60.0|\n",
      "|  3|park|  70.0|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "myRdd=spark.sparkContext.parallelize([(1, 'kim', 50.0), (2, 'lee', 60.0), (3, 'park', 70.0)])\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", DoubleType(), True)\n",
    "])\n",
    "_myDf = spark.createDataFrame(myRdd, schema)\n",
    "_myDf.printSchema()\n",
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.4.3 Pandas\n",
    "\n",
    "Spark Dataframe은 다른 프로그래밍 언어에서도 분석도구로 많이 사용되는 형식이다.\n",
    "엑셀 스프레드쉬트와 비슷하다. 또한 최근 많이 사용되는 **R**의 Dataframe이나 **Python Pandas**를 예로 들 수 있다.\n",
    "Spark와 Pandas의 Dataframe을 비교하면,\n",
    "**Pandas**는 데이터 양이 적은 경우, Spark는 분산처리할 수 있으므로 빅데이터에 보다 적합하다.\n",
    "API를 사용하게 되면 Spark Dataframe과 Pandas 간에는 차이가 있다.\n",
    "\n",
    "DataFrame | Spark | Pandas\n",
    "-------|-------|-------\n",
    "csv file | map split(',') | read_csv()\n",
    "| show() | head(), tail()\n",
    "data types | 맞게 추정 | 모두 strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dataframe을 Pandas로 변환\n",
    "\n",
    "Spark Dataframe을 ```toPandas()``` 함수를 사용하여 **Pandas로 변환**할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>name</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>kim, js</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>lee, sm</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>lim, yg</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>lee</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  year     name  height\n",
       "0    1  kim, js     170\n",
       "1    1  lee, sm     175\n",
       "2    2  lim, yg     180\n",
       "3    2      lee     170"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pandas에서 csv 쓰기\n",
    "\n",
    "Dataframe을 **csv**파일로 내보내고 Pandas로 읽어보자.\n",
    "\n",
    "Dataframe을 csv로 쓰려면 라이브러리 ```com.databricks.spark.csv```를 사용해야 한다. **단일 파일이 아니라 디렉토리가 생성**되고 그 안에 여러 파일로 쓰여지게 된다. Pandas를 사용하면 우리가 보통 사용하는 하나의 파일로 쓰여진다.\n",
    "\n",
    "```python\n",
    "-rw-r--r-- 1 jsl jsl 16 10월  5 15:42 part-r-00000-1b8b272e-7590-4e73-bd51-27d12ffa5d88.csv\n",
    "-rw-r--r-- 1 jsl jsl 16 10월  5 15:42 part-r-00001-1b8b272e-7590-4e73-bd51-27d12ffa5d88.csv\n",
    "-rw-r--r-- 1 jsl jsl 16 10월  5 15:42 part-r-00002-1b8b272e-7590-4e73-bd51-27d12ffa5d88.csv\n",
    "-rw-r--r-- 1 jsl jsl 10 10월  5 15:42 part-r-00003-1b8b272e-7590-4e73-bd51-27d12ffa5d88.csv\n",
    "-rw-r--r-- 1 jsl jsl  0 10월  5 15:42 _SUCCESS\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myDf.write.format('com.databricks.spark.csv').save(os.path.join('data','_myDf.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-r-00000-1b8b272e-7590-4e73-bd51-27d12ffa5d88.csv\r\n",
      "part-r-00001-1b8b272e-7590-4e73-bd51-27d12ffa5d88.csv\r\n",
      "part-r-00002-1b8b272e-7590-4e73-bd51-27d12ffa5d88.csv\r\n",
      "part-r-00003-1b8b272e-7590-4e73-bd51-27d12ffa5d88.csv\r\n",
      "_SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/_myDf.csv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pandas를 이용하여 Dataframe을 csv파일로 내보낼 수 있다.\n",
    "\n",
    "```python\n",
    ",year,name,height\n",
    "0,1,\"kim, js\",170\n",
    "1,1,\"lee, sm\",175\n",
    "2,2,\"lim, yg\",180\n",
    "3,2,lee,170\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myDf.toPandas().to_csv(os.path.join('data','myDf.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pandas에서 JSON 읽기\n",
    "\n",
    "트위터 데이터는 JSON 형식을 가지고 있다.\n",
    "아래는 트윗 1개의 샘플이다.\n",
    "실제 트윗 데이터를 구할 수 없다면 아래 샘플을 파일로 저장한 후 사용하면 된다.\n",
    "\n",
    "```python\n",
    "{\"contributors\": null, \"truncated\": false, \"text\": \"RT @soompi: #SEVENTEEN’s Mingyu, Jin Se Yeon, And Leeteuk To MC For 2016 Super Seoul Dream Concert \\nhttps://t.co/1XRSaRBbE0 https://t.co/fi…\", \"is_quote_status\": false, \"in_reply_to_status_id\": null, \"id\": 801657325836763136, \"favorite_count\": 0, \"entities\": {\"symbols\": [], \"user_mentions\": [{\"id\": 17659206, \"indices\": [3, 10], \"id_str\": \"17659206\", \"screen_name\": \"soompi\", \"name\": \"Soompi\"}], \"hashtags\": [{\"indices\": [12, 22], \"text\": \"SEVENTEEN\"}], \"urls\": [{\"url\": \"https://t.co/1XRSaRBbE0\", \"indices\": [100, 123], \"expanded_url\": \"http://www.soompi.com/2016/11/20/seventeens-mingyu-jin-se-yeon-leeteuk-mc-dream-concert/\", \"display_url\": \"soompi.com/2016/11/20/sev…\"}]}, \"retweeted\": false, \"coordinates\": null, \"source\": \"<a href=\\\"http://twitter.com/download/android\\\" rel=\\\"nofollow\\\">Twitter for Android</a>\", \"in_reply_to_screen_name\": null, \"in_reply_to_user_id\": null, \"retweet_count\": 1487, \"id_str\": \"801657325836763136\", \"favorited\": false, \"retweeted_status\": {\"contributors\": null, \"truncated\": false, \"text\": \"#SEVENTEEN’s Mingyu, Jin Se Yeon, And Leeteuk To MC For 2016 Super Seoul Dream Concert \\nhttps://t.co/1XRSaRBbE0 https://t.co/fifXHpF8or\", \"is_quote_status\": false, \"in_reply_to_status_id\": null, \"id\": 800593781586132993, \"favorite_count\": 1649, \"entities\": {\"symbols\": [], \"user_mentions\": [], \"hashtags\": [{\"indices\": [0, 10], \"text\": \"SEVENTEEN\"}], \"urls\": [{\"url\": \"https://t.co/1XRSaRBbE0\", \"indices\": [88, 111], \"expanded_url\": \"http://www.soompi.com/2016/11/20/seventeens-mingyu-jin-se-yeon-leeteuk-mc-dream-concert/\", \"display_url\": \"soompi.com/2016/11/20/sev…\"}], \"media\": [{\"expanded_url\": \"https://twitter.com/soompi/status/800593781586132993/photo/1\", \"display_url\": \"pic.twitter.com/fifXHpF8or\", \"url\": \"https://t.co/fifXHpF8or\", \"media_url_https\": \"https://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\", \"id_str\": \"800593115165798400\", \"sizes\": {\"small\": {\"h\": 382, \"resize\": \"fit\", \"w\": 680}, \"large\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"medium\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"thumb\": {\"h\": 150, \"resize\": \"crop\", \"w\": 150}}, \"indices\": [112, 135], \"type\": \"photo\", \"id\": 800593115165798400, \"media_url\": \"http://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\"}]}, \"retweeted\": false, \"coordinates\": null, \"source\": \"<a href=\\\"https://about.twitter.com/products/tweetdeck\\\" rel=\\\"nofollow\\\">TweetDeck</a>\", \"in_reply_to_screen_name\": null, \"in_reply_to_user_id\": null, \"retweet_count\": 1487, \"id_str\": \"800593781586132993\", \"favorited\": false, \"user\": {\"follow_request_sent\": false, \"has_extended_profile\": true, \"profile_use_background_image\": true, \"default_profile_image\": false, \"id\": 17659206, \"profile_background_image_url_https\": \"https://pbs.twimg.com/profile_background_images/699864769/1cdde0a85f5c0a994ae1fb06d545a5ec.png\", \"verified\": true, \"translator_type\": \"none\", \"profile_text_color\": \"999999\", \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/792117259489583104/4khJk3zz_normal.jpg\", \"profile_sidebar_fill_color\": \"000000\", \"entities\": {\"url\": {\"urls\": [{\"url\": \"http://t.co/3evT80UlR9\", \"indices\": [0, 22], \"expanded_url\": \"http://www.soompi.com\", \"display_url\": \"soompi.com\"}]}, \"description\": {\"urls\": []}}, \"followers_count\": 987867, \"profile_sidebar_border_color\": \"000000\", \"id_str\": \"17659206\", \"profile_background_color\": \"1E1E1E\", \"listed_count\": 3982, \"is_translation_enabled\": true, \"utc_offset\": -28800, \"statuses_count\": 80038, \"description\": \"The original K-pop community. We take gifs, OTPs, and reporting on your bias' fashion choices seriously. But not rumors. Ain't nobody got time for that.\", \"friends_count\": 3532, \"location\": \"Worldwide\", \"profile_link_color\": \"31B6F4\", \"profile_image_url\": \"http://pbs.twimg.com/profile_images/792117259489583104/4khJk3zz_normal.jpg\", \"following\": false, \"geo_enabled\": false, \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/17659206/1478803767\", \"profile_background_image_url\": \"http://pbs.twimg.com/profile_background_images/699864769/1cdde0a85f5c0a994ae1fb06d545a5ec.png\", \"screen_name\": \"soompi\", \"lang\": \"en\", \"profile_background_tile\": true, \"favourites_count\": 1493, \"name\": \"Soompi\", \"notifications\": false, \"url\": \"http://t.co/3evT80UlR9\", \"created_at\": \"Wed Nov 26 20:48:27 +0000 2008\", \"contributors_enabled\": false, \"time_zone\": \"Pacific Time (US & Canada)\", \"protected\": false, \"default_profile\": false, \"is_translator\": false}, \"geo\": null, \"in_reply_to_user_id_str\": null, \"possibly_sensitive\": false, \"lang\": \"en\", \"created_at\": \"Mon Nov 21 06:56:46 +0000 2016\", \"in_reply_to_status_id_str\": null, \"place\": null, \"extended_entities\": {\"media\": [{\"expanded_url\": \"https://twitter.com/soompi/status/800593781586132993/photo/1\", \"display_url\": \"pic.twitter.com/fifXHpF8or\", \"url\": \"https://t.co/fifXHpF8or\", \"media_url_https\": \"https://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\", \"id_str\": \"800593115165798400\", \"sizes\": {\"small\": {\"h\": 382, \"resize\": \"fit\", \"w\": 680}, \"large\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"medium\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"thumb\": {\"h\": 150, \"resize\": \"crop\", \"w\": 150}}, \"indices\": [112, 135], \"type\": \"photo\", \"id\": 800593115165798400, \"media_url\": \"http://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\"}]}, \"metadata\": {\"iso_language_code\": \"en\", \"result_type\": \"recent\"}}, \"user\": {\"follow_request_sent\": false, \"has_extended_profile\": false, \"profile_use_background_image\": true, \"default_profile_image\": true, \"id\": 791090169818521600, \"profile_background_image_url_https\": null, \"verified\": false, \"translator_type\": \"none\", \"profile_text_color\": \"333333\", \"profile_image_url_https\": \"https://abs.twimg.com/sticky/default_profile_images/default_profile_6_normal.png\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"entities\": {\"description\": {\"urls\": []}}, \"followers_count\": 0, \"profile_sidebar_border_color\": \"C0DEED\", \"id_str\": \"791090169818521600\", \"profile_background_color\": \"F5F8FA\", \"listed_count\": 0, \"is_translation_enabled\": false, \"utc_offset\": null, \"statuses_count\": 96, \"description\": \"\", \"friends_count\": 7, \"location\": \"\", \"profile_link_color\": \"1DA1F2\", \"profile_image_url\": \"http://abs.twimg.com/sticky/default_profile_images/default_profile_6_normal.png\", \"following\": false, \"geo_enabled\": false, \"profile_background_image_url\": null, \"screen_name\": \"enriquesanq\", \"lang\": \"es\", \"profile_background_tile\": false, \"favourites_count\": 161, \"name\": \"Enrique santos\", \"notifications\": false, \"url\": null, \"created_at\": \"Wed Oct 26 01:32:49 +0000 2016\", \"contributors_enabled\": false, \"time_zone\": null, \"protected\": false, \"default_profile\": true, \"is_translator\": false}, \"geo\": null, \"in_reply_to_user_id_str\": null, \"possibly_sensitive\": false, \"lang\": \"en\", \"created_at\": \"Thu Nov 24 05:22:55 +0000 2016\", \"in_reply_to_status_id_str\": null, \"place\": null, \"metadata\": {\"iso_language_code\": \"en\", \"result_type\": \"recent\"}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "그러나 JSON파일은 **JSON이 아니라 문자열**이다. 파일에서 읽은 후 **JSON으로 변환**을 해야 한다.\n",
    "트윗은 '\\n'으로 하나씩 구분되어 있고 ```readlines()``` 함수로 전체를 읽을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "_jfname=os.path.join('src','ds_twitter_seoul_3.json')\n",
    "with open(_jfname, 'rb') as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "문자열  양끝에 붙어있을 수 있는 문자를 ```rstrip()```으로 제거한다. 매개변수가 없으면 whitespace를 제거한다.\n",
    "\n",
    "> whitespace\n",
    "\n",
    "> 스페이스바, 탭, RETURN, ENTER 키를 말한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = map(lambda x: x.rstrip(), data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "위 Tweet데이터는 json형식이다. 따라서 list 구조로 만들어 주려면, 앞 뒤로 대괄호 ```[ ]```를 넣고, 각 tweet은 컴마로 연결한다.\n",
    "```join()``` 함수는 인자를 구분자 \",\"로 병합한다.\n",
    "\n",
    "```python\n",
    "\",\".join([\"A\", \"B\", \"C\"]) # A,B,C\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_json_str = \"[\" + ','.join(data) + \"]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "아직 JSON으로 변환이 되지 않았고, **문자열 전체 길이**를 알아 보자. 파일의 크기를 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11308455"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "이제 Pandas로 읽어 보자.\n",
    "Pandas 라이브러리에서 제공하는 ```read_json()```함수를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_df = pd.read_json(data_json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```shape()```은 행과 열의 갯수를 알려준다. ```count()``` 함수는 행의 갯수를 나타내는데, 숫자가 서로 다른 것은 비워있는 경우가 서로 다르기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contributors                    0\n",
      "coordinates                    55\n",
      "created_at                   2013\n",
      "entities                     2013\n",
      "extended_entities             506\n",
      "favorite_count               2013\n",
      "favorited                    2013\n",
      "geo                            55\n",
      "id                           2013\n",
      "id_str                       2013\n",
      "in_reply_to_screen_name       153\n",
      "in_reply_to_status_id         131\n",
      "in_reply_to_status_id_str     131\n",
      "in_reply_to_user_id           153\n",
      "in_reply_to_user_id_str       153\n",
      "is_quote_status              2013\n",
      "lang                         2013\n",
      "metadata                     2013\n",
      "place                          65\n",
      "possibly_sensitive           1097\n",
      "quoted_status                  19\n",
      "quoted_status_id              133\n",
      "quoted_status_id_str          133\n",
      "retweet_count                2013\n",
      "retweeted                    2013\n",
      "retweeted_status             1304\n",
      "source                       2013\n",
      "text                         2013\n",
      "truncated                    2013\n",
      "user                         2013\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print data_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Spark dataframe과 같이 데이터를 생성하였고, 'id'속성을 10개만 읽어 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    801657325836763136\n",
       "1    801657325677400064\n",
       "2    801657307637678080\n",
       "3    801657305628430336\n",
       "4    801657297449586688\n",
       "5    801657287697895424\n",
       "6    801657280760397824\n",
       "7    801657276788523008\n",
       "8    801657268177604608\n",
       "9    801657258400616449\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['id'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.4.4 csv 파일에서 생성\n",
    "\n",
    "Spark examples 폴더에 있는 아래 내용의 ```people.txt```를 읽어보자.\n",
    "```python\n",
    "Michael, 29\n",
    "Andy, 30\n",
    "Justin, 19\n",
    "```\n",
    "\n",
    "```sparkContext.textFile()``` 함수로 읽은 파일은 RDD이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "cfile= os.path.join(os.environ[\"SPARK_HOME\"],\\\n",
    "           \"examples/src/main/resources/people.txt\")\n",
    "lines = spark.sparkContext.textFile(cfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RDD에서 ```Row()```를 사용하여 Dataframe으로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1].strip())))\n",
    "\n",
    "_myDf = spark.createDataFrame(people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(age=29, name=u'Michael'),\n",
       " Row(age=30, name=u'Andy'),\n",
       " Row(age=19, name=u'Justin')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myDf.printSchema()\n",
    "_myDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### DataFrame으로 직접 읽기\n",
    "\n",
    "csv 패키지를 사용해서 읽어 본다. 우선 [Spark의 csv 패키지를 추가한다.](https://spark-packages.org/package/databricks/spark-csv) 패키지는 설정파일 ```spark-defaults.conf```에 추가할 수 있다.\n",
    "\n",
    "```python\n",
    "$ vim conf/spark-defaults.conf\n",
    "spark.jars.packages=com.databricks:spark-csv_2.11:1.5.0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/ds_spark.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark.csv\n",
    "1,2,3,4\n",
    "11,22,33,44\n",
    "111,222,333,444"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  1|  2|  3|  4|\n",
      "+---+---+---+---+\n",
      "| 11| 22| 33| 44|\n",
      "|111|222|333|444|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header='true', inferschema='true').load('data/ds_spark.csv')\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.4.5 tsv 파일 읽기\n",
    "\n",
    "tsv는 **Tab으로 분리된 파일**을 말한다. '\\t'이 포함되어 있는 경우, Spark는 string으로 데이터타잎을 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.658985,  4.285136])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([float(x) for x in '1.658985\t4.285136'.split('\\t')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.658985,  4.285136])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([float(x) for x in '1.658985 4.285136'.split(' ')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 데이터 출처: http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_Dinov_020108_HeightsWeights\n",
    "    * 마우스로 긁어서 50행만 복사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# %load data/ds_spark_heightweight.txt\n",
    "1\t65.78\t112.99\n",
    "2\t71.52\t136.49\n",
    "3\t69.40\t153.03\n",
    "4\t68.22\t142.34\n",
    "5\t67.79\t144.30\n",
    "6\t68.70\t123.30\n",
    "7\t69.80\t141.49\n",
    "8\t70.01\t136.46\n",
    "9\t67.90\t112.37\n",
    "10\t66.78\t120.67\n",
    "11\t66.49\t127.45\n",
    "12\t67.62\t114.14\n",
    "13\t68.30\t125.61\n",
    "14\t67.12\t122.46\n",
    "15\t68.28\t116.09\n",
    "16\t71.09\t140.00\n",
    "17\t66.46\t129.50\n",
    "18\t68.65\t142.97\n",
    "19\t71.23\t137.90\n",
    "20\t67.13\t124.04\n",
    "21\t67.83\t141.28\n",
    "22\t68.88\t143.54\n",
    "23\t63.48\t97.90\n",
    "24\t68.42\t129.50\n",
    "25\t67.63\t141.85\n",
    "26\t67.21\t129.72\n",
    "27\t70.84\t142.42\n",
    "28\t67.49\t131.55\n",
    "29\t66.53\t108.33\n",
    "30\t65.44\t113.89\n",
    "31\t69.52\t103.30\n",
    "32\t65.81\t120.75\n",
    "33\t67.82\t125.79\n",
    "34\t70.60\t136.22\n",
    "35\t71.80\t140.10\n",
    "36\t69.21\t128.75\n",
    "37\t66.80\t141.80\n",
    "38\t67.66\t121.23\n",
    "39\t67.81\t131.35\n",
    "40\t64.05\t106.71\n",
    "41\t68.57\t124.36\n",
    "42\t65.18\t124.86\n",
    "43\t69.66\t139.67\n",
    "44\t67.97\t137.37\n",
    "45\t65.98\t106.45\n",
    "46\t68.67\t128.76\n",
    "47\t66.88\t145.68\n",
    "48\t67.70\t116.82\n",
    "49\t69.82\t143.62\n",
    "50\t69.09\t134.93"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* tsv는 '\\t'로 분리해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "rdd=spark.sparkContext\\\n",
    "    .textFile(os.path.join('data','ds_spark_heightweight.txt'))\n",
    "\n",
    "tRdd=rdd.map(lambda x:x.split('\\t'))\n",
    "tDf=spark.createDataFrame(tRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=u'1', _2=u'65.78', _3=u'112.99')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tDf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### schema 설정\n",
    "\n",
    "schema를 **자동으로 설정하면, string**으로 읽혀진다.\n",
    "schema를 설정한다고 해도, string -> integer, double로 형변환은 이루어지지 않는다.\n",
    "string의 **형변환을 명시적**으로 해주어야 한다.\n",
    "\n",
    "```python\n",
    "mySchema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"weight\", DoubleType(), True),\n",
    "    StructField(\"height\", DoubleType(), True)\n",
    "])\n",
    "myDf=spark.createDataFrame(myRdd, mySchema)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### withColumn\n",
    "\n",
    "```withColumn()```은 열을 추가한다.\n",
    "기존에 있는 ```_1```행을 ```integer```로 형변환해서 ```id```행을 만들고, 기존의 ```_1```행을 삭제해보자.\n",
    "\n",
    "```drop()```은 열을 삭제할 때 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tDf=tDf.withColumn(\"id\",tDf['_1'].cast(\"integer\")).drop('_1')\n",
    "tDf=tDf.withColumn(\"height\",tDf['_2'].cast(\"double\")).drop('_2')\n",
    "tDf=tDf.withColumn(\"weight\",tDf['_3'].cast(\"double\")).drop('_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, height=65.78, weight=112.99)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tDf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 형변환\n",
    "\n",
    "위 tsv 파일에서 생성한 RDD를 탭으로 분리하면서, ```float()```로 형변환을 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 65.78, 112.99]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import numpy as np\n",
    "#myRdd=rdd.map(lambda line:np.array([float(x) for x in line.split('\\t')]))\n",
    "tRdd=rdd.map(lambda line:[float(x) for x in line.split('\\t')])\n",
    "tRdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### DataFrame 생성\n",
    "\n",
    "위 tRdd로부터 컬럼명을 주어 DataFrame을 생성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tDf=spark.createDataFrame(tRdd,[\"id\",\"weight\",\"height\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1.0, weight=65.78, height=112.99)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tDf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 그래프\n",
    "\n",
    "Spark는 그래프를 그리는 기능이 없다.\n",
    "Python matplotlib을 이용해서 그래프를 표현한다.\n",
    "```plot()```에 사용할 수 있는 Python list, numpy array로 데이터를 생성한다.\n",
    "\n",
    "RDD를 numpy array로 변환하려면 ```collect()```를 해주어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 65.78  71.52  69.4   68.22  67.79]\n",
      "[ 112.99  136.49  153.03  142.34  144.3 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "_weightRdd=tDf.rdd.map(lambda fields:fields[1]).collect()\n",
    "_heightRdd=tDf.rdd.map(lambda fields:fields[2]).collect()\n",
    "print np.array(_weightRdd)[:5]\n",
    "print np.array(_heightRdd)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg0AAAFkCAYAAACjCwibAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncl0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0914569f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.array(_weightRdd), np.array(_heightRdd),'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 문제: 년별 분기별 대여건수\n",
    "\n",
    "서울시 열린데이터 https://data.seoul.go.kr/ 서울시 공공자전거 이용현황 데이터를 분석해보자.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 데이터 읽기\n",
    "\n",
    "서울시 열린데이터에서 데이터 ```서울특별시_공공자전거 일별 대여건수_(2018~2019.03).csv``` 를 다운로드 받아서 저장한다.\n",
    "csv 형식으로 schema는 자동 인식하도록 읽는다.\n",
    "일자는 ```timestamp```로 건수는 ```integer```로 인식되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_bicycle = spark.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header='true', inferschema='true').load('data/seoulBicycleDailyCount_2018_201903.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |--  count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_bicycle.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "전체 건수는 455건, 5건의 데이터만 읽어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "455"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_bicycle.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|               date| count|\n",
      "+-------------------+------+\n",
      "|2018-01-01 00:00:00|  4950|\n",
      "|2018-01-02 00:00:00|  7136|\n",
      "|2018-01-03 00:00:00|  7156|\n",
      "|2018-01-04 00:00:00|  7102|\n",
      "|2018-01-05 00:00:00|  7705|\n",
      "+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_bicycle.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 컬럼명 변경\n",
    "\n",
    "앞서 보듯이 파일을 읽으면서 컬럼명이 인식되었는데 \" count\"가 맨 앞에 공백이 하나 있게 되어 변경해보자.\n",
    "일단 붙여진 컬럼의 명칭을 변경하려면 ```withColumnRenamed()```를 연결하여 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bicycle=_bicycle\\\n",
    "    .withColumnRenamed(\"date\", \"Date\")\\\n",
    "    .withColumnRenamed(\" count\", \"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 컬럼 만들기: substr\n",
    "\n",
    "```substr()``` 함수는 인자가 2개로서, 앞글자 '1'은 시작 '4'는 4글자를 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "bicycle=bicycle.withColumn(\"year\",bicycle.Date.substr(1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "bicycle=bicycle.withColumn(\"month\",bicycle.Date.substr(6, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 컬럼 만들기: F 함수\n",
    "\n",
    "함수를 이용해 년, 월, 일 등을 추출할 수 있다.\n",
    "먼저 앞서 생성된 column을 삭제하고 나서 해보자.\n",
    "여러 컬럼을 삭제하기 위해서는 ```*```를 앞에 붙여 준다.\n",
    "물론 하나씩 삭제할 수도 있고, 그러면 별표는 불필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "bicycle = bicycle\\\n",
    "    .withColumn('year', F.year('date'))\\\n",
    "    .withColumn('month', F.month('date'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 년도별 대여건수 합계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|year|sum(count)|\n",
      "+----+----------+\n",
      "|2018|  10124874|\n",
      "|2019|   1871935|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bicycle.groupBy('year').agg({\"count\":\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 년도별, 분기별 대여건수 합계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+------+------+------+-------+-------+-------+-------+-------+------+------+\n",
      "|year|     1|     2|     3|     4|     5|      6|      7|      8|      9|     10|    11|    12|\n",
      "+----+------+------+------+------+------+-------+-------+-------+-------+-------+------+------+\n",
      "|2018|164367|168741|462661|687885|965609|1207123|1100015|1037505|1447993|1420621|961532|500822|\n",
      "|2019|495573|471543|904819|  null|  null|   null|   null|   null|   null|   null|  null|  null|\n",
      "+----+------+------+------+------+------+-------+-------+-------+-------+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bicycle.groupBy('year').pivot('month').agg({\"count\":\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.4.6 JSON 파일에서 생성\n",
    "\n",
    "* Spark example 폴더에 있는 ```people.json``` JSON 파일을 읽어서 DataFrame을 생성한다.\n",
    "\n",
    "```python\n",
    "{\"name\":\"Michael\"}\n",
    "{\"name\":\"Andy\", \"age\":30}\n",
    "{\"name\":\"Justin\", \"age\":19}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jfile= os.path.join(os.environ[\"SPARK_HOME\"],\\\n",
    "           \"examples/src/main/resources/people.json\")\n",
    "\n",
    "_myDf= spark.read.json(jfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```filter()```는 조건에 따라 데이터를 걸러낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.filter(_myDf['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 문제 월드컵 데이터 분석\n",
    "\n",
    "### URL에서 JSON 읽기\n",
    "\n",
    "웹에서 읽는 JSON은 문자열이라는 점에 유의한다. 따라서 json()함수로 변환하는 것이 필요하다.\n",
    "```python\n",
    "[\n",
    "  {\n",
    "    \"Competition\": \"World Cup\",\n",
    "    \"Year\": 1930,\n",
    "    \"Team\": \"Argentina\",\n",
    "    \"Number\": \"\",\n",
    "    \"Position\": \"GK\",\n",
    "    \"FullName\": \"Ãngel Bossio\",\n",
    "    \"Club\": \"Club AtlÃ©tico Talleres de Remedios de Escalada\",\n",
    "    \"ClubCountry\": \"Argentina\",\n",
    "    \"DateOfBirth\": \"1905-5-5\",\n",
    "    \"IsCaptain\": false\n",
    "  },\n",
    "  {\n",
    "    \"Competition\": \"World Cup\",\n",
    "    ...\n",
    "    \"IsCaptain\": false\n",
    "  },\n",
    "  ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "r=requests.get(\"https://raw.githubusercontent.com/jokecamp/FootballData/master/World%20Cups/all-world-cup-players.json\")\n",
    "wc=r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'> <type 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print type(wc), type(wc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'Club': u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada',\n",
       " u'ClubCountry': u'Argentina',\n",
       " u'Competition': u'World Cup',\n",
       " u'DateOfBirth': u'1905-5-5',\n",
       " u'FullName': u'\\xc3ngel Bossio',\n",
       " u'IsCaptain': False,\n",
       " u'Number': u'',\n",
       " u'Position': u'GK',\n",
       " u'Team': u'Argentina',\n",
       " u'Year': 1930}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### DataFrame 생성\n",
    "\n",
    "Python dict로부터 DataFrame을 생성하는 기능은 지원이 되지 않는다.\n",
    "위 ```wc```는 JSON을 포함하는 리스트이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wcDF=spark.createDataFrame(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Club=u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada', ClubCountry=u'Argentina', Competition=u'World Cup', DateOfBirth=u'1905-5-5', FullName=u'\\xc3ngel Bossio', IsCaptain=False, Number=u'', Position=u'GK', Team=u'Argentina', Year=1930)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcDF.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### RDD 생성\n",
    "\n",
    "RDD를 통해서 DataFrame 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wcRdd=spark.sparkContext.parallelize(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'Club': u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada',\n",
       "  u'ClubCountry': u'Argentina',\n",
       "  u'Competition': u'World Cup',\n",
       "  u'DateOfBirth': u'1905-5-5',\n",
       "  u'FullName': u'\\xc3ngel Bossio',\n",
       "  u'IsCaptain': False,\n",
       "  u'Number': u'',\n",
       "  u'Position': u'GK',\n",
       "  u'Team': u'Argentina',\n",
       "  u'Year': 1930}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcRdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  DataFrame의 shema 설정\n",
    "\n",
    "wcRdd에서 DataFrame을 생성하려고 하면 아래와 같이 schema를 설정할 수 있다.\n",
    "JSON은 key, value로 구성되어, 컬럼명을 key에서 가져올 수 있다.\n",
    "실제 Rdd에서 생성하면 자동인식되어 schema가 설정된다.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import *\n",
    "wcSchema=StructType([\n",
    "    StructField(\"Club\", StringType(), True),\n",
    "    StructField(\"ClubCountry\", StringType(), True),\n",
    "    StructField(\"Competition\", StringType(), True),\n",
    "    StructField(\"DateOfBirth\", DateType(), True),\n",
    "    StructField(\"FullName\", StringType(), True),\n",
    "    StructField(\"IsCaptain\", BooleanType(), True),\n",
    "    StructField(\"Number\", IntegerType(), True),\n",
    "    StructField(\"Position\", StringType(), True),\n",
    "    StructField(\"Team\", StringType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True)\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wcDF=spark.createDataFrame(wcRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Club=u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada', ClubCountry=u'Argentina', Competition=u'World Cup', DateOfBirth=u'1905-5-5', FullName=u'\\xc3ngel Bossio', IsCaptain=False, Number=u'', Position=u'GK', Team=u'Argentina', Year=1930)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcDF.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 형변환\n",
    "\n",
    "컬럼명 'DateOfBirth'는 'DoB'로 ```DateType()```, 컬럼명 'Number'는 'NumberInt'로 \"integer\" 형으로 설정하였다.\n",
    "schema를 출력하면 이를 확인할 수 있다.\n",
    "\n",
    "구분 | 의미\n",
    "-----|-----\n",
    "%d | 일자 1 ~ 31\n",
    "%m | 월 1 ~ 12\n",
    "%y | 세기 불포함 2자리 년수 00 ~ 99\n",
    "%Y | 세기 포함한 4자리 년수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### ```DateType``` 형변환\n",
    "\n",
    "* ```datetime``` 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991-11-25 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print datetime.strptime(\"11/25/1991\", '%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "toDate = udf(lambda x: datetime.strptime(x, '%m/%d/%Y'), DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wcDF = wcDF.withColumn('date1', toDate(wcDF['DateOfBirth']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- date1: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* ```to_date()``` 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "wcDF=wcDF.withColumn('date2', to_date(wcDF['DateOfBirth'], 'yyyy-MM-dd'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* ```cast()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "\n",
    "wcDF=wcDF.withColumn('date3', wcDF['DateOfBirth'].cast(DateType()))\n",
    "wcDF=wcDF.withColumn('NumberInt', wcDF['Number'].cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- date1: date (nullable = true)\n",
      " |-- date3: date (nullable = true)\n",
      " |-- NumberInt: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wcDF=wcDF.drop('date1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Club=u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada', ClubCountry=u'Argentina', Competition=u'World Cup', DateOfBirth=u'1905-5-5', FullName=u'\\xc3ngel Bossio', IsCaptain=False, Number=u'', Position=u'GK', Team=u'Argentina', Year=1930, date3=datetime.date(1905, 5, 5), NumberInt=None)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcDF.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 국가별 인원수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|ClubCountry|count|\n",
      "+-----------+-----+\n",
      "|   England |    4|\n",
      "|   Paraguay|   93|\n",
      "|     Russia|   51|\n",
      "|        POL|   11|\n",
      "|        BRA|   27|\n",
      "|    Senegal|    1|\n",
      "|     Sweden|  154|\n",
      "|   Colombia|    1|\n",
      "|        FRA|  155|\n",
      "|        ALG|    8|\n",
      "|   England |    1|\n",
      "|       RUS |    1|\n",
      "|     Turkey|   65|\n",
      "|      Zaire|   22|\n",
      "|       Iraq|   22|\n",
      "|    Germany|  206|\n",
      "|        RSA|   16|\n",
      "|        ITA|  224|\n",
      "|        UKR|   38|\n",
      "|        GHA|    8|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF.groupBy(wcDF.ClubCountry).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 국가별 포지션별 인원수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----+----+----+----+\n",
      "|ClubCountry|    |  DF|  FW|  GK|  MF|\n",
      "+-----------+----+----+----+----+----+\n",
      "|   England |null|null|   2|null|   2|\n",
      "|   Paraguay|null|  26|  37|  10|  20|\n",
      "|     Russia|null|  20|  11|   4|  16|\n",
      "|        POL|null|   2|   2|   3|   4|\n",
      "|        BRA|null|   7|   5|   4|  11|\n",
      "|    Senegal|null|null|null|   1|null|\n",
      "|     Sweden|null|  40|  47|  25|  42|\n",
      "|   Colombia|null|null|   1|null|null|\n",
      "|        ALG|null|   2|null|   6|null|\n",
      "|        FRA|null|  46|  41|  18|  50|\n",
      "|   England |null|null|null|null|   1|\n",
      "|       RUS |null|null|null|   1|null|\n",
      "|     Turkey|null|  20|  13|  12|  20|\n",
      "|      Zaire|null|   6|   5|   3|   8|\n",
      "|       Iraq|null|   6|   4|   3|   9|\n",
      "|    Germany|null|  64|  51|  16|  75|\n",
      "|        RSA|null|   5|   2|   3|   6|\n",
      "|        UKR|null|  13|   7|   4|  14|\n",
      "|        ITA|null|  74|  42|  19|  89|\n",
      "|        CMR|null|   1|   1|   1|null|\n",
      "+-----------+----+----+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF.groupBy('ClubCountry').pivot('Position').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.4.7 Parquet 파일 읽기, 쓰기\n",
    "\n",
    "Parquet는 Apache Hadoop에서 사용하는 데이터 압축형식으로, JSON에서 읽어서 생성할 수 있다.\n",
    "\n",
    "```python\n",
    "-rw-r--r-- 1 jsl jsl 522 10월  7 15:27 part-r-00000-0318688b-018f-4e55-858b-b4b78ac56532.snappy.parquet\n",
    "-rw-r--r-- 1 jsl jsl   0 10월  7 15:27 _SUCCESS\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_myDf.write.parquet(os.path.join(\"data\",\"people.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "앞서 쓰여진 parquet으로부터 읽어서 출력할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_pDf=spark.read.parquet(os.path.join(\"data\",\"people.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_pDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.5 DataFrame API 사용해 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "앞서 Pandas로 저장한 csv파일을 읽어서 DataFrame을 만들어 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myDf = spark.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header='true', inferschema='true').load('data/myDf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      "\n",
      "+---+----+-------+------+\n",
      "|_c0|year|   name|height|\n",
      "+---+----+-------+------+\n",
      "|  0|   1|kim, js|   170|\n",
      "|  1|   1|lee, sm|   175|\n",
      "|  2|   2|lim, yg|   180|\n",
      "|  3|   2|    lee|   170|\n",
      "+---+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### withColumnRenamed\n",
    "\n",
    "앞서 ```withCoumun()``` 명령어로 열을 추가해 보았다. ```withColumnRenamed()```으로 열의 명칭을 ```ClubCountry```에서 ```ClubNation```으로 변경해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wcDF=wcDF.withColumnRenamed('ClubCountry','ClubNation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubNation: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- DoB: date (nullable = true)\n",
      " |-- NumberInt: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 컬럼 선택\n",
    "\n",
    "컬럼명은 아래와 같이 ```select()```로 선택할 수 있다.\n",
    "\n",
    "컬럼 선택 | 예제 | 권고\n",
    "-----|-----|-----\n",
    "점 연산자로 컬럼명을 선택 | myDf.name | N\n",
    "컬럼명으로 컬럼을 선택 | myDf['name'] | Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<name>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<name>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "그러나 컬럼명으로 데이터를 조회하려면 다음과 같이 해서는 안된다. ```select()```를 사용해서 컬럼을 읽을 수 있다.\n",
    "\n",
    "```python\n",
    "myDf['name'].collect()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name=u'kim, js'),\n",
       " Row(name=u'lee, sm'),\n",
       " Row(name=u'lim, yg'),\n",
       " Row(name=u'lee')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_name=myDf.select('name')\n",
    "_name.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Row\n",
    "\n",
    "Row 객체의 값은 아래와 같이 단순하게 split 할 수 없으며, 오류가 발생한다. ```dict```로 변환한 후 그 값을 split해야 한다.\n",
    "\n",
    "```python\n",
    "_name.rdd.map(lambda line:[line.split(',')])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'kim', u' js']\n"
     ]
    }
   ],
   "source": [
    "r=Row(name=u'kim, js')\n",
    "rd=r.asDict()\n",
    "print rd.values()[0].split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 컬럼과 행을 선택\n",
    "\n",
    "* 행: ```where()```에 따라 컬럼의 조건에 맞는 행을 선택하고,\n",
    "* 열: 앞서 배운 ```select()```로 열을 선택\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|height|\n",
      "+-------+------+\n",
      "|kim, js|   170|\n",
      "|    lee|   170|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.where(myDf['height'] < 175)\\\n",
    "    .select(myDf['name'], myDf['height']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### groupBy\n",
    "\n",
    "컬럼을 키 값에 따라 구분하고 통계 값을 계산할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|year|max(height)|\n",
      "+----+-----------+\n",
      "|   1|        175|\n",
      "|   2|        180|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupby(myDf['year']).max().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 사용자정의 함수 udf\n",
    "\n",
    "UDF User Defined Functions는 사용자 정의함수로서 DataFrame의 행을 처리할 경우 유용하다.\n",
    "보통 함수와 같이 함수명과 반환 값을 미리 정의해서 lambda 함수 또는 만든 함수를 사용할 수 있다.\n",
    "\n",
    "앞서 배운 ```withColumn()```에서 udf를 호출해서 ```height``` 컬럼을 ```long```에서 ```DoubleType()```으로 만들어 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "toDoublefunc = udf(lambda x: float(x),DoubleType())\n",
    "myDf = myDf.withColumn(\"heightD\",toDoublefunc(myDf.height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      " |-- heightD: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "year의 문자열을 정수로 형변환 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, struct\n",
    "from pyspark.sql.types import IntegerType\n",
    "toint=udf(lambda x:int(x),IntegerType())\n",
    "myDf=myDf.withColumn(\"yearI\",toint(myDf['year']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      " |-- heightD: double (nullable = true)\n",
      " |-- yearI: integer (nullable = true)\n",
      "\n",
      "+----+-------+------+-------+-----+\n",
      "|year|   name|height|heightD|yearI|\n",
      "+----+-------+------+-------+-----+\n",
      "|   1|kim, js|   170|  170.0|    1|\n",
      "|   1|lee, sm|   175|  175.0|    1|\n",
      "|   2|lim, yg|   180|  180.0|    2|\n",
      "|   2|    lee|   170|  170.0|    2|\n",
      "+----+-------+------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|yearI|\n",
      "+-----+\n",
      "|    1|\n",
      "|    1|\n",
      "|    2|\n",
      "|    2|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.select('yearI').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "대문자로 만들고 문자열로 만들어 보자.\n",
    "반환값의 타잎을 ```StringType()```으로 정의하고 있다는 점에 유의하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+-------+-----+---------+\n",
      "|_c0|year|   name|height|heightD|yearI|nameUpper|\n",
      "+---+----+-------+------+-------+-----+---------+\n",
      "|  0|   1|kim, js|   170|  170.0|    1|  KIM, JS|\n",
      "|  1|   1|lee, sm|   175|  175.0|    1|  LEE, SM|\n",
      "|  2|   2|lim, yg|   180|  180.0|    2|  LIM, YG|\n",
      "|  3|   2|    lee|   170|  170.0|    2|      LEE|\n",
      "+---+----+-------+------+-------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    " \n",
    "def uppercase(s):\n",
    "    return s.upper()\n",
    "\n",
    "upperUdf = udf(uppercase, StringType())\n",
    "myDf = myDf.withColumn(\"nameUpper\", upperUdf(myDf['name']))\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "키를 175를 기준으로 이분화하고 반환값을 ```StringType()```으로 정의해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+-------+-----+---------+----------+\n",
      "|_c0|year|   name|height|heightD|yearI|nameUpper|height>175|\n",
      "+---+----+-------+------+-------+-----+---------+----------+\n",
      "|  0|   1|kim, js|   170|  170.0|    1|  KIM, JS|   shorter|\n",
      "|  1|   1|lee, sm|   175|  175.0|    1|  LEE, SM|    taller|\n",
      "|  2|   2|lim, yg|   180|  180.0|    2|  LIM, YG|    taller|\n",
      "|  3|   2|    lee|   170|  170.0|    2|      LEE|   shorter|\n",
      "+---+----+-------+------+-------+-----+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "height_udf = udf(lambda height: \"taller\" if height >=175 else \"shorter\", StringType())\n",
    "heightDf=myDf.withColumn(\"height>175\", height_udf(myDf.heightD))\n",
    "heightDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### summary statistics\n",
    "\n",
    "column이 연산가능한 데이터타잎인 경우, 요약 값을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+\n",
      "|summary|            height|           heightD|             yearI|\n",
      "+-------+------------------+------------------+------------------+\n",
      "|  count|                 4|                 4|                 4|\n",
      "|   mean|            173.75|            173.75|               1.5|\n",
      "| stddev|4.7871355387816905|4.7871355387816905|0.5773502691896257|\n",
      "|    min|               170|             170.0|                 1|\n",
      "|    max|               180|             180.0|                 2|\n",
      "+-------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.6 Spark SQL\n",
    "\n",
    "DataFrame을 관계형 데이터베이스에서 우리가 사용하는 Sql을 사용할 수 있다. RDD는 비구조적인 경우에 사용하므로 테이블로 변환한 후 Sql을 사용하게 된다.\n",
    "\n",
    "* Spark SQL 구성\n",
    "\n",
    "구분 | 설명\n",
    "-----|-----\n",
    "Language API | Python, Java, Scala, Hive QL API를 제공\n",
    "Schema RDD | RDD에 Schema를 적용해 임시 테이블로 변환한다.<br>createOrReplaceTempView<br>createGlobalTempView\n",
    "Data Sources | 다양한 형식 지원 - HDFS, Cassandra, HBase, RDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "앞서 만들어 놓은 World Cup 데이터를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubNation: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- DoB: date (nullable = true)\n",
      " |-- NumberInt: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "이제 임시 테이블 ```wc```를 만들고, Sql문으로 데이터를 조회해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----+\n",
      "|                Club|     Team|Year|\n",
      "+--------------------+---------+----+\n",
      "|Club AtlÃ©tico Ta...|Argentina|1930|\n",
      "+--------------------+---------+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF.createOrReplaceTempView(\"wc\")\n",
    "spark.sql(\"select Club,Team,Year from wc\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+---------+----+\n",
      "|    FullName|                Club|     Team|Year|\n",
      "+------------+--------------------+---------+----+\n",
      "|Ãngel Bossio|Club AtlÃ©tico Ta...|Argentina|1930|\n",
      "+------------+--------------------+---------+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcPlayers=spark.sql(\"select FullName,Club,Team,Year from wc\")\n",
    "wcPlayers.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name=u'wc', database=None, description=None, tableType=u'TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```wcPlayers```를 RDD로 변환해서 이름만 출력해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full name: Ãngel Bossio\n",
      "Full name: Juan Botasso\n",
      "Full name: Roberto Cherro\n",
      "Full name: Alberto Chividini\n",
      "Full name: \n"
     ]
    }
   ],
   "source": [
    "namesRdd=wcPlayers.rdd.map(lambda x: \"Full name: \"+x[0])\n",
    "for e in namesRdd.take(5):\n",
    "    print e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### sql.functions and join\n",
    "\n",
    "리스트에 포함되어 있는 과일에 고유번호를 할당해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bucketDf=spark.createDataFrame([[1,[\"orange\", \"apple\", \"pineapple\"]],\n",
    "                                [2,[\"watermelon\",\"apple\",\"bananas\"]]],\n",
    "                               [\"bucketId\",\"items\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------------------+\n",
      "|bucketId|items                       |\n",
      "+--------+----------------------------+\n",
      "|1       |[orange, apple, pineapple]  |\n",
      "|2       |[watermelon, apple, bananas]|\n",
      "+--------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucketDf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* explode\n",
    "\n",
    "컬럼에 List 또는 배열이 포함된 경우 ```explode()``` 함수는 이를 flat해서 새로운 컬럼을 생성하게 된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "bDf=bucketDf.select(bucketDf.bucketId,explode(bucketDf.items).alias('item'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|bucketId|      item|\n",
      "+--------+----------+\n",
      "|       1|    orange|\n",
      "|       1|     apple|\n",
      "|       1| pineapple|\n",
      "|       2|watermelon|\n",
      "|       2|     apple|\n",
      "|       2|   bananas|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "또 다른 DataFrame을 생성해보자. 나중에 앞의 DataFrame과 join하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fDf=spark.createDataFrame([[\"orange\", \"F1\"],\n",
    "                            [\"\", \"F2\"],\n",
    "                            [\"pineapple\",\"F3\"],\n",
    "                            [\"watermelon\",\"F4\"],\n",
    "                            [\"bananas\",\"F5\"]],\n",
    "                            [\"item\",\"itemId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      item|itemId|\n",
      "+----------+------+\n",
      "|    orange|    F1|\n",
      "|          |    F2|\n",
      "| pineapple|    F3|\n",
      "|watermelon|    F4|\n",
      "|   bananas|    F5|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* join\n",
    "\n",
    "join은 ```inner, cross, outer, full, full_outer, left, left_outer, right, right_outer, left_semi, left_anti``` 여러 종류가 있다. ```inner```기준으로 item이 일치하지 않는 것은 제외하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "joinDf=fDf.join(bDf, fDf.item==bDf.item, \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+\n",
      "|itemId|      item|bucketId|\n",
      "+------+----------+--------+\n",
      "|    F5|   bananas|       2|\n",
      "|    F1|    orange|       1|\n",
      "|    F3| pineapple|       1|\n",
      "|    F4|watermelon|       2|\n",
      "+------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinDf.select(fDf.itemId,fDf.item,bDf.bucketId).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### agg\n",
    "\n",
    "```agg()```는 합계 함수를 계산할 수 있으며, 지원하는 함수는 ```avg, max, min, sum, count```이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+-------+-----+\n",
      "|year|   name|height|heightD|yearI|\n",
      "+----+-------+------+-------+-----+\n",
      "|   1|kim, js|   170|  170.0|    1|\n",
      "|   1|lee, sm|   175|  175.0|    1|\n",
      "|   2|lim, yg|   180|  180.0|    2|\n",
      "|   2|    lee|   170|  170.0|    2|\n",
      "+----+-------+------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```how tall``` 컬럼을 조건에 따라 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|how tall|\n",
      "+--------+\n",
      "|    <175|\n",
      "|    <175|\n",
      "|    >175|\n",
      "|    <175|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "_myDf=myDf.select(when(myDf['heightD'] >175.0, \">175\")\\\n",
    "            .otherwise(\"<175\").alias(\"how tall\"))\n",
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```year``` 컬럼에 대해 ```agg()``` 함수로 계산할 수 있다.\n",
    "\n",
    "dictionary 형식으로 key는 컬럼명, value는 합계 함수를 적어준다.\n",
    "예를 들어 {\"heightD\":\"avg\"}에서 \"heightD\"는 컬럼명, \"avg\"는 합계함수이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+\n",
      "|year|avg(heightD)|\n",
      "+----+------------+\n",
      "|   1|       172.5|\n",
      "|   2|       175.0|\n",
      "+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupBy('year').agg({\"heightD\":\"avg\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "또는 아래와 같이 별도 ```sql.functions```을 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------+------------+\n",
      "|min(heightD)|max(heightD)|avg(heightD)|sum(heightD)|\n",
      "+------------+------------+------------+------------+\n",
      "|       170.0|       180.0|      173.75|       695.0|\n",
      "+------------+------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "myDf.agg(F.min(myDf.heightD),F.max(myDf.heightD),F.avg(myDf.heightD),F.sum(myDf.heightD)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-1: 네트워크에 불법적으로 침입하는 사용자의 분석\n",
    "\n",
    "### 문제\n",
    "\n",
    "네트워크에 불법적으로 침입하는 시도는 허용되어서는 안된다.\n",
    "1998년 MIT Lincoln Labs에서 DARPA Intrusion Detection Evaluation Program을 연구하였다.\n",
    "이 데이터의 일부가 1999년 KDD로 만들어져 배포되고 있다.\n",
    "https://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "\n",
    "### 해결\n",
    "\n",
    "마지막 행에 attack의 유형이 구분되어 있다. 네트워크 침입 유형의 특징을 분석해 보자.\n",
    "탐지예방 모델을 구축할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "KDD데이터는 41 항목으로 구성되어 있다.\n",
    "\n",
    "```python\n",
    "연결(초) | duration: continuous.\n",
    "프로토콜 (tcp,udp,etc) | protocol_type: symbolic.\n",
    "서비스 (http,telnet, etc) | service: symbolic.\n",
    "flag: symbolic.\n",
    "src_bytes: continuous.\n",
    "dst_bytes: continuous.\n",
    "land: symbolic.\n",
    "wrong_fragment: continuous.\n",
    "urgent: continuous.\n",
    "hot: continuous.\n",
    "num_failed_logins: continuous.\n",
    "logged_in: symbolic.\n",
    "num_compromised: continuous.\n",
    "root_shell: continuous.\n",
    "su_attempted: continuous.\n",
    "num_root: continuous.\n",
    "num_file_creations: continuous.\n",
    "num_shells: continuous.\n",
    "num_access_files: continuous.\n",
    "num_outbound_cmds: continuous.\n",
    "is_host_login: symbolic.\n",
    "is_guest_login: symbolic.\n",
    "count: continuous.\n",
    "srv_count: continuous.\n",
    "serror_rate: continuous.\n",
    "srv_serror_rate: continuous.\n",
    "rerror_rate: continuous.\n",
    "srv_rerror_rate: continuous.\n",
    "same_srv_rate: continuous.\n",
    "diff_srv_rate: continuous.\n",
    "srv_diff_host_rate: continuous.\n",
    "dst_host_count: continuous.\n",
    "dst_host_srv_count: continuous.\n",
    "dst_host_same_srv_rate: continuous.\n",
    "dst_host_diff_srv_rate: continuous.\n",
    "dst_host_same_src_port_rate: continuous.\n",
    "dst_host_srv_diff_host_rate: continuous.\n",
    "dst_host_serror_rate: continuous.\n",
    "dst_host_srv_serror_rate: continuous.\n",
    "dst_host_rerror_rate: continuous.\n",
    "dst_host_srv_rerror_rate: continuous.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### RDD 생성\n",
    "\n",
    "KDD 파일은 **gz** 압축되어 있다. 파일 확장자 'gz'은 'gzip'이라는 압축 도구에서 생성된 파일이다. 지금은 WinZip에서 읽을 수 있다.\n",
    "**RDD**는 **압축파일에서 데이터를 읽어 올 수 있다**.\n",
    "\n",
    "반면, DataFrame은 구조schema를 정의해야 하기 때문에 쉽지 않다. 여기서는 **오류**가 발생한다.\n",
    "따라서 RDD를 생성하고 난 후, 그로부터 DataFrame을 생성하고, Sql을 사용한다.\n",
    "\n",
    "파일이 로컬 디렉토리 ```data```에 존재하면, 즉 이미 내려받았으므로 또 내려받지 않는다. 그렇지 않을 경우에만 ```urlretrieve()``` 함수로 내려받는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "_url = 'http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz'\n",
    "_fname = os.path.join(os.getcwd(),'data','kddcup.data_10_percent.gz')\n",
    "if(not os.path.exists(_fname)):\n",
    "    print \"%s data does not exist! retrieving..\" % _fname\n",
    "    _f=urllib.urlretrieve(_url,_fname)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```textFile()``` 함수로 RDD를 생성한다. ```count()```는 행의 수를 돌려주는 action 함수이다. action 함수는 바로 실행되므로 시간이 좀 걸린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "_rdd = spark.sparkContext.textFile(_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494021"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```map()``` 함수를 사용하여 csv 형식으로 구성된 파일을 컴마(,)로 분리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "_allRdd=_rdd.map(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'0', u'tcp', u'http', u'SF', u'181', u'5450', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'8', u'8', u'0.00', u'0.00', u'0.00', u'0.00', u'1.00', u'0.00', u'0.00', u'9', u'9', u'1.00', u'0.00', u'0.11', u'0.00', u'0.00', u'0.00', u'0.00', u'0.00', u'normal.']]\n"
     ]
    }
   ],
   "source": [
    "print _allRdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**attack 종류**는 41번째 열에 구분되어 있다. 총 494,021건을 정상 'noraml'과 나머지는 'attack'으로 구분한다.\n",
    "'attack'은 크게 4종류로 나눈다. DOS는 서비스 거부, R2L 원격침입, U2R은 루트권한침입, probing은 탐지이다.\n",
    "\n",
    "attack 4종류 | 설명 | 41번째 열\n",
    "-----|-----|-----\n",
    "DOS | denial-of-service, e.g. syn flood | back, land, neptune, pod, smurf, teardrop\n",
    "R2L | unauthorized access from a remote machine | ftp_write, guess_passwd, imap, multihop, phf, spy, warezclient, warezmaster\n",
    "U2R | unauthorized access to local superuser (root) privileges | buffer_overflow, loadmodule, perl, rootkit\n",
    "probing | surveillance and other probing | ipsweep, nmap, portsweep, satan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "열41에 대해 건수를 세어보자.\n",
    "```reduceByKey()```는 인자로 '함수'가 필요. 키별로 '함수를 사용해서' 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'guess_passwd.', 53),\n",
       " (u'nmap.', 231),\n",
       " (u'warezmaster.', 20),\n",
       " (u'rootkit.', 10),\n",
       " (u'warezclient.', 1020),\n",
       " (u'smurf.', 280790),\n",
       " (u'pod.', 264),\n",
       " (u'neptune.', 107201),\n",
       " (u'normal.', 97278),\n",
       " (u'spy.', 2),\n",
       " (u'ftp_write.', 8),\n",
       " (u'phf.', 4),\n",
       " (u'portsweep.', 1040),\n",
       " (u'teardrop.', 979),\n",
       " (u'buffer_overflow.', 30),\n",
       " (u'land.', 21),\n",
       " (u'imap.', 12),\n",
       " (u'loadmodule.', 9),\n",
       " (u'perl.', 3),\n",
       " (u'multihop.', 7),\n",
       " (u'back.', 2203),\n",
       " (u'ipsweep.', 1247),\n",
       " (u'satan.', 1589)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_41 = _allRdd.map(lambda x: (x[41], 1))\n",
    "_41.reduceByKey(lambda x,y: x+y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```groupByKey()```는 키별로 group한다. 위 ```reduceByKey()```와 달리 ```mapValues()```를 사용해 값을 별도로 계산한다는 점에 유의하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'guess_passwd.', 53),\n",
       " (u'nmap.', 231),\n",
       " (u'warezmaster.', 20),\n",
       " (u'rootkit.', 10),\n",
       " (u'warezclient.', 1020),\n",
       " (u'smurf.', 280790),\n",
       " (u'pod.', 264),\n",
       " (u'neptune.', 107201),\n",
       " (u'normal.', 97278),\n",
       " (u'spy.', 2),\n",
       " (u'ftp_write.', 8),\n",
       " (u'phf.', 4),\n",
       " (u'portsweep.', 1040),\n",
       " (u'teardrop.', 979),\n",
       " (u'buffer_overflow.', 30),\n",
       " (u'land.', 21),\n",
       " (u'imap.', 12),\n",
       " (u'loadmodule.', 9),\n",
       " (u'perl.', 3),\n",
       " (u'multihop.', 7),\n",
       " (u'back.', 2203),\n",
       " (u'ipsweep.', 1247),\n",
       " (u'satan.', 1589)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_41 = _allRdd.map(lambda x: (x[41], 1))\n",
    "def f(x): return len(x)\n",
    "_41.groupByKey().mapValues(f).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "데이터가 ```normal```인 경우와 아닌 경우로 구분하자.\n",
    "```filter()```는 41번째 행을 조건에 따라 데이터를 구분한다.\n",
    "```count()``` 함수로 건수를 계산하면 'normal' 97,278, 'attack'은 396,743 건이다.\n",
    "\n",
    "침입구분 | 건수\n",
    "-------|-------\n",
    "normal | 97278\n",
    "attack | 396743\n",
    "전체 | 494021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "_normalRdd=_allRdd.filter(lambda x: x[41]==\"normal.\")\n",
    "_attackRdd=_allRdd.filter(lambda x: x[41]!=\"normal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97278\n",
      "396743\n"
     ]
    }
   ],
   "source": [
    "print _normalRdd.count()\n",
    "print _attackRdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dataframe 생성\n",
    "\n",
    "열 0, 1, 2, 3, 4, 5, 41을 선별하여 스키마를 정해서 RDD를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "_csv = _rdd.map(lambda l: l.split(\",\"))\n",
    "_csvRdd = _csv.map(lambda p: \n",
    "    Row(\n",
    "        duration=int(p[0]), \n",
    "        protocol=p[1],\n",
    "        service=p[2],\n",
    "        flag=p[3],\n",
    "        src_bytes=int(p[4]),\n",
    "        dst_bytes=int(p[5]),\n",
    "        attack=p[41]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RDD를 Dataframe으로 변환한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "_df=spark.createDataFrame(_csvRdd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- attack: string (nullable = true)\n",
      " |-- dst_bytes: long (nullable = true)\n",
      " |-- duration: long (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- protocol: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- src_bytes: long (nullable = true)\n",
      "\n",
      "+-------+---------+--------+----+--------+-------+---------+\n",
      "| attack|dst_bytes|duration|flag|protocol|service|src_bytes|\n",
      "+-------+---------+--------+----+--------+-------+---------+\n",
      "|normal.|     5450|       0|  SF|     tcp|   http|      181|\n",
      "|normal.|      486|       0|  SF|     tcp|   http|      239|\n",
      "|normal.|     1337|       0|  SF|     tcp|   http|      235|\n",
      "|normal.|     1337|       0|  SF|     tcp|   http|      219|\n",
      "|normal.|     2032|       0|  SF|     tcp|   http|      217|\n",
      "+-------+---------+--------+----+--------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df.printSchema()\n",
    "_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### attack 분류\n",
    "\n",
    "네트워크 침입이 'attack' 또는 'normal'에 따라 구분해서 ```attackB``` 컬럼을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "attack_udf = udf(lambda x: \"normal\" if x ==\"normal.\" else \"attack\", StringType())\n",
    "myDf=_df.withColumn(\"attackB\", attack_udf(_df.attack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- attack: string (nullable = true)\n",
      " |-- dst_bytes: long (nullable = true)\n",
      " |-- duration: long (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- protocol: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- src_bytes: long (nullable = true)\n",
      " |-- attackB: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "네트워크 침입 attack을 세분화하여 normal, dos, r2l, u2r, probling으로 **5종류**로 구분한다.\n",
    "구분 문자열이 **점('.')**으로 끝난다는 점에 주의하다.\n",
    "\n",
    "attack 4종류 | 설명 | 41번째 열\n",
    "-----|-----|-----\n",
    "DOS | denial-of-service, e.g. syn flood | back, land, neptune, pod, smurf, teardrop\n",
    "R2L | unauthorized access from a remote machine | ftp_write, guess_passwd, imap, multihop, phf, spy, warezclient, warezmaster\n",
    "U2R | unauthorized access to local superuser (root) privileges | buffer_overflow, loadmodule, perl, rootkit\n",
    "probing | surveillance and other probing | ipsweep, nmap, portsweep, satan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "위 표에 따라 ```udf()``` 함수를 사용해서 if문으로 'noraml' 및 'attack'을 총 5가지 종류로 구분한다.\n",
    "반환 값은 ```StringType()```이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "def classify41(s):\n",
    "    _5=\"\"\n",
    "    if s==\"normal.\":\n",
    "        _5=\"normal\"\n",
    "    elif s==\"back.\" or s==\"land.\" or s==\"neptune.\" or s==\"pod.\" or s==\"smurf.\" or s==\"teardrop.\":\n",
    "        _5=\"dos\"\n",
    "    elif s==\"ftp_write.\" or s==\"guess_passwd.\" or s==\"imap.\" or s==\"multihop.\" or s==\"phf.\" or\\\n",
    "        s==\"spy.\" or s==\"warezclient.\" or s==\"warezmaster.\":\n",
    "        _5=\"r2l\"\n",
    "    elif s==\"buffer_overflow.\" or s==\"loadmodule.\" or s==\"perl.\" or s==\"rootkit.\":\n",
    "        _5=\"u2r\"\n",
    "    elif s==\"ipsweep.\" or s==\"nmap.\" or s==\"portsweep.\" or s==\"satan.\":\n",
    "        _5=\"probing\"\n",
    "    return _5\n",
    "\n",
    "attack5_udf = udf(classify41, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "myDf=myDf.withColumn(\"attack5\", attack5_udf(_df.attack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- attack: string (nullable = true)\n",
      " |-- dst_bytes: long (nullable = true)\n",
      " |-- duration: long (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- protocol: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- src_bytes: long (nullable = true)\n",
      " |-- attackB: string (nullable = true)\n",
      " |-- attack5: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "잘 분류되었는지 일부 데이터를 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------+----+--------+-------+---------+-------+-------+\n",
      "| attack|dst_bytes|duration|flag|protocol|service|src_bytes|attackB|attack5|\n",
      "+-------+---------+--------+----+--------+-------+---------+-------+-------+\n",
      "|normal.|     5450|       0|  SF|     tcp|   http|      181| normal| normal|\n",
      "|normal.|      486|       0|  SF|     tcp|   http|      239| normal| normal|\n",
      "|normal.|     1337|       0|  SF|     tcp|   http|      235| normal| normal|\n",
      "|normal.|     1337|       0|  SF|     tcp|   http|      219| normal| normal|\n",
      "|normal.|     2032|       0|  SF|     tcp|   http|      217| normal| normal|\n",
      "+-------+---------+--------+----+--------+-------+---------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### attack, normal 특징 분석\n",
    "\n",
    "```attack5``` 별로 건수를 세어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|attack5| count|\n",
      "+-------+------+\n",
      "|probing|  4107|\n",
      "|    u2r|    52|\n",
      "| normal| 97278|\n",
      "|    r2l|  1126|\n",
      "|    dos|391458|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupBy('attack5').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```attack5``` 별로 공격의 특징을 분석해보자. 어떤 ```protocol```, ```src_bytes```, ```duration```이 어떤지 계산할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|protocol| count|\n",
      "+--------+------+\n",
      "|     tcp|190065|\n",
      "|     udp| 20354|\n",
      "|    icmp|283602|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupBy(\"protocol\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+\n",
      "|attackB|protocol| count|\n",
      "+-------+--------+------+\n",
      "| normal|     udp| 19177|\n",
      "| normal|    icmp|  1288|\n",
      "| normal|     tcp| 76813|\n",
      "| attack|    icmp|282314|\n",
      "| attack|     tcp|113252|\n",
      "| attack|     udp|  1177|\n",
      "+-------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupBy('attackB','protocol').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+-----+\n",
      "|attackB|  icmp|   tcp|  udp|\n",
      "+-------+------+------+-----+\n",
      "| normal|  1288| 76813|19177|\n",
      "| attack|282314|113252| 1177|\n",
      "+-------+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupBy('attackB').pivot('protocol').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+\n",
      "|attack5|              icmp|               tcp|               udp|\n",
      "+-------+------------------+------------------+------------------+\n",
      "|probing|10.700793650793651| 261454.6003016591|25.235897435897435|\n",
      "|    u2r|              null| 960.8979591836735|13.333333333333334|\n",
      "| normal| 91.47049689440993|1439.3120305156679| 98.01220211711947|\n",
      "|    r2l|              null|271972.57460035523|              null|\n",
      "|    dos| 936.2672084368129| 1090.303422435458|              28.0|\n",
      "+-------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupBy('attack5').pivot('protocol').avg('src_bytes').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|attack5|       avg(duration)|\n",
      "+-------+--------------------+\n",
      "|probing|   485.0299488677867|\n",
      "|    u2r|    80.9423076923077|\n",
      "| normal|  216.65732231336992|\n",
      "|    r2l|   559.7522202486679|\n",
      "|    dos|7.254929008986916E-4|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupBy('attack5').avg('duration').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------+---+\n",
      "|attackB|icmp|    tcp|udp|\n",
      "+-------+----+-------+---+\n",
      "| normal|   0|5134218|516|\n",
      "| attack|   0|5155468| 74|\n",
      "+-------+----+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "myDf.groupBy('attackB').pivot('protocol').agg(F.max('dst_bytes')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "좀 더 세밀한 조건으로 ```duration>1000)```, ```dst_bytes==0```인 경우의 건수를 계산할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|protocol|count|\n",
      "+--------+-----+\n",
      "|     tcp|  139|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.select(\"protocol\", \"duration\", \"dst_bytes\")\\\n",
    "    .filter(_df.duration>1000)\\\n",
    "    .filter(_df.dst_bytes==0)\\\n",
    "    .groupBy(\"protocol\")\\\n",
    "    .count()\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### SQL\n",
    "\n",
    "SQL을 사용해보자. 위에 사용했던 ```_df```에서 임시 테이블 ```_tab```을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_df.registerTempTable(\"_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tcp_interactions = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT duration, dst_bytes FROM _tab\n",
    "    WHERE protocol = 'tcp' AND duration > 1000 AND dst_bytes = 0\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|duration|dst_bytes|\n",
      "+--------+---------+\n",
      "|    5057|        0|\n",
      "|    5059|        0|\n",
      "|    5051|        0|\n",
      "|    5056|        0|\n",
      "|    5051|        0|\n",
      "+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tcp_interactions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "tcp_interactions_out = tcp_interactions.rdd\\\n",
    "    .map(lambda p: \"Duration: {}, Dest. bytes: {}\".format(p.duration, p.dst_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 5057, Dest. bytes: 0\n",
      "Duration: 5043, Dest. bytes: 0\n",
      "Duration: 5046, Dest. bytes: 0\n",
      "Duration: 5051, Dest. bytes: 0\n",
      "Duration: 5057, Dest. bytes: 0\n",
      "Duration: 5063, Dest. bytes: 0\n",
      "Duration: 42448, Dest. bytes: 0\n",
      "Duration: 40121, Dest. bytes: 0\n",
      "Duration: 31709, Dest. bytes: 0\n",
      "Duration: 30619, Dest. bytes: 0\n",
      "Duration: 22616, Dest. bytes: 0\n",
      "Duration: 21455, Dest. bytes: 0\n",
      "Duration: 13998, Dest. bytes: 0\n",
      "Duration: 12933, Dest. bytes: 0\n"
     ]
    }
   ],
   "source": [
    "for i,ti_out in enumerate(tcp_interactions_out.collect()):\n",
    "    if(i%10==0):\n",
    "        print ti_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-2: Twitter JSON 데이터 읽기\n",
    "\n",
    "* [nok] 현재 디렉토리 _tweet.json\n",
    "    * src/ds_twitter_3.py로 변경 (ds_twitter_3.json으로 저장)\n",
    "\n",
    "\n",
    "\n",
    "* Twitter JSON을 읽을 경우\n",
    "\n",
    "구분 | 예\n",
    "-------|-------\n",
    "unicode를 사용하면 backslash | \"{\\\"created_at\\\":\\\"Sun Nov 13 00:05:19 +0000 2016\\\"\n",
    "보통 | {\"created_at\":\"Sun Nov 13 00:05:19 +0000 2016\"\n",
    "\n",
    "* allowBackslashEscapingAnyCharacter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "json 파일을 읽어서 DataFrame을 생성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "t2df= spark.read.json(os.path.join(\"src\",\"ds_twitter_seoul_3.json\"))\n",
    "print type(t2df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "트윗의 'id','lang','text' 컬럼만을 선택해서 한 줄을 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res=t2df.select('id','lang','text').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801657325836763136 en RT @soompi: #SEVENTEEN’s Mingyu, Jin Se Yeon, And Leeteuk To MC For 2016 Super Seoul Dream Concert \n",
      "https://t.co/1XRSaRBbE0 https://t.co/fi…\n"
     ]
    }
   ],
   "source": [
    "for e in res:\n",
    "    print e['id'],e['lang'],e['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "twitterDF= spark.read.json(os.path.join(\"src\",\"ds_twitter_1_noquote.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contributors: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- entities: struct (nullable = true)\n",
      " |    |-- hashtags: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- symbols: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- urls: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- user_mentions: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- favorite_count: long (nullable = true)\n",
      " |-- favorited: boolean (nullable = true)\n",
      " |-- geo: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- id_str: string (nullable = true)\n",
      " |-- in_reply_to_screen_name: string (nullable = true)\n",
      " |-- in_reply_to_status_id: string (nullable = true)\n",
      " |-- in_reply_to_status_id_str: string (nullable = true)\n",
      " |-- in_reply_to_user_id: string (nullable = true)\n",
      " |-- in_reply_to_user_id_str: string (nullable = true)\n",
      " |-- is_quote_status: boolean (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- place: string (nullable = true)\n",
      " |-- retweet_count: long (nullable = true)\n",
      " |-- retweeted: boolean (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- truncated: boolean (nullable = true)\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- contributors_enabled: boolean (nullable = true)\n",
      " |    |-- created_at: string (nullable = true)\n",
      " |    |-- default_profile: boolean (nullable = true)\n",
      " |    |-- default_profile_image: boolean (nullable = true)\n",
      " |    |-- description: string (nullable = true)\n",
      " |    |-- entities: struct (nullable = true)\n",
      " |    |    |-- description: struct (nullable = true)\n",
      " |    |    |    |-- urls: array (nullable = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |-- favourites_count: long (nullable = true)\n",
      " |    |-- follow_request_sent: boolean (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- following: boolean (nullable = true)\n",
      " |    |-- friends_count: long (nullable = true)\n",
      " |    |-- geo_enabled: boolean (nullable = true)\n",
      " |    |-- has_extended_profile: boolean (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- id_str: string (nullable = true)\n",
      " |    |-- is_translation_enabled: boolean (nullable = true)\n",
      " |    |-- is_translator: boolean (nullable = true)\n",
      " |    |-- lang: string (nullable = true)\n",
      " |    |-- listed_count: long (nullable = true)\n",
      " |    |-- location: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- notifications: boolean (nullable = true)\n",
      " |    |-- profile_background_color: string (nullable = true)\n",
      " |    |-- profile_background_image_url: string (nullable = true)\n",
      " |    |-- profile_background_image_url_https: string (nullable = true)\n",
      " |    |-- profile_background_tile: boolean (nullable = true)\n",
      " |    |-- profile_image_url: string (nullable = true)\n",
      " |    |-- profile_image_url_https: string (nullable = true)\n",
      " |    |-- profile_link_color: string (nullable = true)\n",
      " |    |-- profile_sidebar_border_color: string (nullable = true)\n",
      " |    |-- profile_sidebar_fill_color: string (nullable = true)\n",
      " |    |-- profile_text_color: string (nullable = true)\n",
      " |    |-- profile_use_background_image: boolean (nullable = true)\n",
      " |    |-- protected: boolean (nullable = true)\n",
      " |    |-- screen_name: string (nullable = true)\n",
      " |    |-- statuses_count: long (nullable = true)\n",
      " |    |-- time_zone: string (nullable = true)\n",
      " |    |-- translator_type: string (nullable = true)\n",
      " |    |-- url: string (nullable = true)\n",
      " |    |-- utc_offset: string (nullable = true)\n",
      " |    |-- verified: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitterDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|           text|\n",
      "+---------------+\n",
      "|Hello 21 160924|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitterDF.select('text').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|           text|\n",
      "+---------------+\n",
      "|Hello 21 160924|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitterDF.registerTempTable(\"twitter\")\n",
    "spark.sql(\"select text from twitter\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-3: 뉴욕에서 출생한 신생아 분석\n",
    "\n",
    "### 뉴욕에서 출생한 신생아가 년도별 성별에 차이가 있을까?\n",
    "\n",
    "뉴욕에서 2007년 출생한 유아의 기록이다.\n",
    "https://health.data.ny.gov/Health/Baby-Names-Beginning-2007/jxy9-yhdk\n",
    "\n",
    "Column Name | 설명\n",
    "-----|-----\n",
    "Year | Year data was collected.\n",
    "First Name | 이름\n",
    "County | Location where the baby’s mother resided as stated on their birth certificate.\n",
    "Sex | F= Female M= Male\n",
    "Count | Five (5) or more of the same baby name in a county outside of NYC; Ten (10) or more of the same baby name in a NYC borough.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "https://catalog.data.gov/dataset\n",
    "\n",
    "data/dataGovbabyNames.json 은 메타데이터가 있어서 kaggle.com의 baby names를 사용해서 분석?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```requests.get()``` 함수를 사용해서 url로부터 데이터를 읽어 오면 string이다 (예: ```r.iter_lines()```하면 문자 1개씩 가져옴). response를 json으로 읽으면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "_url=\"https://health.data.ny.gov/api/views/jxy9-yhdk/rows.json?accessType=DOWNLOAD\"\n",
    "_json=requests.get(_url).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* json데이터는 meta, data로 구분해서 만들어져 있다.\n",
    "    * data는 Python List로 구성되어 있다 (앞서 Python dict에서 생성하는 경우와 비교해 본다.)\n",
    "    * data의 건수는 52,252건\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'meta', u'data']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_json.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235510\n"
     ]
    }
   ],
   "source": [
    "_jsonList=_json['data']\n",
    "print len(_jsonList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'row-56s8~p76n_zvtk',\n",
       " u'00000000-0000-0000-39E1-487334739D3C',\n",
       " 0,\n",
       " 1527713235,\n",
       " None,\n",
       " 1527713235,\n",
       " None,\n",
       " u'{ }',\n",
       " u'2016',\n",
       " u'DAVID',\n",
       " u'Kings',\n",
       " u'M',\n",
       " u'231']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_json['data'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Python list로부터 Spark Dataframe을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145570"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df=spark.createDataFrame(_json['data'])\n",
    "_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* schema를 정하지 않았으므로 임의로 생성된 속성을 사용하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      " |-- _4: long (nullable = true)\n",
      " |-- _5: string (nullable = true)\n",
      " |-- _6: long (nullable = true)\n",
      " |-- _7: string (nullable = true)\n",
      " |-- _8: string (nullable = true)\n",
      " |-- _9: string (nullable = true)\n",
      " |-- _10: string (nullable = true)\n",
      " |-- _11: string (nullable = true)\n",
      " |-- _12: string (nullable = true)\n",
      " |-- _13: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 컬럼명을 새로 정의한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "_myDf = _df.select('_9','_10','_11','_12','_13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- fname: string (nullable = true)\n",
      " |-- county: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n",
      "+----+-------+-----------+---+-----+\n",
      "|year|  fname|     county|sex|count|\n",
      "+----+-------+-----------+---+-----+\n",
      "|2013|  GAVIN|ST LAWRENCE|  M|    9|\n",
      "|2013|   LEVI|ST LAWRENCE|  M|    9|\n",
      "|2013|  LOGAN|   NEW YORK|  M|   44|\n",
      "|2013| HUDSON|   NEW YORK|  M|   49|\n",
      "|2013|GABRIEL|   NEW YORK|  M|   50|\n",
      "+----+-------+-----------+---+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf=_myDf.withColumn('count',_df['_13'].cast(\"integer\")).drop('_13')\n",
    "_myDf=_myDf.withColumnRenamed('_9','year')\n",
    "_myDf=_myDf.withColumnRenamed('_10','fname')\n",
    "_myDf=_myDf.withColumnRenamed('_11','county')\n",
    "_myDf=_myDf.withColumnRenamed('_12','sex')\n",
    "_myDf.printSchema()\n",
    "_myDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----------+---+-----+\n",
      "|year|fname|     county|sex|count|\n",
      "+----+-----+-----------+---+-----+\n",
      "|2013|GAVIN|ST LAWRENCE|  M|    9|\n",
      "|2013|GAVIN|    SUFFOLK|  M|   54|\n",
      "+----+-----+-----------+---+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.filter(_myDf['fname'] == u'GAVIN').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Sql을 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| fname|\n",
      "+------+\n",
      "|MILANA|\n",
      "|  JADE|\n",
      "|  ANNA|\n",
      "|HUNTER|\n",
      "|ANJALI|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.registerTempTable(\"babyNames\")\n",
    "spark.sql(\"select distinct(fname) from babyNames\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 년도별 성별 빈도수를 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|year|    F|    M|\n",
      "+----+-----+-----+\n",
      "|2007| 3002| 3365|\n",
      "|2008| 3039| 3442|\n",
      "|2009| 2917| 3395|\n",
      "|2010| 2925| 3267|\n",
      "|2011| 2918| 3298|\n",
      "|2012| 2872| 3292|\n",
      "|2013| 2836| 3322|\n",
      "|2014| 4121| 4241|\n",
      "|2015|50803|42515|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.sort('year').groupBy('year').pivot('sex').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-4: 우버 택시의 운행기록 분석\n",
    "\n",
    "* 질문: 2015년 가장 많은 운행을 한 base는?\n",
    "https://github.com/tmcgrath/spark-with-python-course/blob/master/Spark-SQL-CSV-with-Python.ipynb\n",
    "\n",
    "* fivethirtyeight\n",
    "    * git clone https://github.com/fivethirtyeight/uber-tlc-foil-response.git\n",
    "        daily Uber trip statistics in January and February 2015\n",
    "\n",
    "dispatching_base_number | date | active_vehicles | trips\n",
    "----------|----------|----------|----------\n",
    "B02512 | 1/1/2015 | 190 | 1132\n",
    "B02765 | 1/1/2015 | 225 | 1765\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "data_home=os.path.join(os.environ['HOME'],\"Code/git/else/uber-tlc-foil-response\")\n",
    "filePath=os.path.join(data_home,\"Uber-Jan-Feb-FOIL.csv\")\n",
    "\n",
    "_rdd = spark.sparkContext.textFile(filePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* header는 속성 명을 가지고 있다.\n",
    "* header를 제외하고 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354\n",
      "B02512,1/1/2015,190,1132\n"
     ]
    }
   ],
   "source": [
    "header = _rdd.first() #extract header\n",
    "_rdd = _rdd.filter(lambda x:x != header)\n",
    "\n",
    "print _rdd.count()\n",
    "print _rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* csv는 comma seperated 형식이므로, ','로 분리\n",
    "* 첫번째 열에서 key값을 추출한다 (header값 포함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_myRdd = _rdd.map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'B02617', u'B02682', u'B02598', u'B02765', u'B02512', u'B02764']\n"
     ]
    }
   ],
   "source": [
    "_row0keys=_myRdd.map(lambda row: row[0]).distinct().collect()\n",
    "\n",
    "print _row0keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myRdd.filter(lambda row: \"B02512\" in row).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* B02512인 경우, trips가 2000보다 큰 레코드 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'B02512', u'1/30/2015', u'256', u'2016'],\n",
       " [u'B02512', u'2/5/2015', u'264', u'2022'],\n",
       " [u'B02512', u'2/12/2015', u'269', u'2092'],\n",
       " [u'B02512', u'2/13/2015', u'281', u'2408'],\n",
       " [u'B02512', u'2/14/2015', u'236', u'2055'],\n",
       " [u'B02512', u'2/19/2015', u'250', u'2120'],\n",
       " [u'B02512', u'2/20/2015', u'272', u'2380'],\n",
       " [u'B02512', u'2/21/2015', u'238', u'2149'],\n",
       " [u'B02512', u'2/27/2015', u'272', u'2056']]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myRdd.filter(lambda row: \"B02512\" in row).filter(lambda row: int(row[3])>2000).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'B02617', 725025),\n",
       " (u'B02682', 662509),\n",
       " (u'B02598', 540791),\n",
       " (u'B02765', 193670),\n",
       " (u'B02512', 93786),\n",
       " (u'B02764', 1914449)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myRdd.map(lambda x: (x[0], int(x[3]))).reduceByKey(lambda k,v: k + v).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def countPartitions(id,iterator): \n",
    "    c = 0 \n",
    "    for _ in iterator: \n",
    "        c += 1\n",
    "    yield (id,c) \n",
    "_wc=wc.mapPartitions(countPartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354\n",
      "+------+--------+----+-----+\n",
      "|    _1|      _2|  _3|   _4|\n",
      "+------+--------+----+-----+\n",
      "|B02512|1/1/2015| 190| 1132|\n",
      "|B02765|1/1/2015| 225| 1765|\n",
      "|B02764|1/1/2015|3427|29421|\n",
      "+------+--------+----+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n",
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: string (nullable = true)\n",
      " |-- _4: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "_df=spark.createDataFrame(_myRdd)\n",
    "print _df.count()\n",
    "print _df.show(3)\n",
    "print _df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- baseNum: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- activeVechicles: string (nullable = true)\n",
      " |-- trips: integer (nullable = true)\n",
      "\n",
      "+-------+--------+---------------+-----+\n",
      "|baseNum|    date|activeVechicles|trips|\n",
      "+-------+--------+---------------+-----+\n",
      "| B02512|1/1/2015|            190| 1132|\n",
      "| B02765|1/1/2015|            225| 1765|\n",
      "| B02764|1/1/2015|           3427|29421|\n",
      "| B02682|1/1/2015|            945| 7679|\n",
      "| B02617|1/1/2015|           1228| 9537|\n",
      "+-------+--------+---------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf=_df.withColumn('trips',_df['_4'].cast(\"integer\")).drop('_4')\n",
    "_myDf=_myDf.withColumnRenamed('_1','baseNum')\n",
    "_myDf=_myDf.withColumnRenamed('_2','date')\n",
    "_myDf=_myDf.withColumnRenamed('_3','activeVechicles')\n",
    "_myDf.printSchema()\n",
    "_myDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+\n",
      "|baseNum|trips|count|\n",
      "+-------+-----+-----+\n",
      "| B02682|13355|    1|\n",
      "| B02765| 2623|    1|\n",
      "| B02617|12016|    1|\n",
      "| B02764|36318|    1|\n",
      "| B02682| 4414|    1|\n",
      "| B02617| 4325|    1|\n",
      "| B02764|31173|    1|\n",
      "| B02617|10664|    1|\n",
      "| B02598|11897|    1|\n",
      "| B02617|11401|    1|\n",
      "| B02512| 1831|    1|\n",
      "| B02765| 7658|    1|\n",
      "| B02617|13688|    1|\n",
      "| B02764|31957|    1|\n",
      "| B02682| 8010|    1|\n",
      "| B02764|33756|    1|\n",
      "| B02512| 2056|    1|\n",
      "| B02512|  791|    1|\n",
      "| B02512| 2380|    1|\n",
      "| B02598| 2957|    1|\n",
      "+-------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.groupBy('baseNum','trips').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-5: JDBC를 사용해서 데이터 읽기\n",
    "\n",
    "* jdbc를 연결하는 방식은 Java와 같이 'driver', 'url'을 설정하면 된다.\n",
    "* 여기서는 sqlite를 실습한다.\n",
    "\n",
    "* sqlite와 같이 Spark 패키지가 없는 경우, jar를 다운로드하고\n",
    "설정파일 conf/spark-defaults.conf에 'spark.driver.extraClassPath'를 추가한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jsl/Code/git/bb/jsl/pyds/lib/sqlite-jdbc-3.14.2.jar\n"
     ]
    }
   ],
   "source": [
    "print spark.conf.get('spark.driver.extraClassPath')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|cid|c_name|\n",
      "+---+------+\n",
      "|  1|   Kim|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.format('jdbc')\\\n",
    "    .options(\n",
    "        url=\"jdbc:sqlite:/home/jsl/Code/git/bb/sd/src/ds_sql_hello.db\",\n",
    "        dbtable=\"customer\",\n",
    "        driver=\"org.sqlite.JDBC\"\n",
    "    ).load()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MySql\n",
    "\n",
    "* Spark 패키지를 제공하지 않는다. jar를 'spark.driver.extraClassPath'에 추가하고 사용한다.\n",
    "\n",
    "* 읽기\n",
    "```python\n",
    "dfmysql = sqlContext.read.format('jdbc')\\\n",
    "        .options(\n",
    "          url='jdbc:mysql://localhost/database_name',\n",
    "          driver='com.mysql.jdbc.Driver',\n",
    "          dbtable='SourceTableName',\n",
    "          user='your_user_name',\n",
    "          password='your_password')\\\n",
    "        .load()\n",
    "```\n",
    "\n",
    "* 쓰기\n",
    "```python\n",
    "destination_df.write.format('jdbc')\\\n",
    "        .options(\n",
    "          url='jdbc:mysql://localhost/database_name',\n",
    "          driver='com.mysql.jdbc.Driver',\n",
    "          dbtable='DestinationTableName',\n",
    "          user='your_user_name',\n",
    "          password='your_password')\\\n",
    "        .mode('append')\\\n",
    "        .save()\n",
    "```\n",
    "\n",
    "```python\n",
    "bin/spark-submit --jars mysql-connector-java-5.1.40-bin.jar\n",
    "      /path_to_your_program/spark_database.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.7 MongoDB Spark connector\n",
    "\n",
    "* Spark에서 MongoDB에 저장된 데이터를 읽어 온다.\n",
    "* 참조: pymongo-spark (Spark와 PyMongo를 사용하는 Python 라이브러리, 설치하려면 pip install pymongo-spark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.7.1 설정\n",
    "\n",
    "* 참조 https://docs.mongodb.com/spark-connector/\n",
    "* 설정파일 conf/spark-defaults.conf 수정\n",
    "    * Spark 버전에 맞는 jar를 선택한다.\n",
    "    * MongoDB<3.2인 경우, spark.mongodb.input.partitioner가 필요하다.\n",
    "    * packages 여러 개를 넣을 경우에는 컴마로 분리한다.\n",
    "\n",
    "```python\n",
    "$vim conf/spark-defaults.conf \n",
    "spark.jars.packages=org.mongodb.spark:mongo-spark-connector_2.10:1.1.0\n",
    "spark.mongodb.input.partitioner=MongoPaginateBySizePartitioner\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graphframes:graphframes:0.4.0-spark2.0-s_2.11,org.mongodb.spark:mongo-spark-connector_2.10:2.0.0,com.databricks:spark-csv_2.11:1.5.0\n"
     ]
    }
   ],
   "source": [
    "print spark.conf.get('spark.jars.packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.7.2 uri\n",
    "\n",
    "* SparkSession에 uri를 설정할 수 있다. 연결에 필요한 ip, database, collection을 정의한다.\n",
    "\n",
    "```python\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/myDB.ds_spark_df_mongo\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/myDB.ds_spark_df_mongo\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "* 또는 실행시점에 설정할 수 있다 (아래 참조)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.7.3 MongoDB Python API\n",
    "\n",
    "* format은 \"com.mongodb.spark.sql.DefaultSource\"로 설정한다.\n",
    "* 'option'을 사용해서 실행시점에 Database, Colleciton 명을 설정할 수 있다.\n",
    "\n",
    "구분 | 명령어 예\n",
    "-----|-----\n",
    "쓰기 | DataFrame.write.format(\"com.mongodb.spark.sql.DefaultSource\")\\<br>.mode(\"overwrite\")\\<br>.option(\"uri\",\"mongodb://127.0.0.1/myDB.ds_spark_ml\")\\<br>.save()\n",
    "읽기 | spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\<br>.option(\"uri\",\"mongodb://127.0.0.1/ds_twitter.seoul\")\\<br>.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.7.4 연습으로 쓰기, 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "people = spark.createDataFrame([(\"kim\",10),(\"lee\",20),(\"choi\",30),(\"park\",40)],[\"name\", \"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "people.write.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .mode(\"append\")\\\n",
    "    .option(\"uri\",\"mongodb://127.0.0.1/myDB.ds_spark_ml\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .option(\"uri\",\"mongodb://127.0.0.1/myDB.ds_spark_ml\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "| kim|\n",
      "| lee|\n",
      "|choi|\n",
      "+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('name').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|RT @always_gd: #B...|\n",
      "|RT @InfiniteUpdat...|\n",
      "|RT @InfiniteUpdat...|\n",
      "|RT @PartOfJimin: ...|\n",
      "|RT @MHDEFB: มาแล้...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .option(\"uri\",\"mongodb://127.0.0.1/ds_twitter.seoul\")\\\n",
    "    .load()\n",
    "df.select('text').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_id',\n",
       " 'contributors',\n",
       " 'coordinates',\n",
       " 'created_at',\n",
       " 'entities',\n",
       " 'extended_entities',\n",
       " 'favorite_count',\n",
       " 'favorited',\n",
       " 'geo',\n",
       " 'id',\n",
       " 'id_str',\n",
       " 'in_reply_to_screen_name',\n",
       " 'in_reply_to_status_id',\n",
       " 'in_reply_to_status_id_str',\n",
       " 'in_reply_to_user_id',\n",
       " 'in_reply_to_user_id_str',\n",
       " 'is_quote_status',\n",
       " 'lang',\n",
       " 'metadata',\n",
       " 'place',\n",
       " 'possibly_sensitive',\n",
       " 'quoted_status',\n",
       " 'quoted_status_id',\n",
       " 'quoted_status_id_str',\n",
       " 'retweet_count',\n",
       " 'retweeted',\n",
       " 'retweeted_status',\n",
       " 'source',\n",
       " 'text',\n",
       " 'truncated',\n",
       " 'user']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.8 spark-submit\n",
    "\n",
    "* spark-submit는 일괄실행 (self-contained app in quick-start 참조)\n",
    "\n",
    "* MongoDB를 사용하려면, spark-defaults.conf에 jar를 추가한다 (앞서 미리 설정하였다.)\n",
    "\n",
    "* spark-submit을 실행하기 전, 'conf/log4j.properties'를 수정 log level을 ERROR로 설정하였다.\n",
    "```python\n",
    "log4j.rootCategory=ERROR, console\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.8.1 간단한 작업\n",
    "\n",
    "* DataFrame 만들고, 출력하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_spark_sql.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_spark_sql.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "import pyspark\n",
    "\n",
    "def doIt():\n",
    "    d = [{'name': 'Alice', 'age': 1}]\n",
    "    print spark.createDataFrame(d).collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf=myConf)\\\n",
    "        .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jsl/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jsl/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "graphframes#graphframes added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.10 added as a dependency\n",
      "com.databricks#spark-csv_2.11 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.4.0-spark2.0-s_2.11 in spark-packages\n",
      "\tfound com.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 in central\n",
      "\tfound com.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.11.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.7 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.10;2.0.0 in central\n",
      "\tfound org.mongodb#mongo-java-driver;3.2.2 in central\n",
      "\tfound com.databricks#spark-csv_2.11;1.5.0 in central\n",
      "\tfound org.apache.commons#commons-csv;1.1 in central\n",
      "\tfound com.univocity#univocity-parsers;1.5.1 in central\n",
      ":: resolution report :: resolve 245ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-csv_2.11;1.5.0 from central in [default]\n",
      "\tcom.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 from central in [default]\n",
      "\tcom.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 from central in [default]\n",
      "\tcom.univocity#univocity-parsers;1.5.1 from central in [default]\n",
      "\tgraphframes#graphframes;0.4.0-spark2.0-s_2.11 from spark-packages in [default]\n",
      "\torg.apache.commons#commons-csv;1.1 from central in [default]\n",
      "\torg.mongodb#mongo-java-driver;3.2.2 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.10;2.0.0 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.11.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.7 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   10  |   0   |   0   |   0   ||   10  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 10 already retrieved (0kB/7ms)\n",
      "/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/session.py:316: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n",
      "[Row(age=1, name=u'Alice')]\n"
     ]
    }
   ],
   "source": [
    "!/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/bin/spark-submit src/ds_spark_sql.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.8.2 MongoDB\n",
    "\n",
    "* Database, Collection 읽기, 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_spark_mongo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_spark_mongo.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "import pyspark\n",
    "def doIt():\n",
    "    print \"---------RESULT-----------\"\n",
    "    print \"------mongodb write-------\"\n",
    "    myRdd = spark.sparkContext.parallelize([\n",
    "        (\"js\", 150),\n",
    "        (\"Gandalf\", 1000),\n",
    "        (\"Thorin\", 195),\n",
    "        (\"Balin\", 178),\n",
    "        (\"Kili\", 77),\n",
    "        (\"Dwalin\", 169),\n",
    "        (\"Oin\", 167),\n",
    "        (\"Gloin\", 158),\n",
    "        (\"Fili\", 82),\n",
    "        (\"Bombur\", None)\n",
    "    ])\n",
    "    myDf = spark.createDataFrame(myRdd, [\"name\", \"age\"])\n",
    "    print myDf\n",
    "    myDf.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"overwrite\").save()\n",
    "    print \"---------read-----------\"\n",
    "    df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "    print df.printSchema()\n",
    "    df.registerTempTable(\"myTable\")\n",
    "    myTab = spark.sql(\"SELECT name, age FROM myTable WHERE age >= 100\")\n",
    "    myTab.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/myDB.ds_spark_df_mongo\") \\\n",
    "        .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/myDB.ds_spark_df_mongo\") \\\n",
    "        .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jsl/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jsl/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "graphframes#graphframes added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.10 added as a dependency\n",
      "com.databricks#spark-csv_2.11 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.4.0-spark2.0-s_2.11 in spark-packages\n",
      "\tfound com.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 in central\n",
      "\tfound com.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.11.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.7 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.10;2.0.0 in central\n",
      "\tfound org.mongodb#mongo-java-driver;3.2.2 in central\n",
      "\tfound com.databricks#spark-csv_2.11;1.5.0 in central\n",
      "\tfound org.apache.commons#commons-csv;1.1 in central\n",
      "\tfound com.univocity#univocity-parsers;1.5.1 in central\n",
      ":: resolution report :: resolve 250ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-csv_2.11;1.5.0 from central in [default]\n",
      "\tcom.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 from central in [default]\n",
      "\tcom.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 from central in [default]\n",
      "\tcom.univocity#univocity-parsers;1.5.1 from central in [default]\n",
      "\tgraphframes#graphframes;0.4.0-spark2.0-s_2.11 from spark-packages in [default]\n",
      "\torg.apache.commons#commons-csv;1.1 from central in [default]\n",
      "\torg.mongodb#mongo-java-driver;3.2.2 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.10;2.0.0 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.11.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.7 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   10  |   0   |   0   |   0   ||   10  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 10 already retrieved (0kB/7ms)\n",
      "---------RESULT-----------\n",
      "------mongodb write-------\n",
      "DataFrame[name: string, age: bigint]\n",
      "---------read-----------\n",
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "None\n",
      "+-------+----+\n",
      "|   name| age|\n",
      "+-------+----+\n",
      "|     js| 150|\n",
      "|Gandalf|1000|\n",
      "| Thorin| 195|\n",
      "|  Balin| 178|\n",
      "| Dwalin| 169|\n",
      "|    Oin| 167|\n",
      "|  Gloin| 158|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/bin/spark-submit src/ds_spark_mongo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-6: MongoDB 저장된 열린데이터 읽어오는 spark-submit\n",
    "\n",
    "* MongoDB에 저장된 데이터 읽기\n",
    "\n",
    "구분 | 명\n",
    "-----|-----\n",
    "Database | ds_open_subwayPassengersDb\n",
    "Collection | db_open_subwayTable\n",
    "key | JSON 계층구조를 따라 읽는다. CardSubwayStatisticsService.row.RIDE_PASGR_NUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* MongoDB shell\n",
    "\n",
    "```python\n",
    "$ mongo\n",
    "> use ds_open_subwayPassengersDb\n",
    "switched to db ds_rest_subwayPassengers_mongo_db\n",
    "> show tables\n",
    "db_open_subwayTable\n",
    "system.indexes\n",
    "> db.db_open_subwayTable.find().limit(1)\n",
    "{ \"_id\" : ObjectId(\"57fa386ff5e6e94359c033e9\"), \"CardSubwayStatisticsService\" : { \"row\" : [ { \"COMMT\" : \"\", \"RIDE_PASGR_NUM\" : 111275, \"WORK_DT\" : \"20130723\", \"LINE_NUM\" : \"중앙선\", \"SUB_STA_NM\" : \"용문\", \"ALIGHT_PASGR_NUM\" : 108878, \"USE_MON\" : \"201306\" }, { \"COMMT\" : \"\", \"RIDE_PASGR_NUM\" : 11495, \"WORK_DT\" : \"20130723\", \"LINE_NUM\" : \"중앙선\", \"SUB_STA_NM\" : \"원덕\", \"ALIGHT_PASGR_NUM\" : 10964, \"USE_MON\" : \"201306\" }, { \"COMMT\" : \"\", \"RIDE_PASGR_NUM\" : 118103, \"WORK_DT\" : \"20130723\", \"LINE_NUM\" : \"중앙선\", \"SUB_STA_NM\" : \"양평\", \"ALIGHT_PASGR_NUM\" : 116604, \"USE_MON\" : \"201306\" }, { \"COMMT\" : \"\", \"RIDE_PASGR_NUM\" : 10590, \"WORK_DT\" : \"20130723\", \"LINE_NUM\" : \"중앙선\", \"SUB_STA_NM\" : \"오빈\", \"ALIGHT_PASGR_NUM\" : 10020, \"USE_MON\" : \"201306\" }, { \"COMMT\" : \"\", \"RIDE_PASGR_NUM\" : 26304, \"WORK_DT\" : \"20130723\", \"LINE_NUM\" : \"중앙선\", \"SUB_STA_NM\" : \"아신\", \"ALIGHT_PASGR_NUM\" : 26358, \"USE_MON\" : \"201306\" } ], \"RESULT\" : { \"MESSAGE\" : \"정상 처리되었습니다\", \"CODE\" : \"INFO-000\" }, \"list_total_count\" : 530 } }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* MongoDB에서 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CardSubwayStatisticsService: struct (nullable = true)\n",
      " |    |-- row: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- COMMT: string (nullable = true)\n",
      " |    |    |    |-- RIDE_PASGR_NUM: double (nullable = true)\n",
      " |    |    |    |-- WORK_DT: string (nullable = true)\n",
      " |    |    |    |-- LINE_NUM: string (nullable = true)\n",
      " |    |    |    |-- SUB_STA_NM: string (nullable = true)\n",
      " |    |    |    |-- ALIGHT_PASGR_NUM: double (nullable = true)\n",
      " |    |    |    |-- USE_MON: string (nullable = true)\n",
      " |    |-- RESULT: struct (nullable = true)\n",
      " |    |    |-- MESSAGE: string (nullable = true)\n",
      " |    |    |-- CODE: string (nullable = true)\n",
      " |    |-- list_total_count: integer (nullable = true)\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .option(\"uri\",\"mongodb://127.0.0.1/ds_open_subwayPassengersDb.db_open_subwayTable\")\\\n",
    "    .load()\n",
    "print df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      RIDE_PASGR_NUM|\n",
      "+--------------------+\n",
      "|[111275.0, 11495....|\n",
      "|[30281.0, 10832.0...|\n",
      "|[75553.0, 189783....|\n",
      "|[62789.0, 220544....|\n",
      "|[74486.0, 152381....|\n",
      "|[43418.0, 413533....|\n",
      "|[301856.0, 37978....|\n",
      "|[146909.0, 227066...|\n",
      "|[152275.0, 285263...|\n",
      "|[161298.0, 117720...|\n",
      "|[95996.0, 39150.0...|\n",
      "|[59900.0, 201035....|\n",
      "|[228737.0, 108938...|\n",
      "|[164574.0, 481998...|\n",
      "|[748205.0, 817657...|\n",
      "|[206631.0, 188076...|\n",
      "|[112991.0, 111791...|\n",
      "|[225105.0, 938296...|\n",
      "|[175909.0, 271844...|\n",
      "|[45047.0, 126837....|\n",
      "+--------------------+\n",
      "\n",
      "None\n",
      "Row(RIDE_PASGR_NUM=[111275.0, 11495.0, 118103.0, 10590.0, 26304.0])\n",
      "Row(RIDE_PASGR_NUM=[111275.0, 11495.0, 118103.0, 10590.0, 26304.0])\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable(\"mySubway\")\n",
    "myTab = spark.sql(\"SELECT CardSubwayStatisticsService.row.RIDE_PASGR_NUM FROM mySubway\")\n",
    "#print type(myTab)\n",
    "print myTab.show()\n",
    "print myTab.first()\n",
    "print myTab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_spark_subway.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_spark_subway.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "import pyspark\n",
    "def doIt():\n",
    "    print \"---------read-----------\"\n",
    "    df=spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "    #df = sqlContext.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "    print df.printSchema()\n",
    "    df.registerTempTable(\"mySubway\")\n",
    "    myTab = spark.sql(\"SELECT CardSubwayStatisticsService.row.RIDE_PASGR_NUM FROM mySubway\")\n",
    "    #print type(myTab)\n",
    "    print myTab.show()\n",
    "if __name__ == \"__main__\":\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/ds_open_subwayPassengersDb.db_open_subwayTable\") \\\n",
    "        .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jsl/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jsl/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "graphframes#graphframes added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.10 added as a dependency\n",
      "com.databricks#spark-csv_2.11 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.4.0-spark2.0-s_2.11 in spark-packages\n",
      "\tfound com.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 in central\n",
      "\tfound com.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.11.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.7 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.10;2.0.0 in central\n",
      "\tfound org.mongodb#mongo-java-driver;3.2.2 in central\n",
      "\tfound com.databricks#spark-csv_2.11;1.5.0 in central\n",
      "\tfound org.apache.commons#commons-csv;1.1 in central\n",
      "\tfound com.univocity#univocity-parsers;1.5.1 in central\n",
      ":: resolution report :: resolve 244ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-csv_2.11;1.5.0 from central in [default]\n",
      "\tcom.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 from central in [default]\n",
      "\tcom.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 from central in [default]\n",
      "\tcom.univocity#univocity-parsers;1.5.1 from central in [default]\n",
      "\tgraphframes#graphframes;0.4.0-spark2.0-s_2.11 from spark-packages in [default]\n",
      "\torg.apache.commons#commons-csv;1.1 from central in [default]\n",
      "\torg.mongodb#mongo-java-driver;3.2.2 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.10;2.0.0 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.11.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.7 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   10  |   0   |   0   |   0   ||   10  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 10 already retrieved (0kB/7ms)\n",
      "---------read-----------\n",
      "root\n",
      " |-- CardSubwayStatisticsService: struct (nullable = true)\n",
      " |    |-- row: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- COMMT: string (nullable = true)\n",
      " |    |    |    |-- RIDE_PASGR_NUM: double (nullable = true)\n",
      " |    |    |    |-- WORK_DT: string (nullable = true)\n",
      " |    |    |    |-- LINE_NUM: string (nullable = true)\n",
      " |    |    |    |-- SUB_STA_NM: string (nullable = true)\n",
      " |    |    |    |-- ALIGHT_PASGR_NUM: double (nullable = true)\n",
      " |    |    |    |-- USE_MON: string (nullable = true)\n",
      " |    |-- RESULT: struct (nullable = true)\n",
      " |    |    |-- MESSAGE: string (nullable = true)\n",
      " |    |    |-- CODE: string (nullable = true)\n",
      " |    |-- list_total_count: integer (nullable = true)\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      "\n",
      "None\n",
      "+--------------------+\n",
      "|      RIDE_PASGR_NUM|\n",
      "+--------------------+\n",
      "|[111275.0, 11495....|\n",
      "|[30281.0, 10832.0...|\n",
      "|[75553.0, 189783....|\n",
      "|[62789.0, 220544....|\n",
      "|[74486.0, 152381....|\n",
      "|[43418.0, 413533....|\n",
      "|[301856.0, 37978....|\n",
      "|[146909.0, 227066...|\n",
      "|[152275.0, 285263...|\n",
      "|[161298.0, 117720...|\n",
      "|[95996.0, 39150.0...|\n",
      "|[59900.0, 201035....|\n",
      "|[228737.0, 108938...|\n",
      "|[164574.0, 481998...|\n",
      "|[748205.0, 817657...|\n",
      "|[206631.0, 188076...|\n",
      "|[112991.0, 111791...|\n",
      "|[225105.0, 938296...|\n",
      "|[175909.0, 271844...|\n",
      "|[45047.0, 126837....|\n",
      "+--------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "!/home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/bin/spark-submit src/ds_spark_subway.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jsl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print os.environ['HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filen=\"국내유학중인%2B외국인유학생%2B데이터%282016년%2B12월기준%29.csv\"\n",
    "filen=\"OverseasStudentsInKorea20161229.csv\"\n",
    "myRdd=spark.sparkContext.textFile(os.path.join(os.environ['HOME'],\"Public\",filen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "����\n",
      "����\n",
      "������\n",
      "ü���ڰ�\n",
      "�б���\n",
      "ü���� �õ�\n",
      "ü���� �ñ���\n",
      "<type 'unicode'>\n"
     ]
    }
   ],
   "source": [
    "data=myRdd.first()\n",
    "for i in data.split(','):\n",
    "    print i\n",
    "print type(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header='true', inferschema='true').load(os.path.join(os.environ['HOME'],\"Public\",filen))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ����: string (nullable = true)\n",
      " |-- ����: string (nullable = true)\n",
      " |-- ������: string (nullable = true)\n",
      " |-- ü���ڰ�: string (nullable = true)\n",
      " |-- �б���: string (nullable = true)\n",
      " |-- ü���� �õ�: string (nullable = true)\n",
      " |-- ü���� �ñ���: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
