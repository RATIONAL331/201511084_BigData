{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "home=os.path.expanduser(\"~\")\n",
    "os.environ[\"SPARK_HOME\"]=os.path.join(home, 'spark-2.0.0-bin-hadoop2.7')\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder.master(\"local\").appName(\"myApp\").config(conf=myConf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문제 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When I find myself in times of trouble\n",
      "Mother Mary comes to me\n",
      "Speaking words of wisdom, let it be\n",
      "And in my hour of darkness\n",
      "She is standing right in front of me\n",
      "\n",
      "\n",
      "(7, 'BE')\n",
      "(7, 'LET')\n",
      "(7, 'IT')\n",
      "(3, 'WISDOM,')\n",
      "(3, 'WORDS')\n",
      "(3, 'IN')\n",
      "(2, 'ME')\n",
      "(2, 'SPEAKING')\n",
      "(1, 'AND')\n",
      "(1, 'STANDING')\n",
      "(1, 'I')\n",
      "(1, 'FRONT')\n",
      "(1, 'TO')\n",
      "(1, 'WHISPER')\n",
      "(1, 'MYSELF')\n",
      "(1, 'TROUBLE')\n",
      "(1, 'RIGHT')\n",
      "(1, 'WHEN')\n",
      "(1, 'MY')\n",
      "(1, 'SHE')\n",
      "(1, 'DARKNESS')\n",
      "(1, 'FIND')\n",
      "(1, 'MARY')\n",
      "(1, 'HOUR')\n",
      "(1, 'COMES')\n",
      "(1, 'MOTHER')\n",
      "(1, 'TIMES')\n"
     ]
    }
   ],
   "source": [
    "sing = [\n",
    "\"When I find myself in times of trouble\",\n",
    "\"Mother Mary comes to me\",\n",
    "\"Speaking words of wisdom, let it be\",\n",
    "\"And in my hour of darkness\",\n",
    "\"She is standing right in front of me\",\n",
    "\"Speaking words of wisdom, let it be\",\n",
    "\"Let it be\",\n",
    "\"Let it be\",\n",
    "\"Let it be\",\n",
    "\"Let it be\",\n",
    "\"Whisper words of wisdom, let it be\"]\n",
    "\n",
    "myRdd=spark.sparkContext.parallelize(sing)\n",
    "\n",
    "for line in myRdd.take(5):\n",
    "    print line\n",
    "\n",
    "print '\\n'\n",
    "    \n",
    "stopwords = ['IS','AM','ARE','THE','FOR','A', 'AN', 'AT', 'OF']\n",
    "\n",
    "wordCount = myRdd\\\n",
    "    .flatMap(lambda x : x.split())\\\n",
    "    .filter(lambda x : x.upper() not in stopwords)\\\n",
    "    .map(lambda x : (x.upper(), 1))\\\n",
    "    .reduceByKey(lambda x, y : x + y)\\\n",
    "    .map(lambda x : (x[1], x[0]))\\\n",
    "    .sortByKey(False)\\\n",
    "    .collect()\n",
    "\n",
    "for k in wordCount:\n",
    "    print k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문제 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 'lim js' 127.86811196\n",
      " 'kim js' 174.16518698\n",
      " 'lee js' 143.3004703\n",
      " 'choi js' 145.50509292\n",
      " 'yoon js' 143.3004703\n",
      " 'park js' 220.462262\n",
      "\n",
      "\n",
      " '경영' 100.0\n",
      " '휴먼' 202.0\n",
      " '컴과' 131.0\n",
      "\n",
      "\n",
      " 'lim js' 정상\n",
      " 'kim js' 비만\n",
      " 'lee js' 정상\n",
      " 'choi js' 과체중\n",
      " 'yoon js' 비만\n",
      " 'park js' 고도비만\n"
     ]
    }
   ],
   "source": [
    "marks=[\n",
    "    \"'201411111', 'lim js', '휴먼', 58, 165\",\n",
    "    \"'201811111', 'kim js', '휴먼', 79, 175\",\n",
    "    \"'201211111', 'lee js', '휴먼', 65, 180\",\n",
    "    \"'201511111', 'choi js', '컴과', 66, 163\",\n",
    "    \"'201911111', 'yoon js', '컴과', 65, 158\",\n",
    "    \"'201311111', 'park js', '경영', 100, 160\",    \n",
    "]\n",
    "\n",
    "marksRdd=spark.sparkContext.parallelize(marks)\n",
    "heightInch=marksRdd\\\n",
    "    .map(lambda x:x.split(','))\\\n",
    "    .map(lambda x: (x[1],float(x[3]) * 2.20462262))\\\n",
    "    .collect()\n",
    "for i in heightInch:\n",
    "    print i[0],i[1]\n",
    "    \n",
    "print '\\n'    \n",
    "    \n",
    "marksByDep=marksRdd\\\n",
    "    .map(lambda x:x.split(','))\\\n",
    "    .map(lambda x: (x[2],float(x[3])))\\\n",
    "    .reduceByKey(lambda x,y:x+y)\\\n",
    "    .collect()\n",
    "for i in marksByDep:\n",
    "    print i[0],i[1]   \n",
    "\n",
    "print '\\n'  \n",
    "\n",
    "\n",
    "BMIList=marksRdd\\\n",
    "    .map(lambda x:x.split(','))\\\n",
    "    .map(lambda x:(x[1],float(x[3]), float(x[4])))\\\n",
    "    .collect()\n",
    "\n",
    "for line in BMIList:\n",
    "    bmiValue = line[1] / ((line[2] / 100) * (line[2] / 100))\n",
    "    if bmiValue <= 18.5:\n",
    "        print line[0], '저체중'\n",
    "    elif bmiValue < 23:\n",
    "        print line[0], '정상'\n",
    "    elif bmiValue < 25:\n",
    "        print line[0], '과체중'\n",
    "    elif bmiValue < 30:\n",
    "        print line[0], '비만'\n",
    "    else:\n",
    "        print line[0], '고도비만'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문제 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c4366a94a041>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbicycle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'com.databricks.spark.csv'\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/seoulBicycleDailyCount_2018_201903.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 컬럼 변경\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcleanBicycle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbicycle\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Date\"\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" count\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "bicycle = spark.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header='true', inferschema='true').load('data/seoulBicycleDailyCount_2018_201903.csv')\n",
    "\n",
    "# 컬럼 변경\n",
    "cleanBicycle=bicycle\\\n",
    "    .withColumnRenamed(\"date\", \"Date\")\\\n",
    "    .withColumnRenamed(\" count\", \"Count\")\n",
    "\n",
    "for line in cleanBicycle.take(5):\n",
    "    print line.Date, line.Count\n",
    "\n",
    "print '\\n'\n",
    "    \n",
    "# F함수를 이용하여 컬럼 만들기\n",
    "import pyspark.sql.functions as F\n",
    "cleanBicycle = cleanBicycle\\\n",
    "    .withColumn('year', F.year('date'))\\\n",
    "    .withColumn('month', F.month('date'))\n",
    "\n",
    "cleanBicycle.groupBy('year').agg({\"count\":\"avg\"}).show()\n",
    "cleanBicycle.groupBy('quarter').agg({\"count\":\"avg\"}).show()\n",
    "tDf = cleanBicycle.groupBy('month').agg({\"count\":\"avg\"})\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rowRdd=tDf.rdd.map(lambda fields:fields[0]).collect()\n",
    "colRdd=tDf.rdd.map(lambda fields:fields[1]).collect()\n",
    "plt.plot(np.array(rowRdd), np.array(colRdd),'o')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
