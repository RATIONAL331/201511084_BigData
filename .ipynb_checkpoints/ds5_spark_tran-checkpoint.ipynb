{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 데이터 변환\n",
    "\n",
    "* Last updated 20190909MON1800 20181009_20170421_20161125\n",
    "\n",
    "## S.1 학습내용\n",
    "\n",
    "### S.1.1 목표\n",
    "\n",
    "* Spark ETL을 할 수 있다.\n",
    "* Spark를 사용하여 구조적 데이터를 분석할 수 있다.\n",
    "* Spark를 사용하여 텍스트 분석을 할 수 있다.\n",
    "* Spark를 사용하여 추천을 할 수 있다.\n",
    "* Spark를 사용하여 그래프 분석을 할 수 있다.\n",
    "* 시각화\n",
    "    * matplotlib\n",
    "    * interactive Bokeh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.1.2 목차\n",
    "\n",
    "* S.2 IPython Notebook에서 SparkSession 생성하기\n",
    "* S.3 데이터 타잎\n",
    "* S.3.1 vectors\n",
    "* S.3.2 labeled point\n",
    "* S.3.3 maxtrix\n",
    "* S.3.4 libsvm format\n",
    "* S.4 통계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* S.5 변환 \n",
    "* S.5.1 모델의 입력데이터로 변환\n",
    "* S.5.2 Python을 사용한 단어 빈도 계산\n",
    "* S.5.3 scikit-learn TF-IDF \n",
    "* S.5.4 StringIndexer\n",
    "* S.5.5 Tokenizer\n",
    "* S.5.6 RegTokenizer\n",
    "* S.5.7 Stopwords\n",
    "* S.5.8 CountVectorizer\n",
    "* S.5.9 TF-IDF\n",
    "* S.5.10 Word2Vec\n",
    "* S.5.11 NGram\n",
    "* S.5.12 연속데이터의 변환\n",
    "* 5.5.13 VectorAssembler\n",
    "* S.5.14 Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* S.6 머신러닝\n",
    "* S.6.1 왜 머신러닝?\n",
    "* S.6.2 라이브러리\n",
    "* S.6.3 supervised vs unsupervised\n",
    "* S.6.4 회귀분석\n",
    "* S.6.5 군집화\n",
    "* S.6.6 분류\n",
    "* S.6.7 추천"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* S.7 연속 데이터 분석\n",
    "* S.7.1 데이터\n",
    "* S.7.2 변환\n",
    "* S.7.3 KMeans\n",
    "* S.7.4 Regression\n",
    "* S.8 구조적 데이터 분석\n",
    "* S.8.1 데이터\n",
    "* S.8.2 변환\n",
    "* S.8.3 LogisticRegression\n",
    "* S.8.4 svm\n",
    "* S.8.5 Decision Tree\n",
    "* S.8.6 Naive Bayesian\n",
    "* S.9 텍스트 분석\n",
    "* S.9.1 데이터\n",
    "* S.9.2 변환\n",
    "* S.9.3 LogisticRegression\n",
    "* S.9.4 Decision Tree\n",
    "* S.9.5 Naive Bayesian\n",
    "* S.9.6 svm\n",
    "* S.9.7 LDA\n",
    "* S.10 추천\n",
    "* S.11 scikit-learn\n",
    "* S.12 그래프 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.1.3 문제 \n",
    "\n",
    "* 문제 S-1: 훈련데이터 만들기\n",
    "* 문제 S-2: Kolmogorov-Smirnov 검증\n",
    "* 문제 S-3: 평균, 표준편차와 같은 기본 통계 값을 구한다.\n",
    "* 문제 S-4: spark-submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.2 IPython Notebook에서 SparkSession 생성하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "os.environ[\"SPARK_HOME\"]=os.path.join(os.environ['HOME'],'Downloads','spark-2.0.0-bin-hadoop2.7')\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "print spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.3 Vectors\n",
    "\n",
    "Spark에서는 **```Vector```**, **```Labeled Point```**, **```Matrix```**를 사용할 수 있다.\n",
    "\n",
    "구분 | 설명\n",
    "----------|----------\n",
    "```Vector``` | ```numpy vector```와 같은 기능을 한다. **dense**와 **sparse** vector로 구분한다.\n",
    "```Labeled Point``` | 분류를 의미하는 클래스 또는 **label**과 속성 **features** 이 묶인 구조로서, supervised learning에 사용된다.\n",
    "```Matrix``` | ```numpy matrix```와 같은 특징을 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "이러한 데이터 타잎은 Spark의 **```ml```**, **```mllib```** 패키지 별로 제공되므로, 식별하여 사용한다.\n",
    "```ml``` 패키지를 사용할 경우에는 아래에서 제공하는 자신의 ```pyspark.ml.linalg.Vector``` 등을 사용해야 한다. ```mllib```도 마찬가지이다.\n",
    "\n",
    "패키지 | 설명 | 데이터 타잎 예\n",
    "-------|-------|-----\n",
    "```mllib``` | RDD API를 제공한다. | ```pyspark.ml.linalg.Vector```, ```pyspark.ml.linalg.Matrix```\n",
    "```ml``` | DataFrame API를 제공한다. | ```pyspark.mllib.linalg.Vector```, ```pyspark.mllib.linalg.Matrix```\n",
    "\n",
    "\n",
    "연산\n",
    "As for now (Spark 1.6.0) pyspark.mllib.linalg.distributed API is limited to basic operations like counting rows/columns and transformations between types.\n",
    "\n",
    "\n",
    "Neither pyspark.ml.linalg.Matrix nor pyspark.mllib.linalg.Matrix implements matrix multiplication\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.3.1 vectors\n",
    "\n",
    "**Vector**는 **dense**와 **sparse**로 구분할 수 있다.\n",
    "**sparse**는 실제 **값이 없는 요소, '0'을 제거하**여 만든 vector이다.\n",
    "Spark가 효율적으로 메모리를 사용하기 위해 자동으로 변환하여 사용하기도 한다.\n",
    "Spark에서 type field (1 바이트 길이)를 통해 식별한다 (0: sparse, 1: dense)\n",
    "\n",
    "예를 들어, 다음은 dense vector이다.\n",
    "```python\n",
    "(160,69,24)\n",
    "```\n",
    "\n",
    "이를 sparse vector로 표현하면, 각 컬럼별 해당하는 값을 적는다. 값이 없는 요소가 없으니 더 복잡해 보인다.\n",
    "```python\n",
    "(3,[0,1,2],[160.0,69.0,24.0])\n",
    "```\n",
    "\n",
    "dense vector | sparse vector\n",
    "----------|----------\n",
    "**모든** 행열 값을 가지고 있다. | **인덱스 및 값**의 배열을 별도로 가진다.\n",
    "빈 값이 별로 없는 경우. | 빈 값이 많은 경우 사용. \n",
    "```(160,69,24)``` | ```(3,[0,1,2],[160.0,69.0,24.0])```<br>컬럼 3개, 값이 있는 컬럼, 값\n",
    "numpy array, Python list를 입력으로 사용 | Vectors.sparse(), SciPy’s csc_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Dense vectors\n",
    "\n",
    "numpy array를 사용해도 dense vector를 만들 수 있다. Spark 내부적으로 **numpy.array**를 사용하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dv = np.array([1.0, 2.1, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Spark에서는 **```RDD mllib```** , **```DataFrame ml```**의 **Vectors**를 사용하여 dense vectors를 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0,2.1,3.0] <class 'pyspark.mllib.linalg.DenseVector'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "dv1mllib=Vectors.dense([1.0, 2.1, 3])\n",
    "print dv1mllib, type(dv1mllib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0,2.1,3.0]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "dv1ml=Vectors.dense([1.0, 2.1, 3])\n",
    "print dv1ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "dense vectors는 numpy array와 같은 특징을 가진다.\n",
    "인덱스로 값을 읽을 수 있다. 또한 반복문에서 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 2.1 3.0\n"
     ]
    }
   ],
   "source": [
    "for e in dv1ml:\n",
    "    print e,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "보통 벡터와 같이 **product**, **dot**, **norm**과 같은 벡터 연산을 할 수도 있다.\n",
    "결과 값은 numpy와 동일하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.41\n"
     ]
    }
   ],
   "source": [
    "print dv1ml.dot(dv1ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.41"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(dv,dv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "더하기, 빼기, 곱하기, 나누기 연산은 항목별로 실행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0,4.41,9.0]\n"
     ]
    }
   ],
   "source": [
    "print dv1ml*dv1ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Sparse vectors\n",
    "\n",
    "sparse vectors는 값 중에 0이 포함된 경우 이를 생략한다.\n",
    "```toArray()``` 함수를 사용하면 sparse에서 dense로 벡터를 변환할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  3.]\n"
     ]
    }
   ],
   "source": [
    "sv1 = Vectors.sparse(3, [1, 2], [1.0, 3.0])\n",
    "print sv1.toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "많이 사용되는 라이브러리인 scipy.sparse의 **```Compressed Sparse Column```** 형식과 비교해 보자.\n",
    "\n",
    "행을 보면 0번째에 '1','2' 1번째에 '3', 2번째에 '4','5','6'이므로 **0,0,1,2,2,2**\n",
    "열을 보면 0번째에 '1', 2번째 '2','3', 0번째 '4', 1번째 '5', 2번째 '6'이므로 **0,2,2,0,1,2**\n",
    "\n",
    "**행, 열, 데이터를 한 쌍**으로 읽으면 된다.\n",
    "즉 행 0, 열 0의 위치에 1, 행 0, 열 2의 위치에 2. 이런 식으로 6개의 데이터가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 2]\n",
      " [0 0 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "\n",
    "row = np.array([0, 0, 1, 2, 2, 2])\n",
    "col = np.array([0, 2, 2, 0, 1, 2])\n",
    "data = np.array([1, 2, 3, 4, 5, 6])\n",
    "mtx = sps.csc_matrix((data, (row, col)), shape=(3, 3))\n",
    "print mtx.todense()   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.3.2 Labeled Point\n",
    "\n",
    "#### label, features로 구성\n",
    "\n",
    "**분류** 및 **회귀분석**에 사용되는 데이터 타잎이다.\n",
    "**'label'**과 **'features'**로 구성된다.\n",
    "\n",
    "구분 | 설명\n",
    "-----|-----\n",
    "label | supervised learning에서 '구분 값'으로 사용한다. 데이터타잎은 'Double'\n",
    "features | **sparse**, **dense** 모두 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "label 1.0, features [1.0, 2.0, 3.0]으로 LabeledPoint를 만들어 보자.\n",
    "\n",
    "구분 | 예제 | 설명\n",
    "-----|-----|-----\n",
    "label | 1.0 | 구분 값으로 Double 데이터 타잎\n",
    "features | [1.0, 2.0, 3.0] | **dense vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0,[1.0,2.0,3.0])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "print LabeledPoint(1.0, [1.0, 2.0, 3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1992.0,(10,[0,1,2],[3.0,5.5,10.0]))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "print LabeledPoint(1992, Vectors.sparse(10, {0: 3.0, 1:5.5, 2: 10.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "서로 다른 패키지의 데이터타잎 **```mllib LabeledPoint```**와 **```ml Vectors```**를 혼용하면, 형변환 오류가 발생한다.\n",
    "이러한 오류는 패키지를 혼용하지 않으면 된다.\n",
    "\n",
    "```python\n",
    "Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n",
    "```\n",
    "\n",
    "**```dv1mllib```**은 앞서 **```mllib```**로부터 생성된 dense vector이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [1.0,2.1,3.0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "LabeledPoint(1.0, dv1mllib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**```dv1ml```**은 앞서 **```ml```**로부터 생성된 dense vector이다.\n",
    "```mllib```에서 사용하려면, **```Vectors.fromML()```**를 사용해 ```ml```의 Vectors를 읽어서 ```mllib```로 변환하여 혼용을 막는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [1.0,2.1,3.0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "LabeledPoint(1.0, Vectors.fromML(dv1ml))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### DataFrame에서 Labeled Point\n",
    "\n",
    "* Python list에서 DataFrame 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=1, _2=[1.0, 2.0, 3.0]),\n",
       " Row(_1=1, _2=[1.1, 2.1, 3.1]),\n",
       " Row(_1=0, _2=[1.2, 2.2, 3.3])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = [[1,[1.0,2.0,3.0]],[1,[1.1,2.1,3.1]],[0,[1.2,2.2,3.3]]]\n",
    "trainDf=spark.createDataFrame(p)\n",
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Python list를 LabeledPoint로 생성하면, 'label'과 'features'의 명칭을 가지도록 생성된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([1.0, 2.0, 3.0]), label=1.0),\n",
       " Row(features=DenseVector([1.1, 2.1, 3.1]), label=1.0),\n",
       " Row(features=DenseVector([1.2, 2.2, 3.3]), label=0.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "p = [LabeledPoint(1,[1.0,2.0,3.0]),\n",
    "     LabeledPoint(1,[1.1,2.1,3.1]),\n",
    "     LabeledPoint(0,[1.2,2.2,3.3])]\n",
    "trainDf=spark.createDataFrame(p)\n",
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "mllib.linalg.Vectors를 사용하여 DataFrame을 생성해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=1.0, features=DenseVector([0.0, 1.1, 0.1])),\n",
       " Row(label=0.0, features=DenseVector([2.0, 1.0, 1.0])),\n",
       " Row(label=0.0, features=DenseVector([2.0, 1.3, 1.0])),\n",
       " Row(label=1.0, features=DenseVector([0.0, 1.2, 0.5]))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "trainDf = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, 1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, 0.5]))], [\"label\", \"features\"])\n",
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "schema를 사용해서 DataFrame을 생성해 보자.\n",
    "* 'label'은 **DoubleType**\n",
    "* 'features'는 **VectorType**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import SparseVector, VectorUDT\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "_rdd = spark.sparkContext.parallelize([\n",
    "    (0.0, SparseVector(4, {1: 1.0, 3: 5.5})),\n",
    "    (1.0, SparseVector(4, {0: -1.0, 2: 0.5}))])\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"label\", DoubleType(), True),\n",
    "    StructField(\"features\", VectorUDT(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf=_rdd.toDF(schema)\n",
    "trainDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### sparse에서 dense vector로 변환\n",
    "\n",
    "방금 생성한 ```trainDf```는 sparse vector이다.\n",
    "사용자 함수udf User Defined Type를 사용하여 sparse vector를 dense vector로 변환해 보자.\n",
    "바로 변환할 수 있는 함수 **toDense() 함수를 지원하지 않으므로**, **sparse vector를 ```toArray()``` 함수를 사용해서 dense vector로 변환**한다.\n",
    "\n",
    "또 ```trainDf```는 mllib RDD에서 변환된 데이터이므로, mllib 라이브러리를 사용한다.\n",
    "\n",
    "구분 | 설명\n",
    "-----|-----\n",
    "from pyspark.mllib.linalg import DenseVector,VectorUDT | mllib\n",
    "from pyspark.ml.linalg import DenseVector,VectorUDT | ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "다음 명령문을 요소별로 하나씩 설명한다.\n",
    "```python\n",
    "udf(lambda x: DenseVector(x.toArray()), VectorUDT())\n",
    "```\n",
    "명령문 | 설명\n",
    "-----|-----\n",
    "```udf()``` | 사용자정의 함수\n",
    "```x.toArray()``` | sparse vector로 구성된 trainDf.features를 **```toArray()```**를 사용하여 array로 변환한다.\n",
    "```VectorUDT()``` | 타잎을 지정한다. 지정하지 않으면 ```StringType```을 기본으로, 자동으로 변환된다.\n",
    "```DenseVector()``` | array를 dense vector로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "#from pyspark.ml.linalg import DenseVector, VectorUDT\n",
    "from pyspark.mllib.linalg import DenseVector, VectorUDT\n",
    "#myudf=udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "#myudf=udf(lambda x: Vectors.dense(x))\n",
    "myudf=udf(lambda x: DenseVector(x.toArray()), VectorUDT())\n",
    "_trainDf2=trainDf.withColumn('dvf',myudf(trainDf.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- dvf: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_trainDf2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------------------+\n",
      "|label|            features|               dvf|\n",
      "+-----+--------------------+------------------+\n",
      "|  0.0| (4,[1,3],[1.0,5.5])| [0.0,1.0,0.0,5.5]|\n",
      "|  1.0|(4,[0,2],[-1.0,0.5])|[-1.0,0.0,0.5,0.0]|\n",
      "+-----+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_trainDf2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.3.3 maxtrix\n",
    "\n",
    "* local matrix - pyspark.mllib.linalg.Matrix, Matrices\n",
    "* distributed matrix\n",
    "    * pyspark.mllib.linalg.distributed.RowMatrix\n",
    "    * pyspark.mllib.linalg.distributed.IndexedRow, IndexedRowMatrix\n",
    "    * pyspark.mllib.linalg.distributed.BlockMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Matrix, Matrices\n",
    "\n",
    "# Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\n",
    "dm = Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))\n",
    "sm = Matrices.sparse(3, 2, [0, 1, 3], [0, 2, 1], [9, 6, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-1: RDD 훈련데이터 만들기\n",
    "\n",
    "### 문제\n",
    "\n",
    "머신러닝은 사람이 경험을 통해 배우는 것과 비슷하게 **과거 데이터로부터 학습**을 한다.\n",
    "학습이란 어렵게 생각할 필요 없이, 과거 데이터에서 수학적이나 알고리즘을 활용하여 어떤 패턴을 찾아내는 것이다.\n",
    "spark에서 제공한 **데이터 파일 ```data/mllib/sample_svm_data.txt```을 읽어서 훈련데이터**를 만들어 보자.\n",
    "\n",
    "```python\n",
    "1 0 2.52078447201548 0 0 0 2.004684436494304 2.000347299268466 0 2.228387042742021 2.228387042742023 0 0 0 0 0 0\n",
    "0 2.857738033247042 0 0 2.619965104088255 0 2.004684436494304 2.000347299268466 0 2.228387042742021 2.228387042742023 0 0 0 0 0 0\n",
    "0 2.857738033247042 0 2.061393766919624 0 0 2.004684436494304 0 0 2.228387042742021 2.228387042742023 0 0 0 0 0 0\n",
    "1 0 0 2.061393766919624 2.619965104088255 0 2.004684436494304 2.000347299268466 0 0 0 0 2.055002875864414 0 0 0 0\n",
    "1 2.857738033247042 0 2.061393766919624 2.619965104088255 0 2.004684436494304 0 0 0 0 0 2.055002875864414 0 0 0 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 해결\n",
    "\n",
    "데이러틀 읽어 **RDD**를 생성하고, ```label```, ```features```를 구성하여 ```Labeled Point```로 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Python으로 파일 읽기\n",
    "\n",
    "파일로부터 데이터를 읽기 위해, 파일명을 구성하고 ```try except``` 구문으로 입출력 오류를 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    _fp=os.path.join(os.environ[\"SPARK_HOME\"],\\\n",
    "        'data','mllib','sample_svm_data.txt')\n",
    "except:\n",
    "    print(\"An exception occurred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "파일로부터 데이터를 **```readlines()```** 함수로 모두 읽어 온다.\n",
    "첫 행을 읽으면 label, features로 구성되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_f=open(_fp,'r')\n",
    "_lines=_f.readlines()\n",
    "_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 2.52078447201548 0 0 0 2.004684436494304 2.000347299268466 0 2.228387042742021 2.228387042742023 0 0 0 0 0 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print _lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Spark에서 RDD 생성\n",
    "\n",
    "원본 데이터 ```sample_svm_data.txt```는 공백으로 구분되어 있다.\n",
    "읽을 대상이 파일이므로, RDD를 사용한다. 각 행을 공백으로 분리하여 읽는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_rdd=spark.sparkContext.textFile(_fp)\\\n",
    "    .map(lambda line: [float(x) for x in line.split(' ')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "각 행으로 분리되므로 2차원 리스트가 생성이 된다. 첫째 행을 읽으려면 인덱스를 사용해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 0.0,\n",
       " 2.52078447201548,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.004684436494304,\n",
       " 2.000347299268466,\n",
       " 0.0,\n",
       " 2.228387042742021,\n",
       " 2.228387042742023,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd.take(2)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LabeledPoint 생성\n",
    "\n",
    "위 데이터에서 보듯이 첫 열은 **label**로, 그 나머지는 **features**로 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "_trainRdd0=_rdd.map(lambda line:LabeledPoint(line[0], line[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [0.0,2.52078447202,0.0,0.0,0.0,2.00468443649,2.00034729927,0.0,2.22838704274,2.22838704274,0.0,0.0,0.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_trainRdd0.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "공백을 분리하고, 분리된 데이터를 labeled point로 구성하는 기능을 합쳐서 실행해 본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_trainRdd=spark.sparkContext.textFile(_fp)\\\n",
    "    .map(lambda line: [float(x) for x in line.split(' ')])\\\n",
    "    .map(lambda p:LabeledPoint(p[0], p[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [0.0,2.52078447202,0.0,0.0,0.0,2.00468443649,2.00034729927,0.0,2.22838704274,2.22838704274,0.0,0.0,0.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_trainRdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 정리하면\n",
    "\n",
    "데이터를 변환하는 과정을 함수로 만들었다.\n",
    "```createLP(line)```는 행 데이터를 받아서 LabeledPoint로 생성하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [0.0,2.52078447202,0.0,0.0,0.0,2.00468443649,2.00034729927,0.0,2.22838704274,2.22838704274,0.0,0.0,0.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createLP(line):\n",
    "    p = [float(x) for x in line.split(' ')]\n",
    "    return LabeledPoint(p[0], p[1:])\n",
    "\n",
    "_rdd=spark.sparkContext.textFile(_fp)\n",
    "trainRdd = _rdd.map(createLP)\n",
    "\n",
    "trainRdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.3.4 libsvm format\n",
    "\n",
    "* svm을 처리하기 위한 데이터 형식이다.\n",
    "* 0은 label, 나머지는 index:value 쌍으로 구성한다.\n",
    "```python\n",
    "[label] [index1]:[value1] [index2]:[value2] ...\n",
    "[label] [index1]:[value1] [index2]:[value2] ...\n",
    "```\n",
    "\n",
    "* 예\n",
    "```python\n",
    "0 128:51 129:159 130:253 131:159 132:50 155:48 156:238 157:252 158:252 159:252 160:237 182:54 183:227 184:253 185:252 186:239 187:233 ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fsvm=os.path.join(os.environ[\"SPARK_HOME\"],'data','mllib','sample_libsvm_data.txt')\n",
    "dfsvm = spark.read.format(\"libsvm\").load(fsvm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dfsvm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfsvm.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0.0, features=SparseVector(692, {127: 51.0, 128: 159.0, 129: 253.0, 130: 159.0, 131: 50.0, 154: 48.0, 155: 238.0, 156: 252.0, 157: 252.0, 158: 252.0, 159: 237.0, 181: 54.0, 182: 227.0, 183: 253.0, 184: 252.0, 185: 239.0, 186: 233.0, 187: 252.0, 188: 57.0, 189: 6.0, 207: 10.0, 208: 60.0, 209: 224.0, 210: 252.0, 211: 253.0, 212: 252.0, 213: 202.0, 214: 84.0, 215: 252.0, 216: 253.0, 217: 122.0, 235: 163.0, 236: 252.0, 237: 252.0, 238: 252.0, 239: 253.0, 240: 252.0, 241: 252.0, 242: 96.0, 243: 189.0, 244: 253.0, 245: 167.0, 262: 51.0, 263: 238.0, 264: 253.0, 265: 253.0, 266: 190.0, 267: 114.0, 268: 253.0, 269: 228.0, 270: 47.0, 271: 79.0, 272: 255.0, 273: 168.0, 289: 48.0, 290: 238.0, 291: 252.0, 292: 252.0, 293: 179.0, 294: 12.0, 295: 75.0, 296: 121.0, 297: 21.0, 300: 253.0, 301: 243.0, 302: 50.0, 316: 38.0, 317: 165.0, 318: 253.0, 319: 233.0, 320: 208.0, 321: 84.0, 328: 253.0, 329: 252.0, 330: 165.0, 343: 7.0, 344: 178.0, 345: 252.0, 346: 240.0, 347: 71.0, 348: 19.0, 349: 28.0, 356: 253.0, 357: 252.0, 358: 195.0, 371: 57.0, 372: 252.0, 373: 252.0, 374: 63.0, 384: 253.0, 385: 252.0, 386: 195.0, 399: 198.0, 400: 253.0, 401: 190.0, 412: 255.0, 413: 253.0, 414: 196.0, 426: 76.0, 427: 246.0, 428: 252.0, 429: 112.0, 440: 253.0, 441: 252.0, 442: 148.0, 454: 85.0, 455: 252.0, 456: 230.0, 457: 25.0, 466: 7.0, 467: 135.0, 468: 253.0, 469: 186.0, 470: 12.0, 482: 85.0, 483: 252.0, 484: 223.0, 493: 7.0, 494: 131.0, 495: 252.0, 496: 225.0, 497: 71.0, 510: 85.0, 511: 252.0, 512: 145.0, 520: 48.0, 521: 165.0, 522: 252.0, 523: 173.0, 538: 86.0, 539: 253.0, 540: 225.0, 547: 114.0, 548: 238.0, 549: 253.0, 550: 162.0, 566: 85.0, 567: 252.0, 568: 249.0, 569: 146.0, 570: 48.0, 571: 29.0, 572: 85.0, 573: 178.0, 574: 225.0, 575: 253.0, 576: 223.0, 577: 167.0, 578: 56.0, 594: 85.0, 595: 252.0, 596: 252.0, 597: 252.0, 598: 229.0, 599: 215.0, 600: 252.0, 601: 252.0, 602: 252.0, 603: 196.0, 604: 130.0, 622: 28.0, 623: 199.0, 624: 252.0, 625: 252.0, 626: 253.0, 627: 252.0, 628: 252.0, 629: 233.0, 630: 145.0, 651: 25.0, 652: 128.0, 653: 252.0, 654: 253.0, 655: 252.0, 656: 141.0, 657: 37.0}))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsvm.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.4 통계\n",
    "\n",
    "* mllib 모듈을 사용한다 'pyspark.mllib.stat'\n",
    "* 기본 통계\n",
    "* 가설 검증\n",
    "* 상관관계 - 키와 몸무게의 상관관계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-2:  Kolmogorov-Smirnov 검증\n",
    "\n",
    "정규분표 비모수 추정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kolmogorov-Smirnov test summary:\n",
      "degrees of freedom = 0 \n",
      "statistic = 0.841344746068543 \n",
      "pValue = 5.06089025353873E-6 \n",
      "Very strong presumption against null hypothesis: Sample follows theoretical distribution.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "parallelData = spark.sparkContext.parallelize([1.0, 2.0, 5.0, 4.0, 3.0, 3.3, 5.5])\n",
    "\n",
    "# run a KS test for the sample versus a standard normal distribution\n",
    "testResult = Statistics.kolmogorovSmirnovTest(parallelData, \"norm\", 0, 1)\n",
    "print(testResult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-3:  평균, 표준편차와 같은 기본 통계 값을 구한다.\n",
    "\n",
    "### 문제\n",
    "\n",
    "균등분포 및 정규분포를 무작위로 생성해 기본통계 값을 계산해 보자.\n",
    "\n",
    "### 해결\n",
    "\n",
    "무작위는 발생빈도가 어느 쪽에 치우치지 않는다.\n",
    "Spark에서 무작위로 균등분포 및 정규분포를 생성하고, 기본통계를 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 분포 생성\n",
    "\n",
    "DataFrame에서 제공하는 통계 기능을 사용해 본다.\n",
    "컬럼 3개의 DataFrame을 생성한다.\n",
    "* 첫 컬럼은 'id', SparkSession.range()를 사용한다.\n",
    "* 무작위 수를 추출해서, 나머지 컬럼 데이터를 만든다. pyspark.sql.functions 함수를 사용한다.\n",
    "* rand()는 Uniform분포, randn()은 정규분포를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.range(0,10)\n",
    "df.show()\n",
    "df.select('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "통계에 '무작위' 수는 중요하다. 무작위 샘플, 무작위 수를 발생하여 확률에서 빈번하게 사용한다.\n",
    "**```pyspark.sql.functions```**를 사용하여 생성해 보자.\n",
    "\n",
    "pyspark.sql.functions | 설명\n",
    "-----|-----\n",
    "rand() | 0,1 사이의 균등분포를 생성한다. seed를 넣어서 생성할 수 있다.\n",
    "randn() | 정규분포를 생성한다. seed를 넣어서 생성할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+--------------------+\n",
      "| id|            uniform|              normal|\n",
      "+---+-------------------+--------------------+\n",
      "|  0|0.41371264720975787|  0.5888539012978773|\n",
      "|  1| 0.7311719281896606|  0.8645537008427937|\n",
      "|  2| 0.1982919638208397| 0.06157382353970104|\n",
      "|  3|0.12714181165849525|  0.3623040918178586|\n",
      "|  4| 0.7604318153406678|-0.49575204523675975|\n",
      "|  5|0.12030715258495939|  1.0854146699817222|\n",
      "|  6|0.12131363910425985| -0.5284523629183004|\n",
      "|  7|0.44292918521277047| -0.4798519469521663|\n",
      "|  8| 0.8898784253886249| -0.8820294772950535|\n",
      "|  9|0.03650707717266999| -2.1591956435415334|\n",
      "+---+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand, randn\n",
    "colUniform = rand(seed=10).alias(\"uniform\")\n",
    "colNormal=randn(seed=27).alias(\"normal\")\n",
    "df3=df.select(\"id\", colUniform,colNormal)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 기본 통계\n",
    "\n",
    "주사위는 이산균등분포의 가장 대표적인 예이다. 각 숫자가 나올 확률은 1/6이다.\n",
    "정규분포는 평균 0을 중심으로 빈도가 몰려있어 표준편차만큼 퍼진 특징을 가진다.\n",
    "각 컬럼별로 통계 값을 계산할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+--------------------+\n",
      "|summary|                id|            uniform|              normal|\n",
      "+-------+------------------+-------------------+--------------------+\n",
      "|  count|                10|                 10|                  10|\n",
      "|   mean|               4.5| 0.5488228646413278|0.009861721586543392|\n",
      "| stddev|3.0276503540974917| 0.2856822245344392|  1.2126061129356596|\n",
      "|    min|                 0|0.09430205113458567|  -2.573636861034734|\n",
      "|    max|                 9| 0.9571919406508957|  1.2524569684217643|\n",
      "+-------+------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### freqItems()\n",
    "\n",
    "a, b, c 세 컬럼을 생성한다.\n",
    "홀수 행이면 1,2,3으로 짝수 행이면 다른 수열로 DataFrame을 생성해 보자.\n",
    "이 데이터에 대해 60%이상 발생한 행을 출력해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "|  1|  2|  1|\n",
      "|  1|  2|  3|\n",
      "|  3|  6|  3|\n",
      "|  1|  2|  3|\n",
      "|  5| 10|  1|\n",
      "|  1|  2|  3|\n",
      "|  7| 14|  3|\n",
      "|  1|  2|  3|\n",
      "|  9| 18|  1|\n",
      "+---+---+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame([(1,2,3) if i%2==0 else (i,2*i,i%4) for i in range(100)],[\"a\",\"b\",\"c\"])\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+\n",
      "|a_freqItems|b_freqItems|c_freqItems|\n",
      "+-----------+-----------+-----------+\n",
      "|        [1]|        [2]|        [3]|\n",
      "+-----------+-----------+-----------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "freq = df.stat.freqItems([\"a\",\"b\",\"c\"],0.6)\n",
    "print freq.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.5 변환\n",
    "\n",
    "Spark는 다양한 입력에서 데이터를 추출, 변환하여 예측, 분류, 군집화, 추천과 같은 모델에 사용하게 된다. 이를 **ETL** (Extract, Transform, Load)이라고 하며 Spark **RDD**는 map-reduce와 같은 **transformation**, **action**을 사용한다.\n",
    "**DataFrame**도 비슷하게 **Transformer**, **Estimator**를 사용할 수 있다.\n",
    "**Pipeline**은 여러 Estimator를 묶은 Estimator를 반환한다. 단계적으로 Estimator를 적용하기 위해 사용한다.\n",
    "\n",
    "변환 기능 | 설명 | 함수\n",
    "----------|----------|----------\n",
    "Estimator | 모델의 인자를 설정, 데이터에 적용한다. Transformer를 반환한다.| ```Estimator.fit()```\n",
    "Transformer | 열을 선택, 변환한다. 그 결과를 DataFrame으로 반환한다. | ```Transformer.transform()```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.5.1 모델의 입력데이터로 변환\n",
    "\n",
    "* 군집화, 회귀분석, 분류, 추천 모델에 사용되는 데이터는 '일련의 수' 또는 '텍스트'로 구성된다.\n",
    "* 특징을 추출하여 **feature vectors**를 구성한다.\n",
    "* 분류를 하는 경우에는 **class 또는 label** 값이 필요하다.\n",
    "* 텍스트는 **'bag of words'**으로 표현한다.\n",
    "    * 문서는 단어로 구성된다.\n",
    "    * 단어의 순서는 의미를 가지지 않는다.\n",
    "    \n",
    "구분 | 설명 | 예\n",
    "----------|----------|----------|\n",
    "corpus | 문서 집합 | \"why she had to go\", \"where she have to go\"\n",
    "document | 레코드 | \"why she had to go\"\n",
    "vocabularay | 중복없는 단어 집합 | \"why\",\"she\",\"had\",\"to\",\"go\",\"where\",\"have\"\n",
    "word vector | 있다-없다, 단어빈도, TFIDF 사용할 수 있다.<br>dense, sparse 모두 가능하다. | ```[1,1,1,1,1,0,0],[0,1,0,1,1,1,1]```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 데이터의 구성\n",
    "    * 데이터 컬럼 명은 'lableCol', 'featureCol'으로 설정해서 사용할 수 있다.\n",
    "    * 기본 컬럼 명을 사용하는 경우:\n",
    "\n",
    "구분 | 컬럼 구성\n",
    "-----|-----\n",
    "DataFrame | 'label' (DoubleType), 'features' (sparse or dense vectors)\n",
    "Rdd | LabeledPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 데이터 정련\n",
    "    * 결측 값, 범위를 벗어나는 outlier, \n",
    "    * trainRDD에 마이너스 값 nok?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.5.2 Python을 사용한 단어 빈도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let it be lyrics\n",
    "doc=[\n",
    "    \"When I find myself in times of trouble\",\n",
    "    \"Mother Mary comes to me\",\n",
    "    \"Speaking words of wisdom, let it be\",\n",
    "    \"And in my hour of darkness\",\n",
    "    \"She is standing right in front of me\",\n",
    "    \"Speaking words of wisdom, let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Whisper words of wisdom, let it be\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "문서, 문장, 단어의 계층을 먼저 이해해야 한다.\n",
    "문서는 문장으로 구성되어 있고, 문장은 단어로 구성되어 있다.\n",
    "따라서 첫째 반복문은 문서의 각 문장에 대해, 단어로 분리하고 있다.\n",
    "그 다음 반복문은 각 단어에 대해 빈도를 계산한다.\n",
    "각 단어가 키가 되는데, **키가 존재하면 빈도를 증가하고, 존재하지 않으면 새로운 키를 생성**한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d={}\n",
    "for sentence in doc:\n",
    "    words=sentence.split()\n",
    "    for word in words:\n",
    "        if word in d:\n",
    "            d[word]+=1\n",
    "        else:\n",
    "            d[word]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "앞서 단어 빈도는 dictionary d에 저장하였다.\n",
    "dictionary는 키, 빈도의 쌍으로 저장되어 있어서 ```iteritems()```으로 읽어낼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right 1\n",
      "be 7\n",
      "is 1\n",
      "When 1\n",
      "it 7\n",
      "in 3\n",
      "Mary 1\n",
      "Speaking 2\n",
      "standing 1\n",
      "darkness 1\n",
      "find 1\n",
      "wisdom, 3\n",
      "to 1\n",
      "Let 4\n",
      "And 1\n",
      "I 1\n",
      "let 3\n",
      "She 1\n",
      "words 3\n",
      "Mother 1\n",
      "front 1\n",
      "trouble 1\n",
      "me 2\n",
      "myself 1\n",
      "hour 1\n",
      "of 6\n",
      "times 1\n",
      "Whisper 1\n",
      "my 1\n",
      "comes 1\n"
     ]
    }
   ],
   "source": [
    "# Python 2 - 3 compatible code\n",
    "# for k,v in d.items():\n",
    "for k,v in d.iteritems():\n",
    "    print k,v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.5.3 scikit-learn TF-IDF\n",
    "\n",
    "* ```TfidfTransformer```는 **TF-IDF(Term Frequency-Inverse Document Frequency)**를 계산한다.\n",
    "    * 단계 1: Tokenizer를 사용하여 문장을 단어로 분리 \n",
    "    * 단계 2: CountVectorizer를 사용하여 단어의 빈도수tf를 계산\n",
    "    * 단계 3: HashingTF를 사용하여 'word vector'를 계산.\n",
    "    HashingTF은 hash함수에 따라 단어의 고유번호를 생성,\n",
    "    hash고유번호의 충돌 가능성을 줄이기 위해, 단어 수를 제한할 수 있다.\n",
    "    * 단계 4: IDF를 계산\n",
    "    * 단계 5: TF-IDF를 계산 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### S.5.2.1 TF-IDF 계산\n",
    "\n",
    "'Let it be'가사 세 번째 줄 **'wisdom' 단어**의 TF-IDF를 계산해보자.\n",
    "```TfidfVectorizer```를 사용해서 계산하면 그 결과를 아래와 같이 볼 수 있다.\n",
    "```python\n",
    "(2,12) 2.09861228867\n",
    "```\n",
    "\n",
    "결과에서\n",
    "**'2'**는 3번째 문서번호, **'12'**는 'wisdom' 단어번호\n",
    "TF-IDF는 ```2.09861228867```이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3번째 문장 \"Speaking words of wisdom, let it be\"의 word vector를 구성해 본다.\n",
    "id 값은 모든 문장에서 단어를 추출하고 나서야 부여될 수 있다.\n",
    "\n",
    "단어 (3행 \"Speaking words of wisdom, let it be\") | id | 빈도 | \n",
    "-----|-----|-----\n",
    "Speaking | 7 | 1\n",
    "words | 13 | 1\n",
    "of | stopword | 0\n",
    "wisdom | 12 | 1\n",
    "let | 3 | 1\n",
    "it | stopword | 0\n",
    "be | stopword | 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "위 **word vector**를 표로 나타내면 아래와 같다.\n",
    "행은 문장, 열은 id이다.\n",
    "**3행은 doc2**이다. 해당하는 **단어 id의 빈도**를 적었다. 다른 행과 열은 이해를 돕기 위해 비워 놓았다.\n",
    "\n",
    "```doc``` \\ 단어 id  | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |10 |11 |12 |13 |...\n",
    "------|---|---|---|---|---|---|---|---|---|---|---|---|---|---\n",
    "```doc 0``` |   |   |   |   |   |   |   |   |   |   |   |   |   |...\n",
    "```doc 1``` |   |   |   |   |   |   |   |   |   |   |   |   |   |...\n",
    "```doc 2``` |   |   | 1 |   |   |   | 1 |   |   |   |   | 1 | 1 |...\n",
    "...   |   |   |   |   |   |   |   |   |   |   |   |   |   |..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* TF-IDF 계산\n",
    "\n",
    "항목 | 설명 | 예제\n",
    "-----|-----|-----\n",
    "tf | term frequency 단어의 빈도 수 | $f_{t,d}$ / (number of words in d) = 1/4 = 0.25<br>(3번째 문서에 stopwords를 제외하면 4개의 단어, wisdom은 1회 나타난다.)\n",
    "df | document frequency 단어가 나타난 문서 수 | 3 (wisdom이 포함된 문서는 3)\n",
    "N | number of documents 전체 문서의 수 | 11 (전체의 문서는 11개)\n",
    "idf | inverse document frequency 단어가 나타난 문서의 비율을 거꾸로 | ln(N+1 / df+1) + 1 = log(12/4) + 1 = 1.09861 + 1<br>0으로 나뉘는 것을 방지하기 위해 **smoothing**, 즉 1을 더한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.09861228867\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "tf=1./4\n",
    "df=3.\n",
    "N=11.\n",
    "idf=math.log((N+1)/(df+1))+1\n",
    "print idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### S.5.2.2 sklearn을 사용한 TF-IDF\n",
    "\n",
    "Spark는 'sklearn'의 TF-IDF와 동일한 방식으로 계산한다.\n",
    "**```CountVectorizer```**를 사용하여, 문서 x 단어를 표로 계산결과를 출력할 수 있다.\n",
    "그 다음으로, TF-IDF를 계산할 수 있다. 이 때 (문서id, 단어id) 별로 결과가 출력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1]\n",
      " [1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1]]\n",
      "{u'and': 0, u'be': 1, u'right': 17, u'whisper': 25, u'is': 8, u'it': 9, u'wisdom': 26, u'me': 12, u'let': 10, u'words': 27, u'in': 7, u'front': 5, u'trouble': 23, u'find': 4, u'standing': 20, u'comes': 2, u'myself': 15, u'darkness': 3, u'hour': 6, u'of': 16, u'when': 24, u'times': 21, u'to': 22, u'she': 18, u'mother': 13, u'my': 14, u'mary': 11, u'speaking': 19}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "vectorizer = CountVectorizer()\n",
    "print vectorizer.fit_transform(doc).todense()\n",
    "print vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10)\t2.79175946923\n",
      "  (0, 9)\t2.79175946923\n",
      "  (1, 0)\t2.79175946923\n",
      "  (1, 4)\t2.79175946923\n",
      "  (1, 5)\t2.79175946923\n",
      "  (2, 3)\t1.40546510811\n",
      "  (2, 12)\t2.09861228867\n",
      "  (2, 13)\t2.09861228867\n",
      "  (2, 7)\t2.38629436112\n",
      "  (3, 1)\t2.79175946923\n",
      "  (3, 2)\t2.79175946923\n",
      "  (4, 6)\t2.79175946923\n",
      "  (4, 8)\t2.79175946923\n",
      "  (5, 3)\t1.40546510811\n",
      "  (5, 12)\t2.09861228867\n",
      "  (5, 13)\t2.09861228867\n",
      "  (5, 7)\t2.38629436112\n",
      "  (6, 3)\t1.40546510811\n",
      "  (7, 3)\t1.40546510811\n",
      "  (8, 3)\t1.40546510811\n",
      "  (9, 3)\t1.40546510811\n",
      "  (10, 11)\t2.79175946923\n",
      "  (10, 3)\t1.40546510811\n",
      "  (10, 12)\t2.09861228867\n",
      "  (10, 13)\t2.09861228867\n",
      "{u'standing': 8, u'right': 6, u'darkness': 1, u'hour': 2, u'whisper': 11, u'times': 9, u'let': 3, u'speaking': 7, u'words': 13, u'mother': 5, u'trouble': 10, u'wisdom': 12, u'mary': 4, u'comes': 0}\n",
      "[ 2.79175947  2.79175947  2.79175947  1.40546511  2.79175947  2.79175947\n",
      "  2.79175947  2.38629436  2.79175947  2.79175947  2.79175947  2.79175947\n",
      "  2.09861229  2.09861229]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, stop_words='english',norm = None)\n",
    "\n",
    "print vectorizer.fit_transform(doc)\n",
    "print vectorizer.vocabulary_\n",
    "print vectorizer.idf_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* doc를 list of list로 만들어, DataFrame을 생성한다.\n",
    "* schema는 만들어 주지 않아도 된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+\n",
      "|sent                                  |\n",
      "+--------------------------------------+\n",
      "|When I find myself in times of trouble|\n",
      "|Mother Mary comes to me               |\n",
      "|Speaking words of wisdom, let it be   |\n",
      "|And in my hour of darkness            |\n",
      "|She is standing right in front of me  |\n",
      "|Speaking words of wisdom, let it be   |\n",
      "|우리 Let it be                          |\n",
      "|나 Let it be                           |\n",
      "|너 Let it be                           |\n",
      "|Let it be                             |\n",
      "|Whisper words of wisdom, let it be    |\n",
      "+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc=[\n",
    "    [\"When I find myself in times of trouble\"],\n",
    "    [\"Mother Mary comes to me\"],\n",
    "    [\"Speaking words of wisdom, let it be\"],\n",
    "    [\"And in my hour of darkness\"],\n",
    "    [\"She is standing right in front of me\"],\n",
    "    [\"Speaking words of wisdom, let it be\"],\n",
    "    [u\"우리 Let it be\"],\n",
    "    [u\"나 Let it be\"],\n",
    "    [u\"너 Let it be\"],\n",
    "    [\"Let it be\"],\n",
    "    [\"Whisper words of wisdom, let it be\"]\n",
    "]\n",
    "\n",
    "myDf=spark.createDataFrame(doc,['sent'])\n",
    "myDf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.5.4 StringIndexer\n",
    "\n",
    "* 문자를 인덱스 값, double로 변환된다.\n",
    "\n",
    "구분 | 설명 | 예\n",
    "-----|-----|-----\n",
    "nominal | 명목 또는 구분 값 cateogry  | 사자, 호랑이, 사람\n",
    "ordinal | 명목값과 다른 점은 순서가 있다. | 키 low, med, high\n",
    "interval | 일정한 간격이 있다. | 150-165, 165-180, 180-195\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|                sent|sentLabel|\n",
      "+--------------------+---------+\n",
      "|When I find mysel...|      9.0|\n",
      "|Mother Mary comes...|      8.0|\n",
      "|Speaking words of...|      0.0|\n",
      "|And in my hour of...|      5.0|\n",
      "|She is standing r...|      4.0|\n",
      "|Speaking words of...|      0.0|\n",
      "|        우리 Let it be|      6.0|\n",
      "|         나 Let it be|      1.0|\n",
      "|         너 Let it be|      2.0|\n",
      "|           Let it be|      7.0|\n",
      "|Whisper words of ...|      3.0|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "labelIndexer = StringIndexer(inputCol=\"sent\", outputCol=\"sentLabel\")\n",
    "model=labelIndexer.fit(myDf)\n",
    "siDf=model.transform(myDf)\n",
    "siDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### S.5.5 Tokenizer\n",
    "\n",
    "* 문장을 단어와 같은 token으로 분리한다.\n",
    "* 단어는 배열로 구성한다. 요소는 string이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(sent=u'When I find myself in times of trouble', words=[u'when', u'i', u'find', u'myself', u'in', u'times', u'of', u'trouble'])\n",
      "Row(sent=u'Mother Mary comes to me', words=[u'mother', u'mary', u'comes', u'to', u'me'])\n",
      "Row(sent=u'Speaking words of wisdom, let it be', words=[u'speaking', u'words', u'of', u'wisdom,', u'let', u'it', u'be'])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"sent\", outputCol=\"words\")\n",
    "tokDf = tokenizer.transform(myDf)\n",
    "for r in tokDf.select(\"sent\", \"words\").take(3):\n",
    "    print r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### S.5.6 RegTokenizer\n",
    "\n",
    "* 단어를 분리하기 위한 패턴을 적용할 수 있다.\n",
    "* 한글에는 \\w 패턴이 적용되지 않는다. \n",
    "* whitespace \\s 패턴을 적용한다. 공백, TAB, CR, New Line 등이 해당된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                sent|            wordsReg|\n",
      "+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|\n",
      "|Mother Mary comes...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|\n",
      "|And in my hour of...|[and, in, my, hou...|\n",
      "|She is standing r...|[she, is, standin...|\n",
      "|Speaking words of...|[speaking, words,...|\n",
      "|        우리 Let it be|   [우리, let, it, be]|\n",
      "|         나 Let it be|    [나, let, it, be]|\n",
      "|         너 Let it be|    [너, let, it, be]|\n",
      "|           Let it be|       [let, it, be]|\n",
      "|Whisper words of ...|[whisper, words, ...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "re = RegexTokenizer(inputCol=\"sent\", outputCol=\"wordsReg\", pattern=\"\\\\s+\")\n",
    "reDf=re.transform(myDf)\n",
    "reDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### S.5.7 Stopwords\n",
    "\n",
    "* 한 단어 등 불용어.\n",
    "* http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "stop = StopWordsRemover(inputCol=\"wordsReg\", outputCol=\"nostops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 현재 stop words에 자신의 것을 추가해서, 재설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopWordsRemover_44dba16d6fe647309715"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords=list()\n",
    "_stopwords=stop.getStopWords()\n",
    "for e in _stopwords:\n",
    "    stopwords.append(e)\n",
    "\n",
    "_mystopwords=[u\"나\",u\"너\", u\"우리\"]\n",
    "for e in _mystopwords:\n",
    "    stopwords.append(e)\n",
    "stop.setStopWords(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i me my myself we our ours ourselves you your yours yourself yourselves he him his himself she her hers herself it its itself they them their theirs themselves what which who whom this that these those am is are was were be been being have has had having do does did doing a an the and but if or because as until while of at by for with about against between into through during before after above below to from up down in out on off over under again further then once here there when where why how all any both each few more most other some such no nor not only own same so than too very s t can will just don should now d ll m o re ve y ain aren couldn didn doesn hadn hasn haven isn ma mightn mustn needn shan shouldn wasn weren won wouldn 나 너 우리\n"
     ]
    }
   ],
   "source": [
    "for e in stop.getStopWords():\n",
    "    print e,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 한글의 stop words '너','우리'가 제거되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                sent|            wordsReg|             nostops|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|[find, times, tro...|\n",
      "|Mother Mary comes...|[mother, mary, co...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|\n",
      "|And in my hour of...|[and, in, my, hou...|    [hour, darkness]|\n",
      "|She is standing r...|[she, is, standin...|[standing, right,...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|\n",
      "|        우리 Let it be|   [우리, let, it, be]|               [let]|\n",
      "|         나 Let it be|    [나, let, it, be]|               [let]|\n",
      "|         너 Let it be|    [너, let, it, be]|               [let]|\n",
      "|           Let it be|       [let, it, be]|               [let]|\n",
      "|Whisper words of ...|[whisper, words, ...|[whisper, words, ...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopDf=stop.transform(reDf)\n",
    "stopDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### S.5.8 CountVectorizer\n",
    "\n",
    "* 입력: a collection of text documents\n",
    "* 출력: word vector (sparse) vocabulary x TF\n",
    "* tokenize하고 나서 사용\n",
    "* minDF\n",
    "    * 소수점은 비율, 사용된 문서 수/전체 문서 수\n",
    "        * 정수는 사용된 문서 수, 단어가 몇 개의 문서에 사용되어야 하는지\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                sent|             nostops|                  cv|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[find, times, tro...|(16,[4,9,11],[1.0...|\n",
      "|Mother Mary comes...|[mother, mary, co...|(16,[7,8,10],[1.0...|\n",
      "|Speaking words of...|[speaking, words,...|(16,[0,1,2,3],[1....|\n",
      "|And in my hour of...|    [hour, darkness]|(16,[5,15],[1.0,1...|\n",
      "|She is standing r...|[standing, right,...|(16,[6,13,14],[1....|\n",
      "|Speaking words of...|[speaking, words,...|(16,[0,1,2,3],[1....|\n",
      "|        우리 Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|         나 Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|         너 Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|           Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|Whisper words of ...|[whisper, words, ...|(16,[0,1,2,12],[1...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n",
      "let words wisdom, speaking find darkness front mary mother trouble comes times whisper right standing hour\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"nostops\", outputCol=\"cv\",\n",
    "    vocabSize=30,minDF=1.0)\n",
    "cvModel = cv.fit(stopDf)\n",
    "cvDf = cvModel.transform(stopDf)\n",
    "\n",
    "cvDf.collect()\n",
    "cvDf.select('sent','nostops','cv').show()\n",
    "for v in cvModel.vocabulary:\n",
    "    print v,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### S.5.9 TF-IDF\n",
    "\n",
    "* Term frequency-inverse document frequency (TF-IDF)\n",
    "* tokenizer하고 나서 사용해야 함. \n",
    "*  HashingTF  고정길이 word vectors.\n",
    "* IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(nostops=[u'find', u'times', u'trouble'], hash=SparseVector(50, {10: 1.0, 24: 1.0, 43: 1.0}))\n",
      "Row(nostops=[u'mother', u'mary', u'comes'], hash=SparseVector(50, {1: 1.0, 21: 1.0, 24: 1.0}))\n",
      "Row(nostops=[u'speaking', u'words', u'wisdom,', u'let'], hash=SparseVector(50, {9: 1.0, 12: 1.0, 14: 1.0, 41: 1.0}))\n",
      "Row(nostops=[u'hour', u'darkness'], hash=SparseVector(50, {23: 1.0, 27: 1.0}))\n",
      "Row(nostops=[u'standing', u'right', u'front'], hash=SparseVector(50, {24: 1.0, 43: 1.0, 46: 1.0}))\n",
      "Row(nostops=[u'speaking', u'words', u'wisdom,', u'let'], hash=SparseVector(50, {9: 1.0, 12: 1.0, 14: 1.0, 41: 1.0}))\n",
      "Row(nostops=[u'let'], hash=SparseVector(50, {14: 1.0}))\n",
      "Row(nostops=[u'let'], hash=SparseVector(50, {14: 1.0}))\n",
      "Row(nostops=[u'let'], hash=SparseVector(50, {14: 1.0}))\n",
      "Row(nostops=[u'let'], hash=SparseVector(50, {14: 1.0}))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "hashTF = HashingTF(inputCol=\"nostops\", outputCol=\"hash\", numFeatures=50)\n",
    "hashDf = hashTF.transform(stopDf)\n",
    "idf = IDF(inputCol=\"hash\", outputCol=\"idf\")\n",
    "idfModel = idf.fit(hashDf)\n",
    "idfDf = idfModel.transform(hashDf)\n",
    "for e in idfDf.select(\"nostops\",\"hash\").take(10):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### S.5.10 Word2Vec\n",
    "\n",
    "* see wikipedia https://en.wikipedia.org/wiki/Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(w2v=DenseVector([-0.0367, 0.0097, 0.0479]))\n",
      "Row(w2v=DenseVector([-0.0482, 0.0223, 0.0095]))\n",
      "Row(w2v=DenseVector([0.052, -0.001, -0.0019]))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "word2Vec = Word2Vec(vectorSize=3,minCount=0,inputCol=\"words\",outputCol=\"w2v\")\n",
    "model = word2Vec.fit(tokDf)\n",
    "w2vDf = model.transform(tokDf)\n",
    "for e in w2vDf.select(\"w2v\").take(3):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### S.5.11 NGram\n",
    "\n",
    "* unigram은 한 단어로, bigram은 두 단어로 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                sent|               words|              ngrams|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|[when i, i find, ...|\n",
      "|Mother Mary comes...|[mother, mary, co...|[mother mary, mar...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking words, ...|\n",
      "|And in my hour of...|[and, in, my, hou...|[and in, in my, m...|\n",
      "|She is standing r...|[she, is, standin...|[she is, is stand...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking words, ...|\n",
      "|        우리 Let it be|   [우리, let, it, be]|[우리 let, let it, ...|\n",
      "|         나 Let it be|    [나, let, it, be]|[나 let, let it, i...|\n",
      "|         너 Let it be|    [너, let, it, be]|[너 let, let it, i...|\n",
      "|           Let it be|       [let, it, be]|     [let it, it be]|\n",
      "|Whisper words of ...|[whisper, words, ...|[whisper words, w...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n",
      "Row(words=[u'when', u'i', u'find', u'myself', u'in', u'times', u'of', u'trouble'], ngrams=[u'when i', u'i find', u'find myself', u'myself in', u'in times', u'times of', u'of trouble'])\n",
      "Row(words=[u'mother', u'mary', u'comes', u'to', u'me'], ngrams=[u'mother mary', u'mary comes', u'comes to', u'to me'])\n",
      "Row(words=[u'speaking', u'words', u'of', u'wisdom,', u'let', u'it', u'be'], ngrams=[u'speaking words', u'words of', u'of wisdom,', u'wisdom, let', u'let it', u'it be'])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "ngramDf = ngram.transform(tokDf)\n",
    "ngramDf.show()\n",
    "for e in ngramDf.select(\"words\",\"ngrams\").take(3):\n",
    "    print e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### S.5.12 연속데이터의 변환\n",
    "\n",
    "몸무게(inches), 키(pounds) 데이터를 분석해보자.\n",
    "이 데이터는 정량, 연속 데이터이다. \n",
    "출처는 https://people.sc.fsu.edu/~jburkardt/data/csv/hw_200.csv\n",
    "\n",
    "```python\n",
    "1\t65.78\t112.99\n",
    "2\t71.52\t136.49\n",
    "3\t69.40\t153.03\n",
    "4\t68.22\t142.34\n",
    "5\t67.79\t144.30\n",
    "6\t68.70\t123.30\n",
    "7\t69.80\t141.49\n",
    "8\t70.01\t136.46\n",
    "9\t67.90\t112.37\n",
    "10\t66.78\t120.67\n",
    "11\t66.49\t127.45\n",
    "12\t67.62\t114.14\n",
    "13\t68.30\t125.61\n",
    "14\t67.12\t122.46\n",
    "15\t68.28\t116.09\n",
    "16\t71.09\t140.00\n",
    "17\t66.46\t129.50\n",
    "18\t68.65\t142.97\n",
    "19\t71.23\t137.90\n",
    "20\t67.13\t124.04\n",
    "21\t67.83\t141.28\n",
    "22\t68.88\t143.54\n",
    "23\t63.48\t97.90\n",
    "24\t68.42\t129.50\n",
    "25\t67.63\t141.85\n",
    "26\t67.21\t129.72\n",
    "27\t70.84\t142.42\n",
    "28\t67.49\t131.55\n",
    "29\t66.53\t108.33\n",
    "30\t65.44\t113.89\n",
    "31\t69.52\t103.30\n",
    "32\t65.81\t120.75\n",
    "33\t67.82\t125.79\n",
    "34\t70.60\t136.22\n",
    "35\t71.80\t140.10\n",
    "36\t69.21\t128.75\n",
    "37\t66.80\t141.80\n",
    "38\t67.66\t121.23\n",
    "39\t67.81\t131.35\n",
    "40\t64.05\t106.71\n",
    "41\t68.57\t124.36\n",
    "42\t65.18\t124.86\n",
    "43\t69.66\t139.67\n",
    "44\t67.97\t137.37\n",
    "45\t65.98\t106.45\n",
    "46\t68.67\t128.76\n",
    "47\t66.88\t145.68\n",
    "48\t67.70\t116.82\n",
    "49\t69.82\t143.62\n",
    "50\t69.09\t134.93\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "rdd=spark.sparkContext\\\n",
    "    .textFile(os.path.join('data','ds_spark_heightweight.txt'))\n",
    "\n",
    "myRdd=rdd.map(lambda line:[float(x) for x in line.split('\\t')])\n",
    "myDf=spark.createDataFrame(myRdd,[\"id\",\"weight\",\"height\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+\n",
      "|  id|weight|height|weight2|\n",
      "+----+------+------+-------+\n",
      "| 1.0| 65.78|112.99|    0.0|\n",
      "| 2.0| 71.52|136.49|    1.0|\n",
      "| 3.0|  69.4|153.03|    1.0|\n",
      "| 4.0| 68.22|142.34|    1.0|\n",
      "| 5.0| 67.79| 144.3|    0.0|\n",
      "| 6.0|  68.7| 123.3|    1.0|\n",
      "| 7.0|  69.8|141.49|    1.0|\n",
      "| 8.0| 70.01|136.46|    1.0|\n",
      "| 9.0|  67.9|112.37|    0.0|\n",
      "|10.0| 66.78|120.67|    0.0|\n",
      "+----+------+------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "binarizer = Binarizer(threshold=68.0, inputCol=\"weight\", outputCol=\"weight2\")\n",
    "binDf = binarizer.transform(myDf)\n",
    "binDf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+-------+\n",
      "|  id|weight|height|weight2|height3|\n",
      "+----+------+------+-------+-------+\n",
      "| 1.0| 65.78|112.99|    0.0|    0.0|\n",
      "| 2.0| 71.52|136.49|    1.0|    1.0|\n",
      "| 3.0|  69.4|153.03|    1.0|    2.0|\n",
      "| 4.0| 68.22|142.34|    1.0|    2.0|\n",
      "| 5.0| 67.79| 144.3|    0.0|    2.0|\n",
      "| 6.0|  68.7| 123.3|    1.0|    0.0|\n",
      "| 7.0|  69.8|141.49|    1.0|    2.0|\n",
      "| 8.0| 70.01|136.46|    1.0|    1.0|\n",
      "| 9.0|  67.9|112.37|    0.0|    0.0|\n",
      "|10.0| 66.78|120.67|    0.0|    0.0|\n",
      "+----+------+------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\n",
    "discretizer = QuantileDiscretizer(numBuckets=3, inputCol=\"height\", outputCol=\"height3\")\n",
    "qdDf = discretizer.fit(binDf).transform(binDf)\n",
    "qdDf.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 5.5.13 VectorAssembler\n",
    "\n",
    "* 열을 묶어서 Vector열로 만든다.\n",
    "* string은 묶을 수 없다.\n",
    "* pyspark.ml.linalg.Vectors를 사용한다. (주의: pyspark.mllib.linalg.Vectors를 사용하지 않는다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- weight2: double (nullable = true)\n",
      " |-- height3: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+---+------+------+-------+-------+---------+\n",
      "| id|weight|height|weight2|height3| features|\n",
      "+---+------+------+-------+-------+---------+\n",
      "|1.0| 65.78|112.99|    0.0|    0.0|(2,[],[])|\n",
      "|2.0| 71.52|136.49|    1.0|    1.0|[1.0,1.0]|\n",
      "|3.0|  69.4|153.03|    1.0|    2.0|[1.0,2.0]|\n",
      "|4.0| 68.22|142.34|    1.0|    2.0|[1.0,2.0]|\n",
      "|5.0| 67.79| 144.3|    0.0|    2.0|[0.0,2.0]|\n",
      "+---+------+------+-------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "va = VectorAssembler(inputCols=[\"weight2\",\"height3\"],outputCol=\"features\")\n",
    "vaDf = va.transform(qdDf)\n",
    "vaDf.printSchema()\n",
    "vaDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### S.5.14 Pipeline\n",
    "\n",
    "* Pipeline은 여러 작업을 묶어, 순서대로 단계적으로 처리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0L, \"a b c d e spark\", 1.0),\n",
    "    (1L, \"b d\", 0.0),\n",
    "    (2L, \"spark f g h\", 1.0),\n",
    "    (3L, \"hadoop mapreduce\", 0.0),\n",
    "    (4L, \"my dog has flea problems. help please.\",0.0)\n",
    "    ], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(df)\n",
    "myDf = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
