{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "home=os.path.expanduser(\"~\")\n",
    "os.environ[\"SPARK_HOME\"]=os.path.join(home, 'spark-2.0.0-bin-hadoop2.7')\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "# python과 자바를 연결하는 파일을 경로 설정에 집어 넣는다. # pip install py4j\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))\n",
    "# pyspark.zip을 경로 설정에 집어 넣는다. # pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder.master(\"local\").appName(\"myApp\").config(conf=myConf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'file:${system:user.dir}/spark-warehouse'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.sql.warehouse.dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "pyspark-shell\n",
      "local[*]\n",
      "192.168.214.177\n"
     ]
    }
   ],
   "source": [
    "print spark.version\n",
    "print spark.conf.get('spark.app.name')\n",
    "print spark.conf.get('spark.master')\n",
    "print spark.conf.get('spark.driver.host')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList=[1,2,3,4,5,6,7]\n",
    "myRdd1 = spark.sparkContext.parallelize(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(myRdd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd1.collect()[:3]\n",
    "# list => [], dictonary => {}, tuple => ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd1.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/ds_spark_wiki.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_wiki.txt\n",
    "Wikipedia\n",
    "Apache Spark is an open source cluster computing framework.\n",
    "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
    "Apache Spark Apache Spark Apache Spark Apache Spark\n",
    "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
    "Originally developed at the University of California, Berkeley's AMPLab,\n",
    "the Spark codebase was later donated to the Apache Software Foundation,\n",
    "which has maintained it since.\n",
    "Spark provides an interface for programming entire clusters with\n",
    "implicit data parallelism and fault-tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd2=spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))\n",
    "# RDD 방식으로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(myRdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Wikipedia'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(value=u'Wikipedia')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "myDf = spark.read.text(os.path.join(\"data\", \"ds_spark_wiki.txt\"))\n",
    "# Spark Dataframe으로 만든다. Dataframe은 줄단위 숫자에 적합하다.\n",
    "print (myDf.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(myDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data/ds_spark_2cols.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/ds_spark_2cols.csv\n",
    "35, 2\n",
    "40, 27\n",
    "12, 38\n",
    "15, 31\n",
    "21, 1\n",
    "14, 19\n",
    "46, 1\n",
    "10, 34\n",
    "28, 3\n",
    "48, 1\n",
    "16, 2\n",
    "30, 3\n",
    "32, 2\n",
    "48, 1\n",
    "31, 2\n",
    "22, 1\n",
    "12, 3\n",
    "39, 29\n",
    "19, 37\n",
    "25, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd4 = spark.sparkContext.textFile(os.path.join(\"data\", \"ds_spark_2cols.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'35, 2',\n",
       " u'40, 27',\n",
       " u'12, 38',\n",
       " u'15, 31',\n",
       " u'21, 1',\n",
       " u'14, 19',\n",
       " u'46, 1',\n",
       " u'10, 34',\n",
       " u'28, 3',\n",
       " u'48, 1',\n",
       " u'16, 2',\n",
       " u'30, 3',\n",
       " u'32, 2',\n",
       " u'48, 1',\n",
       " u'31, 2',\n",
       " u'22, 1',\n",
       " u'12, 3',\n",
       " u'39, 29',\n",
       " u'19, 37',\n",
       " u'25, 2']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'35, 2', u'40, 27', u'12, 38', u'15, 31', u'21, 1']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd4.take(5) #출력은 리스트, 출력결과는 문자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35, 2\n",
      "40, 27\n",
      "12, 38\n",
      "15, 31\n",
      "21, 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myList = myRdd4.take(5)\n",
    "for i in myList:\n",
    "    print (i)\n",
    "type(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/rational331/201511084_BigData'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rational331/201511084_BigData/data\n"
     ]
    }
   ],
   "source": [
    "print (os.path.join(os.getcwd(), \"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'35', u' 2'],\n",
       " [u'40', u' 27'],\n",
       " [u'12', u' 38'],\n",
       " [u'15', u' 31'],\n",
       " [u'21', u' 1'],\n",
       " [u'14', u' 19'],\n",
       " [u'46', u' 1'],\n",
       " [u'10', u' 34'],\n",
       " [u'28', u' 3'],\n",
       " [u'48', u' 1'],\n",
       " [u'16', u' 2'],\n",
       " [u'30', u' 3'],\n",
       " [u'32', u' 2'],\n",
       " [u'48', u' 1'],\n",
       " [u'31', u' 2'],\n",
       " [u'22', u' 1'],\n",
       " [u'12', u' 3'],\n",
       " [u'39', u' 29'],\n",
       " [u'19', u' 37'],\n",
       " [u'25', u' 2']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map은 각 요소에 대해 적용\n",
    "myRdd5 = myRdd4.map(lambda line : line.split(','))\n",
    "myRdd5.collect() # 2차원 리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD API\n",
    "## RDD는 Transformations데이터 변환, Actions액션 2가지로 분류됨\n",
    "## Dataframe에서는 Transformer, Estimator로 구분됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flatMap은 2차원을 1차원으로 줄일 때\n",
    "# groupByKey 그룹화함\n",
    "\n",
    "# OLTP (계정계) \n",
    "# OLAP(정보계)=> 기간'별'로 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변환은 lazy연산을 한다. 즉, 실제 변환은 action이 수행되는 시점까지 늦추어진다. 리턴값은 RDD이다.\n",
    "actions은 리스트로 값을 리턴한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "celsius=[39.2, 36.5, 37.3, 37.8]\n",
    "def c2f(c):\n",
    "    f=list()\n",
    "    for i in c:\n",
    "        _f=(float(9)/5) * i + 32\n",
    "        f.append(_f)\n",
    "    return f;\n",
    "\n",
    "print c2f(celsius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "celsius=[39.2, 36.5, 37.3, 37.8]\n",
    "def c2f(c):\n",
    "    return (float(9)/5)*c + 32\n",
    "\n",
    "f=map(c2f, celsius)\n",
    "#함수를 인자로 전달함 # map함수는 각각의 원소에 대해서 적용한다.\n",
    "print f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x*2\n",
    "\n",
    "y=f(1)\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "y=lambda i : i*2\n",
    "print y(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[102.56, 97.7, 99.14, 100.03999999999999]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celsius=[39.2, 36.5, 37.3, 37.8]\n",
    "map(lambda c : float(9)/5*c + 32, celsius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['H'],\n",
       " ['e'],\n",
       " ['l'],\n",
       " ['l'],\n",
       " ['o'],\n",
       " [','],\n",
       " [],\n",
       " ['W'],\n",
       " ['o'],\n",
       " ['r'],\n",
       " ['l'],\n",
       " ['d'],\n",
       " ['!']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hello, World!\"\n",
    "map(lambda c : c.split(), sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hello', 'World!'], ['Good', 'Morning']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence=[\"Hello World!\", \"Good Morning\"]\n",
    "map(lambda c : c.split(), sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 3, 5, 13, 21, 55]\n"
     ]
    }
   ],
   "source": [
    "fib = [0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55]\n",
    "result = filter(lambda x : (x % 2) == 1, fib)\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda x, y: x+y, range(1, 101))\n",
    "\n",
    "\n",
    "#1  2  3  4  5  6  7  8  9\n",
    "#| /  /  /\n",
    "# 3  6  10  ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[15] at RDD at PythonRDD.scala:48\n"
     ]
    }
   ],
   "source": [
    "nRdd = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "squared = nRdd.map(lambda x: x * x)\n",
    "print (squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16]\n"
     ]
    }
   ],
   "source": [
    "print (squared.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd100 = spark.sparkContext.parallelize(range(1, 101))\n",
    "myRdd100.reduce(lambda x, y : x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum:  10\n",
      "min:  1\n",
      "max:  4\n",
      "standard deviation: 1.11803398875\n",
      "variance:  1.25\n"
     ]
    }
   ],
   "source": [
    "print \"sum: \",nRdd.sum()\n",
    "print \"min: \",nRdd.min()\n",
    "print \"max: \", nRdd.max()\n",
    "print \"standard deviation:\", nRdd.stdev()\n",
    "print \"variance: \", nRdd.variance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 조건에 맞는 문장 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many lines having 'Spark':  4\n"
     ]
    }
   ],
   "source": [
    "myRdd_spark=myRdd2.filter(lambda line: \"Spark\" in line)\n",
    "print \"How many lines having 'Spark': \",myRdd_spark.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 들어 있는 문장 찾기 한글일 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n"
     ]
    }
   ],
   "source": [
    "myRdd_unicode = myRdd2.filter(lambda line: u\"스파크\" in line)\n",
    "print myRdd_unicode.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stopwords 제거하기\n",
    "* stopwords란 의미 없는 대명사(이, 그, 저) 또는 한 글자 단어(등..) 불용어를 의미한다.\n",
    "* 한글은 유니코드로 처리해야한다.\n",
    "\n",
    "# flatMap과 filter을 연달아 쓴 것에 유의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia\n",
      "Apache\n",
      "Spark\n",
      "open\n",
      "source\n",
      "cluster\n",
      "computing\n",
      "framework.\n",
      "아파치\n",
      "스파크는\n",
      "오픈\n",
      "소스\n",
      "클러스터\n",
      "컴퓨팅\n",
      "프레임워크이다.\n",
      "Apache\n",
      "Spark\n",
      "Apache\n",
      "Spark\n",
      "Apache\n",
      "Spark\n",
      "Apache\n",
      "Spark\n",
      "아파치\n",
      "스파크\n",
      "아파치\n",
      "스파크\n",
      "아파치\n",
      "스파크\n",
      "아파치\n",
      "스파크\n",
      "Originally\n",
      "developed\n",
      "University\n",
      "of\n",
      "California,\n",
      "Berkeley's\n",
      "AMPLab,\n",
      "Spark\n",
      "codebase\n",
      "was\n",
      "later\n",
      "donated\n",
      "to\n",
      "Apache\n",
      "Software\n",
      "Foundation,\n",
      "which\n",
      "has\n",
      "maintained\n",
      "it\n",
      "since.\n",
      "Spark\n",
      "provides\n",
      "interface\n",
      "programming\n",
      "entire\n",
      "clusters\n",
      "with\n",
      "implicit\n",
      "data\n",
      "parallelism\n",
      "and\n",
      "fault-tolerance.\n"
     ]
    }
   ],
   "source": [
    "stopwords = ['is','am','are','the','for','a', 'an', 'at']\n",
    "myRdd_stop = myRdd2.flatMap(lambda x:x.split()).filter(lambda x: x not in stopwords)\n",
    "# flatMap과 filter 연달아 씀\n",
    "for words in myRdd_stop.collect():\n",
    "    print words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# foreach()는 반환 값이 없지만 각각에 대해서 적용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x): print(\"> \", x)\n",
    "spark.sparkContext.parallelize([1, 2, 3, 4, 5]).foreach(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize([1, 2, 3, 4, 5]).map(lambda x: x + 1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2=spark.sparkContext.textFile(os.path.join(\"data\", \"ds_spark_wiki.txt\"))\n",
    "sentences  = myRdd2.map(lambda x : x.split(\" \"));\n",
    "sentences.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mySplit(x):\n",
    "    return x.split(\" \")\n",
    "\n",
    "sentences2 = myRdd2.map(mySplit)\n",
    "sentences2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Wikipedia'],\n",
       " [u'Apache',\n",
       "  u'Spark',\n",
       "  u'is',\n",
       "  u'an',\n",
       "  u'open',\n",
       "  u'source',\n",
       "  u'cluster',\n",
       "  u'computing',\n",
       "  u'framework.'],\n",
       " [u'\\uc544\\ud30c\\uce58',\n",
       "  u'\\uc2a4\\ud30c\\ud06c\\ub294',\n",
       "  u'\\uc624\\ud508',\n",
       "  u'\\uc18c\\uc2a4',\n",
       "  u'\\ud074\\ub7ec\\uc2a4\\ud130',\n",
       "  u'\\ucef4\\ud4e8\\ud305',\n",
       "  u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.']]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia \n",
      "-------------------------------------------------------------------\n",
      "Apache Spark is an open source cluster computing framework. \n",
      "-------------------------------------------------------------------\n",
      "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다. \n",
      "-------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for line in sentences.take(3):\n",
    "    for word in line:\n",
    "        print word, \n",
    "    print \"\\n-------------------------------------------------------------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"Apache Spark is an open source cluster computing framework.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(u\"아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 59, 32, 51, 31, 72, 71, 30, 64, 46]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.map(lambda s:len(s)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is'], ['a', 'line']]\n"
     ]
    }
   ],
   "source": [
    "myList=[\"this is\",\"a line\"]\n",
    "_rdd=spark.sparkContext.parallelize(myList)\n",
    "wordsRdd=_rdd.map(lambda x:x.split())\n",
    "print wordsRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is', 'AA line']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repRdd=_rdd.map(lambda x:x.replace(\"a\",\"AA\"))\n",
    "repRdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 바로 밑 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'line']\n"
     ]
    }
   ],
   "source": [
    "myList=[\"this is\",\"a line\"]\n",
    "_rdd=spark.sparkContext.parallelize(myList)\n",
    "wordsRdd=_rdd.flatMap(lambda x:x.split())\n",
    "print wordsRdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THIS', 'A']\n"
     ]
    }
   ],
   "source": [
    "upperRDD =wordsRdd.map(lambda x: x[0].upper())\n",
    "print upperRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['THIS', 'IS'], ['A', 'LINE']]\n"
     ]
    }
   ],
   "source": [
    "upper2RDD =wordsRdd.map(lambda x: [i.upper() for i in x])\n",
    "print upper2RDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
