{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark 시작하기\n",
    "\n",
    "* Last Updated on 20190909MON1100 20181112_20180430_20171203_20160412\n",
    "* [ok] prebuilt 설치\n",
    "    * Spark2.0 for Hadoop2.7 (Python 2.7x)\n",
    "    * Spark2.2 for Hadoop2.7 (Python 3.6x)\n",
    "    * Spark1.6 for Hadoop2.6\n",
    "\n",
    "## S.1 학습내용\n",
    "\n",
    "### S.1.1 목표\n",
    "\n",
    "* Spark를 설치할 수 있다.\n",
    "* SparkSession을 생성하여 Spark를 사용할 수 있다.\n",
    "* Spark의 설정을 변경할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.1.2 목차\n",
    "\n",
    "* S.2 Why Spark?\n",
    "* S.2.1 빅데이터 처리\n",
    "* S.2.2 Hadoop과 Spark\n",
    "* S.2.3 Spark 프레임워크\n",
    "* S.3 설치\n",
    "* S.3.1 사전 설치\n",
    "* S.3.2 설치 방법\n",
    "* S.4 실행\n",
    "* S.4.1 일괄실행\n",
    "* S.4.2 interactive shell\n",
    "* S.4.3 오류\n",
    "* S.5 ipython shell로 spark사용하기\n",
    "* S.6 Jupyter Notebook으로 Spark사용하기\n",
    "클러스터 구성\n",
    "* S.7 설정\n",
    "* S.7.1 필요한 jar를 추가\n",
    "* S.7.2 log\n",
    "* S.8 spark-sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.1.3 문제\n",
    "\n",
    "* S-1: Spark 시작하기\n",
    "* S-2: Spark 클러스터를 시작하기\n",
    "* S-3: MongoDB를 사용하기 위한 jar를 추가하는 설정 변경하기\n",
    "* S-4: sqlite를 사용하기 위한 jar를 추가하는 설정 변경하기\n",
    "* 개설 강좌\n",
    "    * 버클리대학 BerkeleyX: CS100.1x Introduction to Big Data with Apache Spark\n",
    "    * 시라큐스대학 IST718: Advanced Information Analytics http://classes.ischool.syr.edu/ist718/\n",
    "    * 듀크대학 STA663 Statistical Computing and Computation, Spring 2016\n",
    "        * https://github.com/cliburn/sta-663-2016\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.2 Why Spark?\n",
    "\n",
    "### S.2.1 빅데이터 처리\n",
    "\n",
    "* 빅데이터는 그 이름에서 알 수 있듯이, 과거와는 다르게 **규모가 크다**는 특징이 있다. 과거에는 메가바이트 규모라고 한다면, 그 몇 천배 이상으로 크다.\n",
    "* 또한 발생하는 **출처가 다양**하다. 종전에는 '숫자'로 된 재무제표, 시계열 데이터 등 정량적 데이터 또는 'Hard Data'가 많았다면, 빅데이터의 경우는 다르다. SNS에 사람들이 남긴 글, 센서에서 발생하는 데이터, 사람들의 핸드폰에서 발생하는 다양한 디지털 지문 등 다양한 형태로 발생한다. 발생하는 출처가 다양하므로, 그 데이터 형식이 **비구조적**이다. 소문이나 평판과 같은 'Soft Data'도 분석하는 틀을 정하기 어렵다.\n",
    "* 따라서 **한 대의 컴퓨터가 가지고 있는 저장용량으로는 감당할 수 없기 때문에 데이터가 분산**될 수 밖에 없다.\n",
    "* 그러면 당연히 데이터의 처리도 나누어서 수행되어야 한다. 따라서 이러한 분산작업을 제어하는 기능이 필요하게 된다. 대량의 데이터를 여러 컴퓨터에 나누어 처리하고, 집계하는 **맵리듀스 MapReduce**와 같은 알고리즘이 많이 사용되고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 거래를 처리하는 OLTP OnLine Transaction Processing과 구분하여, 분석을 위한 **OLAP OnLine Analytical Processing**이 데이터 웨어하우스 Data Warehouse, 데이터 마이닝 Data Mining과 같이 사용되고 있다.\n",
    "* 이러한 데이터웨어하우스를 구성하기 위해 **ETL (Extract, Transfrom and Load)** 작업이 필요하다. 빅데이터는 이 뿐만 아니라 데이터 수집 및 분석도 중요하다. 다양한 출처에서 실시간 발생하는 대량의 데이터에서 '집단지성'을 분석해내는 것이 '빅데이터'인 것이다.\n",
    "\n",
    "단계 | 설명\n",
    "-----|-----\n",
    "Extract | 다양한 입력에서 데이터 추출 (HDFS, files, JSON, DB, ...)\n",
    "Transform | 데이터 변환.\n",
    "Load | 다양한 형식의 저장 및 예측, 분류, 추천모델 등 지원 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.2.2 Hadoop과 Spark\n",
    "\n",
    "* 데이터가 대량으로 증가하면서, 이를 처리하기 위한 분산 프레임워크 Hadoop이 개발되어 쓰이고 있다.\n",
    "* 최근에는 Spark의 사용이 늘어나고 있어서 Hadoop과 비교되고 있다.\n",
    "* Hadoop은 데이터를 **수집**하는 목적으로 주로 사용된다.\n",
    "    * Hadoop 자체의 파일시스템 HDFS (Hadoop Distributed File System)와\n",
    "    * **맵리듀스map-reduce** 작업으로 데이터를 추출할 수 있다.\n",
    "* 반면 Spark는 수집한 데이터를 **분석**하는 용도로 사용된다.\n",
    "    * 자체 파일 시스템이 없고, RDD를 통해 Hadoop의 파일시스템 HDFS를 사용할 수 있다.\n",
    "    * Spark는 Machine Learning 라이브러리를 가지고 있다.\n",
    "    * Hadoop과 달리 메모리에서 처리하기 때문에 빠르다 (pipeline). 빅데이터를 빠르게 map-reduce 할 수 있다.\n",
    "\n",
    "구분 | Spark | Hadoop\n",
    "-------|-------|-------\n",
    "사용 목적 | 데이터 분석 | 데이터 수집\n",
    "파일 시스템 | 자체 파일 시스템이 없슴. hdfs, db, csv등을 사용 | hdfs\n",
    "속도 | 파이프라인을 사용하므로 빠름 | 보다 느림"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.2.3 Spark 프레임워크\n",
    "\n",
    "* 발전\n",
    "    * 2009년 UC Berkeley에서 Matei Zaharia가 박사학위 과정에서 개뱔\n",
    "    * 2010년 BSD 라이센스 오픈소스로 전환.\n",
    "    * 2013년 Apache 2.0 license로 전환\n",
    "    * 현재 개발자가 **Databricks**를 설립해서 관리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 분산 클러스터 컴퓨팅 프레임워크로서, API를 사용해서 데이터추출, 변환, 기계학습, 그래프분석을 할 수 있다.\n",
    "* Spark Core가 분산작업에 필요한 바탕이 되고, 그 위 **Sql, streaming, mllib, graphx**를 제공한다.\n",
    "* **Scala**로 개발되어 jvm에서 실행. 그러나 Scala, Java, Python, R 여러 언어를 섞어서 할 수 있는 환경을 제공 (polyglot)\n",
    "* REPL (Read Eval Print Looop)이 가능해서 배우기 쉽다. 쉘 환경이 있어 편리하다. Standalone으로 시작할 수 있다.\n",
    "\n",
    "구분 | 구성 | 설명\n",
    "-------|-------|-------\n",
    "Spark engine | Spark Core | 작업배분, 입출력 등 분산작업에 필요한 기능\n",
    "Spark Applicaiton Frameworks | Spark SQL | DataFrames\n",
    "| Spark Streaming | 실시간 처리\n",
    "| MLlib | 머신러닝 (참조 scikit-learn)\n",
    "|GraphX | 그래프 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.3 설치\n",
    "\n",
    "### S.3.1 사전 설치\n",
    "\n",
    "* Spark는 프로그래밍 언어를 다양하게 선택하여 사용할 수 있다.\n",
    "* 사용하는 언어에 따라 다르겠지만 Java, Scala, Python이 미리 설치되어 있어야 한다.\n",
    "* Pyspark를 사용하려면 Java와 Python을 반드시 설치해 놓아야 한다.\n",
    "\n",
    "구분 | Pyspark를 사용하려면 | 설명\n",
    "----|-----|-----\n",
    "Java | 필수 | Java 7+, Java SE Development Kit을 설치\n",
    "Python | 필수 | 2.6+/3.4+ (Spark 배포시기에 버전을 맞추는 편이 좋다.)\n",
    "Scala | 선택 (Scala사용하는 경우 필수) | Scala 2.11.x<br>리눅스 ```sudo apt install scala```<br>윈도우 scala-2.xx.x.msi 다운로드, 설치 및 환경설정 https://www.scala-lang.org/download/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.3.2  Spark 설치\n",
    "\n",
    "* **방법 1: prebuilt 바이너리를 내려 받아, 압축 풀고 저장**\n",
    "    * [Spark 다운로드](https://spark.apache.org/downloads.html)\n",
    "    * 하둡 HDFS를 사용할 수 있도록 하둡의 버전을 정해서 설치파일을 내려받고 압축을 풀어준다\n",
    "    (Hadoop없는 prebuilt를 설치하면 오류. 아래 참조)\n",
    "    * spark 2.0 hadoop2.7\n",
    "```python\n",
    "tar -xvzf spark-2.0.0-bin-hadoop2.7.tgz\n",
    "```\n",
    "\n",
    "* 방법 2: source code를 github에서 내려받아서 컴파일\n",
    "\n",
    "* 방법 3: apt로 설치하는 경우\n",
    "    * Ubuntu 16.04에서 제공되기 시작했다. 그러나 spark 실행에 필요한 shell을 설치하지 않는다.\n",
    "    \n",
    "```python\n",
    "sudo apt install spark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.3.3 경로설정\n",
    "\n",
    "* SPARK가 설치된 디렉토리에 맞게 설정한다.\n",
    "\n",
    "구분 | 리눅스 | 윈도우\n",
    "-----|-----|-----\n",
    "**SPARK_HOME** | ```export SPARK_HOME=$HOME/Downloads/spark-1.6.0-bin-hadoop2.6``` | setx SPARK_HOME \"C:\\Users\\jsl\\Downloads\\spark-1.6.0-bin-hadoop2.6\"\n",
    "**JAVA_HOME** | apt 설치한 경우 별도 설정x | setx JAVA_HOME \"C:\\Program Files\\Java\\jdk1.8.0\"\n",
    "**HADOOP_HOME** | 별도 설정x| - 윈도우에서는 Hadoop을 내려받아 설치하거나, **'winutils.exe'**를 설치해야 한다 (https://github.com/steveloughran/winutils/)<br>- %HADOOP_HOME%\\bin을 만들고 그 아래 'winutils.exe'를 복사해 놓음<br> 또는SPARK_HOME과 동일하게 설정 ```setx HADOOP_HOME %SPARK_HOME%```<br>- Hadoop이 사용하는 ```C:/tmp/hive``` 폴더를 생성하고, 명령창에서 **```winutils chmod 777 c:\\tmp\\hive```**해서 권한을 부여함\n",
    "**PATH** | ```export PATH=$SPARK_HOME/bin:$PATH``` | setx PATH \"%PATH%;%SPARK_HOME%\\bin;%JAVA_HOME%\\bin\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* ipython과 ipython notebook을 사용하기 위해서는 추가 경로를 설정한다. 아래와 같이 하면 명령 프롬프트에서 ```$ pyspark```라고 실행하면 notebook이 실행된다.\n",
    "\n",
    "구분 | 리눅스 | 윈도우\n",
    "-----|-----|-----\n",
    "python | ```export PYSPARK_PYTHON=ipython``` | ```sex PYSPARK_PYTHON ~/anaconda/bin/python```\n",
    "ipython 사용 | ```export PYSPARK_DRIVER_PYTHON=ipython``` | ```setx PYSPARK_DRIVER_PYTHON ~/anaconda/bin/ipython```\n",
    "ipython notebook 사용 | ```export PYSPARK_DRIVER_PYTHON_OPTS=notebook``` | ```setx PYSPARK_DRIVER_PYTHON_OPTS notebook```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 경로의 설정이 올바르게 되었는지 확인한다.\n",
    "* java, python 버전을 확인한다. java, javac가 다른 경우 일치 시킨다.\n",
    "\n",
    "리눅스 | 윈도우 \n",
    "-----|-----\n",
    "update-alternatives --config java | 관리자 권한으로 단말을 열어서 \"Run as administrator\"<br>```cd C:\\ProgramData\\Oracle\\Java\\javapath```<br>```mklink java.exe \"c:\\Program Files\\Java\\jdk1.8.0_152\\bin\\java.exe\"```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java version \"9.0.4\"\r\n",
      "Java(TM) SE Runtime Environment (build 9.0.4+11)\r\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 9.0.4+11, mixed mode)\r\n"
     ]
    }
   ],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "javac 9.0.4\r\n"
     ]
    }
   ],
   "source": [
    "!javac -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scala code runner version 2.11.6 -- Copyright 2002-2013, LAMP/EPFL\r\n"
     ]
    }
   ],
   "source": [
    "!scala -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 2.7.12\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 윈도우에서는 ```!where python```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## S.4 실행\n",
    "\n",
    "* Spark는 **batch, streaming, iterative, interactive 4가지 방식**으로 실행할 수 있다.\n",
    "\n",
    "### S.4.1 일괄실행\n",
    "\n",
    "* Spark 프로그램을 일괄 실행\n",
    "* Python 프로그램 test.py를 **spark-submit**하는 경우:\n",
    "\n",
    "```python\n",
    "spark-submit test.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.4.2 insteractive shell\n",
    "\n",
    "* Scala, Python에서 지원되는 REPL (the Read-Eval-Print-Loop)\n",
    "* scala\n",
    "\n",
    "```python\n",
    "spark-1.6.0-bin-hadoop2.6/bin/bin$ spark-shell\n",
    "scala>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* python\n",
    "    * spark설치 디렉토리 아래 bin으로 가서 pyspark를 실행하면, spark, sc는 기본 제공된다.\n",
    "    (윈도우는 **관리자권한'Run as administator'**으로 열어야 한다. 아니면 오류.)\n",
    "\n",
    "```python\n",
    "spark-2.0.0-bin-hadoop2.7/bin$ ./pyspark \n",
    "Python 2.7.12 (default, Nov 19 2016, 06:48:10) \n",
    "[GCC 5.4.0 20160609] on linux2\n",
    "\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.0.0\n",
    "      /_/\n",
    "\n",
    "Using Python version 2.7.12 (default, Nov 19 2016 06:48:10)\n",
    "SparkSession available as 'spark'.\n",
    ">>> sc.version\n",
    "u'2.0.0'\n",
    ">>> text=sc.textFile(\"derby.log\")\n",
    ">>>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.4.3 오류\n",
    "\n",
    "* **Hadoop 없는 버전**을 내려 받아 설치하면 오류 발생\n",
    "    * prebuilt with hadoop을 설치하면 된다.\n",
    "    * 예를 들어, Hadoop 없는 'spark-x.x.x'을 설치하고, 'spark-shell'를 실행하면,\n",
    "        * 'Failed to find Spark assembly spark' 오류\n",
    "        * so need to install sbt\n",
    "```python\n",
    "download tar from http://www.scala-sbt.org/\n",
    "tar -xvf sbt.tar\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* SparkContext: Error initializing SparkContext\n",
    "    * /etc/hostname이 지금 설정과 다른 경우의 오류\n",
    "    * 변경하려면\n",
    "```python\n",
    "$ scutil --set HostName jsl.com\n",
    "```\n",
    "        \n",
    "    * 확인하려면   \n",
    "```python\n",
    "$ hostname\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 'cannot import name accumulators'\n",
    "    * PYTHONPATH가 올바르게 설정되지 않아서 발생하는 오류\n",
    "    * sys.path()를 사용해 경로를 추가한다.\n",
    "\n",
    "* 사용하는 포트를 해제하지 않고 실행하는 경우 java.net.BindException: Address already in use\n",
    "    * 포트를 해제한다.\n",
    "```python\n",
    "ps -aux | grep spark\n",
    "kill -9 3370\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.5 ipython shell로 Spark사용하기\n",
    "\n",
    "* pyspark는 python shell을 사용한다. ipython shell로 변경하여 실행하려면:\n",
    "\n",
    "* **방법 1: 단순한 방식**\n",
    "    * 'PYSPARK_DRIVER_PYTHON=ipython'라고 설정하고 명령창에서 아래와 같이 pyspark를 실행한다.\n",
    "    (윈도우에서는 명령창을 관리자권한으로 열어 pyspark를 실행한다. 명령창에서 설정을 한 후에는 닫은 후 다시 열어야 한다.)\n",
    "\n",
    "```python\n",
    "\\$ export PYSPARK_DRIVER_PYTHON=ipython\n",
    "$ ./bin/pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 방법2: profile로 환경 설정하기\n",
    "    * ipython profile 생성하고, ipython shell 실행\n",
    "\n",
    "```python\n",
    "$ ipython console --profile=pyspark # 아래 ipython profile 생성 후 실행\n",
    "```\n",
    "\n",
    "    * ipython profile 생성\n",
    "\n",
    "```python\n",
    "\\$ ipython profile create pyspark # ipython profile 생성\n",
    "$ vim ~/.ipython/profile_pyspark/startup/00-pyspark-setup.py # profile startup 파일 생성\n",
    "import os\n",
    "import sys\n",
    "if \"SPARK_HOME\" not in os.environ:\n",
    "    os.environ[\"SPARK_HOME\"]=os.path.join(os.environ['HOME'],'Downloads','spark-2.0.0-bin-hadoop2.7')\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))\n",
    "#sys.path.insert(0,os.path.join(SPARK_HOME,\"python\")\n",
    "execfile(os.path.join(os.environ[\"SPARK_HOME\"],'python/pyspark/shell.py'))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## S.6 Jupyter Notebook으로 Spark사용하기\n",
    "\n",
    "* **방법1: spark/bin/pyspark를 사용해서 Jupyter Notebook.**\n",
    "    * 명령창에서 다음 명령어, 노트북 뜨고 나면 spark, sc는 기본 지원 (윈도우는 관리자권한으로 명령창을 열어야 한다)\n",
    "    * Spark 2.0이후 IPYTHON_OPTS 명령어 지원 하지 않음\n",
    "   \n",
    "```python\n",
    " 1096  cd ~/Downloads/spark-2.0.0-bin-hadoop2.7/bin/\n",
    " 1097  export PYSPARK_DRIVER_PYTHON=ipython\n",
    " 1098  ./pyspark # ipython이 뜸\n",
    " 1099  export PYSPARK_DRIVER_PYTHON_OPTS='notebook'\n",
    " 1100  ./pyspark # 노트북이 뜸\n",
    "```\n",
    "\n",
    "    * 다음 명령어는 오류 (IPTYHON_OPTS를 사용했던 경우) -> 7077 can not be reached\n",
    "```python\n",
    "MASTER=\"spark://127.0.0.1:7077\" SPARK_EXECUTOR_MEMORY=\"6G\" IPYTHON_OPTS=\"notebook\" ~/Downloads/spark-1.6.0-bin-hadoop2.6/bin/pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **방법 2: IPython Notebook을 실행하고, 설정을 변경한다. 강의자료는 이 기준으로 만들어져 있다.**\n",
    "    * PyPi에서 pyspark를 설치해서 사용하는 경우이다.\n",
    "    * SPARK_HOME 및 PYTHONPATH를 설정한다.\n",
    "    * Spark 2.0+를 사용하는 경우, SparkSession을 생성한다.\n",
    "\n",
    "설정 항목 | 설명\n",
    "----------|----------\n",
    "SPARK_HOME | Spark를 설치한 자신의 경로로 수정한다.\n",
    "PYTHONPATH | sys.path.insert()를 사용하여 PYTHONPATH를 수정한다. pyspark.zip, py4j-0.10.1-src.zip를 추가\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Pyspark를 PyPi에서 설치할 수 있다. Spark의 모든 기능을 포함하지 않는다. 따라서 spark를 설치 해 놓아야 한다.\n",
    "    * py4j를 자동으로 같이 설치한다.\n",
    "        * SPARK_HOME/bin/pyspark로 실행했어도, ```import pyspark```를 사용해도 된다. spark, sc가 제공되는지 아닌지의 차이만 있다.\n",
    "\n",
    "```python\n",
    "pip install pyspark\n",
    "```\n",
    "\n",
    "* py4j (선택적, 설치하면 PYTHONPATH에 py4j를 추가하지 않아도 된다.)\n",
    "\n",
    "```python\n",
    "pip install py4j\n",
    "``` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 방법 3: findspark 라이브러리를 설치\n",
    "    * 설치 디렉토리를 설정하면, findspark이 필요한 PYTHONPATH를 설정해 준다.\n",
    "\n",
    "```python\n",
    "pip install findspark\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "home=os.path.expanduser(\"~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(os.path.join(home,\"Downloads\",\"spark-2.0.0-bin-hadoop2.7\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"myAppName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SparkSession을 중지할 경우 stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 방법 4: ipython notebook kernel을 설정 (mac ok 20160902 fri)\n",
    "    * 'kernel.json' 작성 후, ipython notebook을 실행하면 kernel에 PySpark가 추가된다.\n",
    "```python        \n",
    "$ mkdir -p ~/.ipython/kernels/pyspark\n",
    "$ vim ~/.ipython/kernels/pyspark/kernel.json\n",
    "```\n",
    "\n",
    "    * kernel.json은 아래 내용을 수정해서 저장\n",
    "```python\n",
    "{\n",
    "    \"display_name\": \"pySpark (Spark 1.6.0) with graphFrames\",\n",
    "    \"language\": \"python\",\n",
    "    \"argv\": [\n",
    "        \"/usr/bin/python2.7\",\n",
    "        \"-m\",\n",
    "        \"ipykernel\",\n",
    "        \"-f\",\n",
    "        \"{connection_file}\"\n",
    "    ],  \n",
    "    \"env\": {\n",
    "        \"SPARK_HOME\": \"/Users/media/Downloads/spark-1.6.0-bin-hadoop2.6\",\n",
    "        \"PYTHONPATH\": \"/Users/media/Downloads/spark-1.6.0-bin-hadoop2.6/python:/Users/media/Downloads/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip\",\n",
    "        \"PYTHONSTARTUP\": \"/Users/media/Downloads/spark-1.6.0-bin-hadoop2.6/python/pyspark/shell.py\",\n",
    "        \"PYSPARK_SUBMIT_ARGS\": \"--packages graphframes:graphframes:0.1.0-spark1.6 --master spark://127.0.0.1:7077 pyspark-shell\",\n",
    "        \"SPARK_DRIVER_MEMORY\":\"10G\"\n",
    "     }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-1: Spark 시작하기\n",
    "\n",
    "* SPARK_HOME 설치 경로 및 PYTHONPATH에 추가하는 경로는 자신의 것으로 수정해야 한다.\n",
    "\n",
    "* 필요하지 않은 라이브러리의 경로를 제거하려면:\n",
    "```python\n",
    "sys.path.remove('/home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/python/lib/py4j-0.9-src.zip')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "os.environ[\"SPARK_HOME\"]=os.path.join(os.environ['HOME'],'Downloads','spark-2.0.0-bin-hadoop2.7')\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip\n",
      "/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/python/lib/py4j-0.10.1-src.zip\n",
      "\n",
      "/usr/lib/python2.7\n",
      "/usr/lib/python2.7/plat-x86_64-linux-gnu\n",
      "/usr/lib/python2.7/lib-tk\n",
      "/usr/lib/python2.7/lib-old\n",
      "/usr/lib/python2.7/lib-dynload\n",
      "/home/jsl/.local/lib/python2.7/site-packages\n",
      "/usr/local/lib/python2.7/dist-packages\n",
      "/usr/lib/python2.7/dist-packages\n",
      "/usr/lib/python2.7/dist-packages/PILcompat\n",
      "/usr/lib/python2.7/dist-packages/gtk-2.0\n",
      "/usr/lib/python2.7/dist-packages/ubuntu-sso-client\n",
      "/usr/local/lib/python2.7/dist-packages/IPython/extensions\n",
      "/home/jsl/.ipython\n"
     ]
    }
   ],
   "source": [
    "for i in sys.path:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 참고로 Spark 1.6은 다음과 같이 **SparkContext**를 생성해서 사용한다. Spark 2.0은 **SparkSession**으로 통합되었다.\n",
    "\n",
    "```python\n",
    "import pyspark\n",
    "conf=pyspark.SparkConf()\n",
    "conf = pyspark.SparkConf().setAppName(\"myAppName\")\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-2: Spark standalone cluster 구성하기\n",
    "\n",
    "* 클러스터를 구성하지 않으면, 단독형standalone으로 실행한다.\n",
    "    * curl로 7077, 8080확인해도 없음 -> NO cluster!\n",
    "\n",
    "* 클러스터의 종류:\n",
    "    * Spark-Standalone – Spark workers are registered with spark master\n",
    "    * Yarn – Spark workers are registered with YARN Cluster manager.\n",
    "    * Mesos – Spark workers are registered with Mesos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 클러스터 환경 구성\n",
    "    * Client: spark shell, pyspark shell\n",
    "    * Cluster\n",
    "        * Cluster 1: 마스터\n",
    "            * Spark master / Spark worker\n",
    "            * HDFS namenode / datanode\n",
    "        * Cluster n: 작업노드\n",
    "            * Spark worker\n",
    "            * HDFS datanode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Spark-Standalone의 master-slave 실행\n",
    "* 단계1 JAVA_HOME을 설정\n",
    "    * JAVA_HOME을 설정한다.\n",
    "        * automatically set symlink to java binary /usr/bin/java\n",
    "        * JAVA_HOME을 설정하려면 /etc/environment에 하는 것이 좋다.\n",
    "```python\n",
    "$ echo $JAVA_HOME\n",
    "$ update-alternatives --config java\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 단계2: master 실행\n",
    "    * SPARK_HOME은 /home/jsl/Downloads/spark-1.6.0-bin-hadoop2.6/\n",
    "    * $SPARK_HOME/conf/spark-env.sh에 master ip설정\n",
    "    * spark://IPADRESS_OF_YOUR_MASTER_SYSTEM:7077\n",
    "    * 기본 port는 7077 (web UI는 localhost:8080)\n",
    "\n",
    "```python\n",
    "SPARK_MASTER_IP=\n",
    "```\n",
    "\n",
    "    * 실행\n",
    "\n",
    "```python\n",
    "$ sh $SPARK_HOME/sbin/start-master.sh\n",
    "```\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 단계3: slave 실행 (worker라고 함)\n",
    "```python\n",
    "$ sh $SPARK_HOME/sbin//start-slave.sh spark://IPADRESS_OF_YOUR_MASTER_SYSTEM:7077\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 쉘 명령어\n",
    "\n",
    "sbin디렉토리의 shell | 설명\n",
    "----------|----------\n",
    "start-master.sh, stop-master.sh | 마스터를 시작 (종료)\n",
    "start-slaves.sh, stop-slaves.sh | 각 노드의 슬레이브를 시작 (종료)\n",
    "start-all.sh, stop-all.sh | 마스터, 슬레이브를 모두 시작 (종료)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.7 설정\n",
    "\n",
    "* Spark 설정은\n",
    "    * 프로그램 내부에서 SparkConf()를 사용해서 하거나,\n",
    "    * 프로그램 외부에서도 할 수 있다. conf/ 디렉토리에 주어진 spark-defaults.conf, spark-env.sh, log4j.properties 등 설정파일의 환경변수를 변경해서 할 수 있다\n",
    "        * conf/ 아닌 디렉토리를 사용하려면 'SPARK_CONF_DIR'를 수정하면 된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.7.1 필요한 jar를 추가\n",
    "\n",
    "* Scala, Java에서 하는 것 보다 Python에서 jar를 추가하는 것은 쉽지 않다.\n",
    "* SparkContext를 생성하고 난 후, 동적으로 jar를 추가할 수 없다.\n",
    "\n",
    "#### S.7.1.1 설정 파일\n",
    "\n",
    "* 단계 1: Spark Package에서 jar url 검색 https://spark-packages.org/\n",
    "    * Spark Package의 명칭은 maven에서 명명하는 방식에 따라, 다음을 콜론(:)으로 합쳐서 사용한다.\n",
    "\n",
    "구분 | 설명\n",
    "-----|-----\n",
    "groupId | 패키지 명칭.\n",
    "artifactId | jar 명칭. 확장자는 생략한다.\n",
    "version | 버전 정보, x.x.x\n",
    "\n",
    "```python\n",
    "org.mongodb.spark:mongo-spark-connector_2.10:2.0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 단계 2: 'spark-default.conf' 설정 파일의 수정\n",
    "\n",
    "구분 | 설명\n",
    "----------|----------\n",
    "spark 패키지 | spark.jars.package의 property를 수정한다. 여러 packages는 컴마로 분리\n",
    "jar | driver, executor 설정. 와일드카드 '*'를 사용할 수 있다.<br>spark.driver.extraClassPath /path/to/my.jar<br>spark.executor.extraClassPath /path/to/my.jar\n",
    "\n",
    "* 단계 3: 실행\n",
    "    * 설정을 해 놓으면 'pyspark', 'submit-spark' 모두 그 설정을 읽어 실행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### S.7.1.2 다른 방법\n",
    "\n",
    "* jar를 내려 받아서 shell에서 실행하는 방법도 있다.\n",
    "    * https://spark-packages.org/에 가서 Spark 패키지 해당하는 jar를 내려 받고,\n",
    "    * 디렉토리를 만든 후 복사하고, symlink를 만들어 이름을 줄일 수 있다.\n",
    "```python\n",
    "cd ~/Downloads/spark-1.6.0-bin-hadoop2.6/lib\n",
    "ln -s graphframes-0.1.0-spark1.6.jar graphframes.jar\n",
    "```\n",
    "\n",
    "* [ok] pyspark --jars\n",
    "\n",
    "```python\n",
    "$ bin/pyspark --py-files lib/graphframes.jar --jars lib/graphframes.jar \n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.6.0\n",
    "      /_/\n",
    "\n",
    "Using Python version 2.7.12 (default, Jul  1 2016 15:12:24)\n",
    "SparkContext available as sc, HiveContext available as sqlContext.\n",
    ">>> from graphframes import *\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* [ok] spark-submit --packages\n",
    "```python\n",
    "./bin/spark-submit src/ds_spark_dataframe.py \\\n",
    "   --packages graphframes:graphframes:0.1.0-spark1.6\n",
    "```\n",
    "\n",
    "* [x] addJar()는 사용할 수 없다.\n",
    "```python\n",
    "SparkContext.addJar(...) method\n",
    "SparkContext.addFile(...) method\n",
    "```\n",
    "    \n",
    "* [not yet] loading jars\n",
    "```python\n",
    "from py4j.java_gateway import java_import\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* [nok] os.environ - submit_args\n",
    "    * IPyton Notebook에서 class-not-found errors. python kernel이 일단 실행되고 나면, 동적으로 추가하는 것이 가능하지 않다. (subprocess.Popen은 env값을 가져올 수 없다?)\n",
    "```python\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages graphframes:graphframes:0.2.0-spark2.0-s_2.11'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-3: MongoDB를 사용하기 위한 jar를 추가하는 설정 변경하기\n",
    "\n",
    "* 참고 MongoDB의 Spark 연결 설명 https://docs.mongodb.com/spark-connector/\n",
    "* 우선, MongoDB가 설치되어 서버가 실행되어 있어야 한다. 올바른 jar 버전을 사용하기 위해 MongoDB의 버전을 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "db version v2.6.10\r\n",
      "2017-04-15T06:05:02.080+0900 git version: nogitversion\r\n",
      "2017-04-15T06:05:02.080+0900 OpenSSL version: OpenSSL 1.0.2g  1 Mar 2016\r\n"
     ]
    }
   ],
   "source": [
    "!mongod --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 단계 1: Spark Package 설치 확인 https://spark-packages.org/\n",
    "    * mongo-spark를 검색하여, 자신의 Spark Version, Scala Version을 확인해서 맞는 jar를 선택한다.\n",
    "        * mongo-spark-connector_2.10을 사용하려면 Scala 2.10.x이 필요하다.\n",
    "        * scala 버전이 2.11이라 mongo-spark-connector_2.11을 넣어야 하지만, 2.10으로 ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 단계 2: 설정파일 conf/spark-defaults.conf 수정\n",
    "    * MongoDB<3.2인 경우 spark.mongodb.input.partitioner가 필요하다.\n",
    "```python\n",
    "$vim conf/spark-defaults.conf \n",
    "spark.jars.packages=org.mongodb.spark:mongo-spark-connector_2.10:1.1.0\n",
    "spark.mongodb.input.partitioner=MongoPaginateBySizePartitioner\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 단계 3: pyspark에서 SparkConf \n",
    "```python\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/myDB.ds_spark_df_mongo\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/myDB.ds_spark_df_mongo\") \\\n",
    "    .getOrCreate()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 단계 4: 실행\n",
    "    * pyspark로 사용할 경우\n",
    "\n",
    "```python\n",
    "./bin/pyspark --conf \"spark.mongodb.input.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred\" \\\n",
    "              --conf \"spark.mongodb.output.uri=mongodb://127.0.0.1/test.myCollection\" \\\n",
    "              --packages org.mongodb.spark:mongo-spark-connector_2.10:1.1.0\n",
    "```\n",
    "\n",
    "* 오류 DefaultMongoPartitioner: MongoDB version < 3.2 detected. 설정에 추가\n",
    "```python\n",
    "spark.mongodb.input.partitioner=MongoPaginateBySizePartitioner\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graphframes:graphframes:0.4.0-spark2.0-s_2.11,org.mongodb.spark:mongo-spark-connector_2.10:2.0.0,com.databricks:spark-csv_2.11:1.5.0\n"
     ]
    }
   ],
   "source": [
    "print spark.conf.get('spark.jars.packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-4: sqlite를 사용하기 위한 jar를 추가하는 설정 변경하기\n",
    "\n",
    "* sqlite와 같이 Spark 패키지가 없는 경우, jar를 다운로드하고 'spark.driver.extraClassPath'에 추가한다.\n",
    "\n",
    "* 단계 1: jar 다운로드\n",
    "    * 따라서 jar를 다운로드 받아서, 추가한다. \n",
    "* 단계 2: 설정파일 conf/spark-defaults.conf 수정\n",
    "    * 여러 jar를 추가할 경우, 리눅스는 ':' 윈도우 ';'로 분리하여 추가한다.\n",
    "    * 이전 버전과의 호환성에 필요한 경우 'spark.executor.extraClassPath'도 추가할 수 있다.\n",
    "\n",
    "```python\n",
    "$vim conf/spark-defaults.conf \n",
    "spark.driver.extraClassPath /home/jsl/Code/git/bb/jsl/pyds/lib/sqlite-jdbc-3.14.2.jar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jsl/Code/git/bb/jsl/pyds/lib/sqlite-jdbc-3.14.2.jar\n"
     ]
    }
   ],
   "source": [
    "print spark.conf.get('spark.driver.extraClassPath')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.7.2 log\n",
    "\n",
    "* log4j.properties를 사용해서 변경한다.\n",
    "\n",
    "```python\n",
    "$ cp conf/log4j.properties.template conf/log4j.properties\n",
    "$ vim conf/log4j.properties\n",
    "log4j.rootCategory=ERROR, console\n",
    "```\n",
    "\n",
    "* log level\n",
    "    * ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.8 spark-sklearn\n",
    "\n",
    "sklearn은 데이터분석에 많이 사용되는 패키지이다.\n",
    "Spark에서 sklearn 패키지를 그대로 사용한다면 보다 많은 기계학습 라이브러리를 사용할 수 있게 한다.\n",
    "\n",
    "* [spark-sklearn](https://github.com/databricks/spark-sklearn)\n",
    "```python\n",
    "pip install spark-sklearn\n",
    "```\n",
    "\n",
    "* spark-shell\n",
    "```python\n",
    "$SPARK_HOME/bin/spark-shell --packages databricks:spark-sklearn:0.2.0\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
