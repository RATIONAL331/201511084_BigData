{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "home=os.path.expanduser(\"~\")\n",
    "os.environ[\"SPARK_HOME\"]=os.path.join(home, 'spark-2.0.0-bin-hadoop2.7')\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "# python과 자바를 연결하는 파일을 경로 설정에 집어 넣는다. # pip install py4j\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))\n",
    "# pyspark.zip을 경로 설정에 집어 넣는다. # pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder.master(\"local\").appName(\"myApp\").config(conf=myConf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe\n",
    "* 행과 열이 존재한다.\n",
    "* RDD는 행과 열이 존재하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n",
      "[Row(_1=u'1', _2=u'kim, js', _3=170)]\n"
     ]
    }
   ],
   "source": [
    "myList=[('1','kim, js',170),\n",
    "        ('1','lee, sm', 175),\n",
    "        ('2','lim, yg',180),\n",
    "        ('2','lee',170)]\n",
    "\n",
    "myDf=spark.createDataFrame(myList)\n",
    "myDf.printSchema()\n",
    "print myDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- item: string (nullable = true)\n",
      "\n",
      "+----+----------+\n",
      "|name|      item|\n",
      "+----+----------+\n",
      "| kim|  espresso|\n",
      "| lee|     latte|\n",
      "| lee| americano|\n",
      "| lim|  affocato|\n",
      "| kim|long black|\n",
      "| lee|  macciato|\n",
      "| lee|  espresso|\n",
      "| lim|     latte|\n",
      "| kim| americano|\n",
      "| lee|  affocato|\n",
      "+----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = [\"kim\",\"lee\",\"lee\",\"lim\"]\n",
    "items = [\"espresso\",\"latte\",\"americano\",\"affocato\",\"long black\",\"macciato\"]\n",
    "df = spark.createDataFrame([(names[i%4], items[i%6]) for i in range(100)],\\\n",
    "                           [\"name\",\"item\"])\n",
    "df.printSchema()\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|short name|\n",
      "+----------+\n",
      "|       esp|\n",
      "|       lat|\n",
      "|       ame|\n",
      "|       aff|\n",
      "|       lon|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.item.substr(1, 3).alias(\"short name\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROW 객체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "Person = Row('year','name', 'height')\n",
    "row1=Person('1','kim, js',170)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      "\n",
      "None\n",
      "+----+-------+------+\n",
      "|year|   name|height|\n",
      "+----+-------+------+\n",
      "|   1|kim, js|   170|\n",
      "|   1|lee, sm|   175|\n",
      "|   2|lim, yg|   180|\n",
      "|   2|    lee|   170|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myRows = [row1,\n",
    "          Person('1','lee, sm', 175),\n",
    "          Person('2','lim, yg',180),\n",
    "          Person('2','lee',170)]\n",
    "myDf=spark.createDataFrame(myRows)\n",
    "\n",
    "\n",
    "print myDf.printSchema()\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# schema 정의하고 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "mySchema=StructType([\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      "\n",
      "+----+-------+------+\n",
      "|year|   name|height|\n",
      "+----+-------+------+\n",
      "|   1|kim, js|   170|\n",
      "|   1|lee, sm|   175|\n",
      "|   2|lim, yg|   180|\n",
      "|   2|    lee|   170|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf=spark.createDataFrame(myRows, mySchema)\n",
    "myDf.printSchema()\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD에서 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "myList=[('1','kim, js',170),('1','lee, sm', 175),('2','lim, yg',180),('2','lee',170)]\n",
    "myRdd = spark.sparkContext.parallelize(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf=myRdd.toDF()\n",
    "rddDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf=spark.createDataFrame(myRdd)\n",
    "rddDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| _1|     _2|\n",
      "+---+-------+\n",
      "|  1|kim, js|\n",
      "|  2|    lee|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf.where(rddDf._3 < 175)\\\n",
    "    .select([rddDf._1, rddDf._2])\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| _1|max(_3)|\n",
      "+---+-------+\n",
      "|  1|    175|\n",
      "|  2|    180|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf.groupby(rddDf._1).max().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _1속성 int로 캐스팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- height: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myRdd=myRdd.map(lambda x:Row(year=int(x[0]),name=x[1],height=int(x[2])))\n",
    "_myDf=spark.createDataFrame(_myRdd)\n",
    "_myDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=10, name='js1'), Row(age=20, name='js2')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType, TimestampType\n",
    "r1=Row(name=\"js1\",age=10)\n",
    "r2=Row(name=\"js2\",age=20)\n",
    "_myRdd=spark.sparkContext.parallelize([r1,r2])\n",
    "\n",
    "_myRdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 10| js1|\n",
      "| 20| js2|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema=StructType([\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    #StructField(\"created\", TimestampType(), True)\n",
    "])\n",
    "_myDf=spark.createDataFrame(_myRdd,schema)\n",
    "_myDf.printSchema()\n",
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n",
      "+---+----+------+\n",
      "| id|name|height|\n",
      "+---+----+------+\n",
      "|  1| kim|  50.0|\n",
      "|  2| lee|  60.0|\n",
      "|  3|park|  70.0|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "myRdd=spark.sparkContext.parallelize([(1, 'kim', 50.0), (2, 'lee', 60.0), (3, 'park', 70.0)])\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", DoubleType(), True)\n",
    "])\n",
    "_myDf = spark.createDataFrame(myRdd, schema)\n",
    "_myDf.printSchema()\n",
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>name</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>kim, js</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>lee, sm</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>lim, yg</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>lee</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  year     name  height\n",
       "0    1  kim, js     170\n",
       "1    1  lee, sm     175\n",
       "2    2  lim, yg     180\n",
       "3    2      lee     170"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "myDf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data/ds_twitter_seoul_3.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/ds_twitter_seoul_3.json\n",
    "{\"contributors\": null, \"truncated\": false, \"text\": \"RT @soompi: #SEVENTEEN’s Mingyu, Jin Se Yeon, And Leeteuk To MC For 2016 Super Seoul Dream Concert \\nhttps://t.co/1XRSaRBbE0 https://t.co/fi…\", \"is_quote_status\": false, \"in_reply_to_status_id\": null, \"id\": 801657325836763136, \"favorite_count\": 0, \"entities\": {\"symbols\": [], \"user_mentions\": [{\"id\": 17659206, \"indices\": [3, 10], \"id_str\": \"17659206\", \"screen_name\": \"soompi\", \"name\": \"Soompi\"}], \"hashtags\": [{\"indices\": [12, 22], \"text\": \"SEVENTEEN\"}], \"urls\": [{\"url\": \"https://t.co/1XRSaRBbE0\", \"indices\": [100, 123], \"expanded_url\": \"http://www.soompi.com/2016/11/20/seventeens-mingyu-jin-se-yeon-leeteuk-mc-dream-concert/\", \"display_url\": \"soompi.com/2016/11/20/sev…\"}]}, \"retweeted\": false, \"coordinates\": null, \"source\": \"<a href=\\\"http://twitter.com/download/android\\\" rel=\\\"nofollow\\\">Twitter for Android</a>\", \"in_reply_to_screen_name\": null, \"in_reply_to_user_id\": null, \"retweet_count\": 1487, \"id_str\": \"801657325836763136\", \"favorited\": false, \"retweeted_status\": {\"contributors\": null, \"truncated\": false, \"text\": \"#SEVENTEEN’s Mingyu, Jin Se Yeon, And Leeteuk To MC For 2016 Super Seoul Dream Concert \\nhttps://t.co/1XRSaRBbE0 https://t.co/fifXHpF8or\", \"is_quote_status\": false, \"in_reply_to_status_id\": null, \"id\": 800593781586132993, \"favorite_count\": 1649, \"entities\": {\"symbols\": [], \"user_mentions\": [], \"hashtags\": [{\"indices\": [0, 10], \"text\": \"SEVENTEEN\"}], \"urls\": [{\"url\": \"https://t.co/1XRSaRBbE0\", \"indices\": [88, 111], \"expanded_url\": \"http://www.soompi.com/2016/11/20/seventeens-mingyu-jin-se-yeon-leeteuk-mc-dream-concert/\", \"display_url\": \"soompi.com/2016/11/20/sev…\"}], \"media\": [{\"expanded_url\": \"https://twitter.com/soompi/status/800593781586132993/photo/1\", \"display_url\": \"pic.twitter.com/fifXHpF8or\", \"url\": \"https://t.co/fifXHpF8or\", \"media_url_https\": \"https://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\", \"id_str\": \"800593115165798400\", \"sizes\": {\"small\": {\"h\": 382, \"resize\": \"fit\", \"w\": 680}, \"large\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"medium\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"thumb\": {\"h\": 150, \"resize\": \"crop\", \"w\": 150}}, \"indices\": [112, 135], \"type\": \"photo\", \"id\": 800593115165798400, \"media_url\": \"http://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\"}]}, \"retweeted\": false, \"coordinates\": null, \"source\": \"<a href=\\\"https://about.twitter.com/products/tweetdeck\\\" rel=\\\"nofollow\\\">TweetDeck</a>\", \"in_reply_to_screen_name\": null, \"in_reply_to_user_id\": null, \"retweet_count\": 1487, \"id_str\": \"800593781586132993\", \"favorited\": false, \"user\": {\"follow_request_sent\": false, \"has_extended_profile\": true, \"profile_use_background_image\": true, \"default_profile_image\": false, \"id\": 17659206, \"profile_background_image_url_https\": \"https://pbs.twimg.com/profile_background_images/699864769/1cdde0a85f5c0a994ae1fb06d545a5ec.png\", \"verified\": true, \"translator_type\": \"none\", \"profile_text_color\": \"999999\", \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/792117259489583104/4khJk3zz_normal.jpg\", \"profile_sidebar_fill_color\": \"000000\", \"entities\": {\"url\": {\"urls\": [{\"url\": \"http://t.co/3evT80UlR9\", \"indices\": [0, 22], \"expanded_url\": \"http://www.soompi.com\", \"display_url\": \"soompi.com\"}]}, \"description\": {\"urls\": []}}, \"followers_count\": 987867, \"profile_sidebar_border_color\": \"000000\", \"id_str\": \"17659206\", \"profile_background_color\": \"1E1E1E\", \"listed_count\": 3982, \"is_translation_enabled\": true, \"utc_offset\": -28800, \"statuses_count\": 80038, \"description\": \"The original K-pop community. We take gifs, OTPs, and reporting on your bias' fashion choices seriously. But not rumors. Ain't nobody got time for that.\", \"friends_count\": 3532, \"location\": \"Worldwide\", \"profile_link_color\": \"31B6F4\", \"profile_image_url\": \"http://pbs.twimg.com/profile_images/792117259489583104/4khJk3zz_normal.jpg\", \"following\": false, \"geo_enabled\": false, \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/17659206/1478803767\", \"profile_background_image_url\": \"http://pbs.twimg.com/profile_background_images/699864769/1cdde0a85f5c0a994ae1fb06d545a5ec.png\", \"screen_name\": \"soompi\", \"lang\": \"en\", \"profile_background_tile\": true, \"favourites_count\": 1493, \"name\": \"Soompi\", \"notifications\": false, \"url\": \"http://t.co/3evT80UlR9\", \"created_at\": \"Wed Nov 26 20:48:27 +0000 2008\", \"contributors_enabled\": false, \"time_zone\": \"Pacific Time (US & Canada)\", \"protected\": false, \"default_profile\": false, \"is_translator\": false}, \"geo\": null, \"in_reply_to_user_id_str\": null, \"possibly_sensitive\": false, \"lang\": \"en\", \"created_at\": \"Mon Nov 21 06:56:46 +0000 2016\", \"in_reply_to_status_id_str\": null, \"place\": null, \"extended_entities\": {\"media\": [{\"expanded_url\": \"https://twitter.com/soompi/status/800593781586132993/photo/1\", \"display_url\": \"pic.twitter.com/fifXHpF8or\", \"url\": \"https://t.co/fifXHpF8or\", \"media_url_https\": \"https://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\", \"id_str\": \"800593115165798400\", \"sizes\": {\"small\": {\"h\": 382, \"resize\": \"fit\", \"w\": 680}, \"large\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"medium\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"thumb\": {\"h\": 150, \"resize\": \"crop\", \"w\": 150}}, \"indices\": [112, 135], \"type\": \"photo\", \"id\": 800593115165798400, \"media_url\": \"http://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\"}]}, \"metadata\": {\"iso_language_code\": \"en\", \"result_type\": \"recent\"}}, \"user\": {\"follow_request_sent\": false, \"has_extended_profile\": false, \"profile_use_background_image\": true, \"default_profile_image\": true, \"id\": 791090169818521600, \"profile_background_image_url_https\": null, \"verified\": false, \"translator_type\": \"none\", \"profile_text_color\": \"333333\", \"profile_image_url_https\": \"https://abs.twimg.com/sticky/default_profile_images/default_profile_6_normal.png\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"entities\": {\"description\": {\"urls\": []}}, \"followers_count\": 0, \"profile_sidebar_border_color\": \"C0DEED\", \"id_str\": \"791090169818521600\", \"profile_background_color\": \"F5F8FA\", \"listed_count\": 0, \"is_translation_enabled\": false, \"utc_offset\": null, \"statuses_count\": 96, \"description\": \"\", \"friends_count\": 7, \"location\": \"\", \"profile_link_color\": \"1DA1F2\", \"profile_image_url\": \"http://abs.twimg.com/sticky/default_profile_images/default_profile_6_normal.png\", \"following\": false, \"geo_enabled\": false, \"profile_background_image_url\": null, \"screen_name\": \"enriquesanq\", \"lang\": \"es\", \"profile_background_tile\": false, \"favourites_count\": 161, \"name\": \"Enrique santos\", \"notifications\": false, \"url\": null, \"created_at\": \"Wed Oct 26 01:32:49 +0000 2016\", \"contributors_enabled\": false, \"time_zone\": null, \"protected\": false, \"default_profile\": true, \"is_translator\": false}, \"geo\": null, \"in_reply_to_user_id_str\": null, \"possibly_sensitive\": false, \"lang\": \"en\", \"created_at\": \"Thu Nov 24 05:22:55 +0000 2016\", \"in_reply_to_status_id_str\": null, \"place\": null, \"metadata\": {\"iso_language_code\": \"en\", \"result_type\": \"recent\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "_jfname=os.path.join('data','ds_twitter_seoul_3.json')\n",
    "with open(_jfname, 'rb') as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = map(lambda x: x.rstrip(), data)\n",
    "data_json_str = \"[\" + ','.join(data) + \"]\"\n",
    "\n",
    "import pandas as pd\n",
    "data_df = pd.read_json(data_json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6908"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contributors                 0\n",
      "coordinates                  0\n",
      "created_at                   1\n",
      "entities                     1\n",
      "favorite_count               1\n",
      "favorited                    1\n",
      "geo                          0\n",
      "id                           1\n",
      "id_str                       1\n",
      "in_reply_to_screen_name      0\n",
      "in_reply_to_status_id        0\n",
      "in_reply_to_status_id_str    0\n",
      "in_reply_to_user_id          0\n",
      "in_reply_to_user_id_str      0\n",
      "is_quote_status              1\n",
      "lang                         1\n",
      "metadata                     1\n",
      "place                        0\n",
      "possibly_sensitive           1\n",
      "retweet_count                1\n",
      "retweeted                    1\n",
      "retweeted_status             1\n",
      "source                       1\n",
      "text                         1\n",
      "truncated                    1\n",
      "user                         1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print data_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    801657325836763136\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['id'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(age=29, name=u'Michael'),\n",
       " Row(age=30, name=u'Andy'),\n",
       " Row(age=19, name=u'Justin')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "cfile= os.path.join(os.environ[\"SPARK_HOME\"],\"examples/src/main/resources/people.txt\")\n",
    "lines = spark.sparkContext.textFile(cfile)\n",
    "\n",
    "\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1].strip())))\n",
    "\n",
    "_myDf = spark.createDataFrame(people)\n",
    "\n",
    "_myDf.printSchema()\n",
    "_myDf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/ds_spark.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark.csv\n",
    "1,2,3,4\n",
    "11,22,33,44\n",
    "111,222,333,444"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  1|  2|  3|  4|\n",
      "+---+---+---+---+\n",
      "| 11| 22| 33| 44|\n",
      "|111|222|333|444|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header='true', inferschema='true').load('data/ds_spark.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
